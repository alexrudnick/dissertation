\chapter{Related Work}
\label{chap:relatedwork}

In this chapter, I will review some of the recent related work, including
techniques for CL-WSD considered as a task on its own, the application of
classifiers to lexical selection in various machine translation architectures,
techniques used in recent runnings of the SemEval task on CL-WSD, and some
relevant approaches for word-sense disambiguation more broadly. The Chipa
software and the work described in the rest of the dissertation draws heavily
from ideas described here; how particular ideas have been influential will be
highlighted as we get to them.

\section{CL-WSD \emph{in vitro}}
Some of the related work on CL-WSD has studied the CL-WSD task in isolation, 
without being embedded into a machine translation system as such.
It is sensible to explore CL-WSD on its own for several reasons. Firstly, even
if our motivation for developing CL-WSD is to improve machine translation
output, from an engineering perspective, we still want to get a sense of
how well the individual components of our pipeline are functioning.
Furthermore, if the MT pipeline is relying on a CL-WSD system to help it with
lexical selection, one expects that improved disambiguation accuracy will
result in better translations overall -- though of course in complex software,
unexpected interactions may occur.

\subsection{Multilingual Evidence for CL-WSD}
One of the most interesting CL-WSD systems of recent years, which inspired our
use of multilingual evidence in Chipa, is ParaSense
\cite{lefever-hoste-decock:2011:ACL-HLT2011}, developed by Els Lefever and
colleagues.

ParaSense uses memory-based learning to predict target-language translations of
individual source-language words in a lexical sample task. It starts with
features that one would expect for a text classification problem: the surface
forms, POS tags, and lemmas for the focus word and the tokens in a small window
around it, as well as some syntactic information from a chunk parser. But
ParaSense also includes bag-of-words features for translations of the input
sentence into languages \emph{other} than the target language. ParaSense
handles translation from English into French, Spanish, Italian, Dutch and
German, so for example when translating from English to French, it includes
bag-of-words features extracted from the translations of the English sentence
into Spanish, Italian, Dutch and German (but not French).

Given corpora that are parallel over many languages, such as Europarl, this is
straightforward at training time. However, at testing time it is faced with a
novel sentence for which it has no translation. Here ParaSense requires a
complete MT system for each of the four other languages: it generates a
translation into the other four languages.
When ParaSense was being developed, it simply called out to the Google
Translate API (which is, as of this writing, no longer available gratis) to
generate the bag-of-words features required for test sentences.
This seems unwieldy and arguably detrimental to reproducible research: it
benefits from whatever techniques Google Translate uses for word choice,
which are not necessarily discussed with the public, and will change
without warning over time.

Critiques aside, ParaSense posted excellent results. When applied to the
SemEval 2010 CL-WSD task test data (described in
\cite{lefever-hoste:2010:SemEval}), it outperformed all of the systems
submitted for that task for nearly all of the target languages, being
outperformed, by a small margin, for one system when targeting Spanish
\cite{lefever-hoste-decock:2011:ACL-HLT2011}. A later version, described in
Lefever's dissertation, outperformed all of the SemEval systems.

In our earlier work, we prototyped a system that addresses some of the issues
with ParaSense \cite{rudnick-liu-gasser:2013:SemEval-2013}; these ideas are
developed further in Chapter \ref{chap:multilingual}.
Chipa requires neither a locally running MT system nor access to an online
translation API for its feature extraction, but still learns from several
mutually parallel bitexts that share a source language.

\section{WSD with Sequence Models}
To my knowledge, there has not been other work that explicitly frames all-words
CL-WSD as a sequence labeling problem. Machine translation systems, and
especially statistical machine translation systems, address this problem
implicitly -- jointly along with other problems -- when they use language
models to encourage coherent word choices in the generated text. The
decomposition of SMT systems into a translation model and a language model
allows LMs to be trained on very large target-language corpora.  In contrast,
in our earlier work on CL-WSD with sequence models, we only trained on
sentence-aligned bitext.

There have, however, been some uses of sequence models for monolingual WSD; in
this setting, systems can find an assignment disambiguating the entire input
jointly.  Molina et al. \cite{DBLP:conf/iberamia/MolinaPS02} have described the
use of hidden Markov models for WSD, situating all-words WSD as a tagging
problem.  They also used this approach in a Senseval entry
\cite{molina-pla-segarra:2004:Senseval-3} in the English all-words task at
Senseval-3, where it posted results towards the middle of the pack.

Ciaramita and Altun \cite{ciaramita-altun:2006:EMNLP} frame both coarse-grained
WSD and information extraction as a tagging problem, using a discriminative
variation of HMMs trained with perceptrons \cite{collins:2002:EMNLP02}
to maps nouns and verbs in the input text onto one of 41 ``supersenses", a
small ontology developed by Ciaramita and Johnson.

While it is not explicitly about sequence models, there is a related research
program underway in the world of graph-based algorithms for knowledge-based
word-sense disambiguation, exemplified by the work of Moro et al.
\cite{DBLP:journals/tacl/0001RN14}; they perform word-sense disambiguation and
entity linking jointly, searching for a globally coherent solution to both
problems.

\section{CL-WSD for Lexical Selection in RBMT}
Francis Tyers, in his dissertation work \cite{tyers-dissertation}, develops a
practical approach for improving lexical selection for the Apertium RBMT
system.
As he describes in \cite{tyers-fst}, his software learns finite-state
transducers for lexical selection from the available parallel corpora.
It is intended to be both very fast, for use in practical translation systems,
and to produce lexical selection rules that are understandable and modifiable
by humans. He also describes experiments in which humans wrote lexical
selection rules with the same framework.
The rules available to the system can reference the lexical items and parts of
speech surrounding the focus word, and when one of them fires, it selects a
translation.

%% XXX
There's more to say about this.


\section{CL-WSD for Statistical Machine Translation}
While the idea has, for the most part, fallen by the wayside in recent
years,\footnote{There are a number of papers on the topic, but for the past
decade or so, the ``standard" model of phrase-based SMT has not included WSD as
a component. There are a just three paragraphs on the topic in Koehn's book on
SMT \cite[\S 5.3.6]{koehn2010statistical}, and the major open-source SMT
systems have not enabled WSD modules by default.}
some of the earliest work on statistical machine translation at IBM included
an explicit WSD module \cite{Brown91word-sensedisambiguation}.
Brown et al. considered the senses of a word to be its possible translations,
explicitly distancing their work from other WSD efforts that focused on
dictionary senses.
The WSD module described in the 1991 paper used binary classifiers similar to
decision stumps. Each focus word was allocated a single classifier, which could
check a single feature, e.g., the identity of the nearest noun to the right of
the focus word.
These simple classifiers would then bias the decoder to choose a
translation from one of two classes of target-language translations,
which often significantly reduced the decoder's uncertainty. The inclusion of
WSD improved the translation output markedly; the authors deemed 45 of the
output sentences ``acceptable" with WSD enabled, over only 37 without it.
Later versions of IBM's CANDIDE system included a more sophisticated
disambiguation system: contextually-tuned translation models based on
maximum entropy modeling for the most common words in the source language
\cite{Berger:1994:CSM:1075812.1075844}.

The idea of training classifiers on source-language context to help lexical
selection is an attractive one, and has resurfaced several times in the SMT
literature. However, by the early-to-mid 2000s, phrase-based SMT techniques
\cite{koehn2003statistical}, which translate short coherent chunks of text
rather than individual words and thus often get correct word choice ``for
free", seemed to make WSD modules unnecessary.
Work by Cabezas and
Resnik~\cite{cabezas2005using} and early attempts by Carpuat and
Wu~\cite{carpuat-wu:2005:ACL} cast doubt as to whether phrase-based SMT
systems, or even word-based SMT systems trained on large data sets, could
benefit from WSD; Cabezas and Resnik, though expressing enthusiasm for future
developments, did not manage to improve translation performance with WSD, and
initially (in 2005) Carpuat and Wu argued that SMT could be considered a
generalization of word-sense disambiguation. Vickrey et al.
\cite{vickrey-EtAl:2005:HLTEMNLP} noted that lexical selection to translate
phrases as units would be important for applications in MT, though they did not
implement it in their work on the topic.

Not long thereafter, Carpuat and Wu showed how to use classifiers to
improve modern phrase-based SMT systems with a series of papers on the topic
\cite{carpuatpsd,carpuat-wu:2007:EMNLP-CoNLL2007,carpuat2008evaluation,improvingsmtwsd}.
In this work, they show that in order for phrase-based SMT systems to benefit
from classifiers for lexical selection, the system should do
\emph{phrase}-sense disambiguation, rather than word sense. This is to say, the
classifiers should be used to select the translations of source-language
phrases from the corresponding target-language phrases, better matching the
model used by the rest of the pipeline.
Here Carpuat and Wu use an ensemble of classifiers, including naïve Bayes, a
maximum entropy model, an Adaboost classifier, and a nearest-neighbor model in
which feature vectors have been projected into a new space with a kernel and
PCA has been performed to reduce dimensionality.
They used features based on local word context, some syntactic features, and
the position of the focus phrase in the sentence. Their classifiers interact
with the decoder by modifying the phrase table just before decoding, adding
features usable by the decoder's log-linear model.
This approach outperformed the earlier attempts in which individual words had
been labeled either with senses from some sense inventory or with
target-language translations; they posted significant improvements for
Chinese-English SMT on a variety of test sets and metrics.

There have been a number of related efforts in the SMT world recently.
Gimpel and Smith \cite{gimpel-smith:2008:WMT} build phrase table entries in
which the probabilities of target-language phrases are conditioned not only on
the source-language phrase, but also on features extracted from the broader
source-language context; their approach is ``omnivorous" in that it can include
any number of features based on source-language analysis. They tune feature
weights directly with MERT to produce a linear model for the contextual
probability of the target-language phrase. Features used here include the
n-grams in the surrounding context, the parts of speech of the focus word and
the surrounding context, a number of syntactic features based on a parse of
the input, and the focus phrase's approximate position in the sentence.

Mauser et al. \cite{mauser-hasan-ney:2009:EMNLP} present a similar approach,
although in their work they model translations of individual words
independently, rather than using phrases from the phrase table, and their
classifiers consider input sentences as sets of words. They argue that
phrase-based SMT models are already good at predicting sequences, and many of
the signals that might trigger a different word choice, especially in their
source language, Chinese, can be placed freely in the input text. They also
include a ``trigger-based" probability model similar to IBM Model 1
\cite{DBLP:journals/coling/BrownPPM94}, but conditioned on two source-language
words rather than one. Like IBM Model 1, this model is trained with
Expectation-Maximization.

Žabokrtsk\'{y} et al. \cite{vzabokrtsky-popel-marevcek:2010:WMT} use
discriminative MaxEnt models to adapt the ``forward" translation model
probabilities in an SMT system based on dependency grammars. While it has much
the same effect, this work does not describe itself in terms of word sense
disambiguation.

Recently, Tamchyna et al. -- a team that notably includes Marine Carpuat --
have integrated CL-WSD software into the popular open source Moses SMT system
\cite{tamchyna2014integrating}.
This approach does not use one classifier per source phrase, as had been done
in Carpuat's earlier work, but one enormous classifier for all known source
words, trained with the Vowpal Wabbit (VW) machine learning toolkit. This
approach allows generalizations to be learned across different source-language
phrases, which might be otherwise lost. The software is designed for speed and
is significantly faster than earlier architectures, they report.

\section{WSD for lower-resourced languages}
Scaling WSD techniques to new languages is a particularly thorny issue; the
resource acquisition bottleneck presents a problem even for English.
The problem is alleviated somewhat when we are interested in supervised CL-WSD,
with its reliance on bitext rather than sense-annotated corpora.

There has been some work on using WSD techniques for translation into
lower-resourced languages.  \v{S}pela et al.
\cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012} discuss applying WSD to
machine translation for the English-Slovene language pair, using a graph-based
WSD system and the linked Slovene wordnet to disambiguate English words to aid
in Slovene lexical selection.

Dinu and Kübler~\cite{Dinu07} presented work on monolingual WSD for Romanian.
They use memory-based learning for classification, with a small number of
contextual features. With all of its features included, the system
substantially outperformed the ``most frequent sense" baseline, but adding a
feature selection step caused a significant further improvement.

In other work on lower-resourced languages, Sarrafzdadeh et al. develop
a version of the Extended Lesk algorithm for Farsi \cite{sarrafzdadeh}.

One point that should be made about this dissertation is that it is ultimately
not about word-sense disambiguation for an under-resourced language.  Here we
are focusing on WSD for Spanish, for which we have very large data sets and a
number of good NLP tools. While our sense inventory happens to be extracted
from bitext corpora where the other languages involved are under-resourced, the
decisions to be made are about the meanings of Spanish words.

\section{CL-WSD at SemEval}
%% XXX
CL-WSD has received enough attention to warrant shared tasks at recent SemEval
workshops; the most recent running of the task is described by Lefever and
Hoste \cite{task10}.

Similar was the earlier task: \cite{lefever-hoste:2009:SEW}

Also the one for English-Hindi
\cite{chklovski-EtAl:2004:Senseval-3}

There was also the Writing Assistant task \cite{vangompel-EtAl:2014:SemEval},
which is related but not quite the same.

In the two CL-WSD tasks, participants were asked to translate twenty different
polysemous English nouns into five different European languages, in a variety
of contexts.

Some of the approaches explored included...

\section{Translation into Morphologically Rich Languages}
In Section \ref{sec:terere}, we describe a complete, if elementary, MT system
for the Spanish-Guarani language pair. As Guarani has a very rich morphology,
which we will need to predict and synthesize. As such, here we will quickly
review some of the work on morphological prediction and generation for
statistical MT systems.

%% XXX
Chris Dyer's recent paper at EMNLP
\cite{chahuneau:2013:emnlp}

Talk about prediction for morphology.
\cite{toutanova-suzuki-ruopp:2008:ACLMain}

Also factored models...
\cite{yeniterzi-oflazer:2010:ACL}

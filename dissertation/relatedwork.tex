\chapter{Related Work}
\label{chap:relatedwork}

In this chapter, I will review some of the recent related work, including
techniques for CL-WSD considered as a task on its own, the application of
classifiers to lexical selection in various machine translation architectures,
techniques used in recent runnings of the SemEval task on CL-WSD, and some
relevant approaches for word-sense disambiguation more broadly. The Chipa
software and the work described in the rest of the dissertation draws heavily
from ideas described here; how particular ideas have been influential will be
highlighted as we get to them.

\section{CL-WSD \emph{in vitro}}
Some of the related work on CL-WSD has studied the CL-WSD task in isolation, 
without being embedded into a machine translation system as such.
It is sensible to explore CL-WSD on its own for several reasons. Firstly, even
if our motivation for developing CL-WSD is to improve machine translation
output, from an engineering perspective, we still want to get a sense of
how well the individual components of our pipeline are functioning.
Furthermore, if the MT pipeline is relying on a CL-WSD system to help it with
lexical selection, one expects that improved disambiguation accuracy will
result in better translations overall -- though of course in complex software,
unexpected interactions may occur.

\subsection{Multilingual Evidence for CL-WSD}
One of the most interesting CL-WSD systems of recent years, which inspired our
use of multilingual evidence in Chipa, is ParaSense
\cite{lefever-hoste-decock:2011:ACL-HLT2011}, developed by Els Lefever and
colleagues.

ParaSense uses memory-based learning to predict target-language translations of
individual source-language words in a lexical sample task. It starts with
features that one would expect for a text classification problem: the surface
forms, POS tags, and lemmas for the focus word and the tokens in a small window
around it, as well as some syntactic information from a chunk parser. But
ParaSense also includes bag-of-words features for translations of the input
sentence into languages \emph{other} than the target language. ParaSense
handles translation from English into French, Spanish, Italian, Dutch and
German, so for example when translating from English to French, it includes
bag-of-words features extracted from the translations of the English sentence
into Spanish, Italian, Dutch and German (but not French).

Given corpora that are parallel over many languages, such as Europarl, this is
straightforward at training time. However, at testing time it is faced with a
novel sentence for which it has no translation. Here ParaSense requires a
complete MT system for each of the four other languages: it generates a
translation into the other four languages.
When ParaSense was being developed, it simply called out to the Google
Translate API (which is, as of this writing, no longer available gratis) to
generate the bag-of-words features required for test sentences.
This seems unwieldy and arguably detrimental to reproducible research: it
benefits from whatever techniques Google Translate uses for word choice,
which are not necessarily discussed with the public, and will change
without warning over time.

Critiques aside, ParaSense posted excellent results. When applied to the
SemEval 2010 CL-WSD task test data (described in
\cite{lefever-hoste:2010:SemEval}), it outperformed all of the systems
submitted for that task for nearly all of the target languages, being
outperformed, by a small margin, for one system when targeting Spanish
\cite{lefever-hoste-decock:2011:ACL-HLT2011}. A later version, described in
Lefever's dissertation, outperformed all of the SemEval systems.

In our earlier work, we prototyped a system that addresses some of the issues
with ParaSense \cite{rudnick-liu-gasser:2013:SemEval-2013}; these ideas are
developed further in Chapter \ref{chap:multilingual}.
Chipa requires neither a locally running MT system nor access to an online
translation API for its feature extraction, but still learns from several
mutually parallel bitexts that share a source language.

\section{WSD with Sequence Models}
To my knowledge, there has not been other work that explicitly frames all-words
CL-WSD as a sequence labeling problem. Machine translation systems, and
especially statistical machine translation systems, address this problem
implicitly -- jointly along with other problems -- when they use language
models to encourage coherent word choices in the generated text. The
decomposition of SMT systems into a translation model and a language model
allows LMs to be trained on very large target-language corpora.  In contrast,
in our earlier work on CL-WSD with sequence models, we only trained on
sentence-aligned bitext.

There have, however, been some uses of sequence models for monolingual WSD; in
this setting, systems can find an assignment disambiguating the entire input
jointly.  Molina et al. \cite{DBLP:conf/iberamia/MolinaPS02} have described the
use of hidden Markov models for WSD, situating all-words WSD as a tagging
problem.  They also used this approach in a Senseval entry
\cite{molina-pla-segarra:2004:Senseval-3} in the English all-words task at
Senseval-3, where it posted results towards the middle of the pack.

Ciaramita and Altun \cite{ciaramita-altun:2006:EMNLP} frame both coarse-grained
WSD and information extraction as a tagging problem, using a discriminative
variation of HMMs trained with perceptrons \cite{collins:2002:EMNLP02}
to maps nouns and verbs in the input text onto one of 41 ``supersenses", a
small ontology developed by Ciaramita and Johnson.

While it is not explicitly about sequence models, there is a related research
program underway in the world of graph-based algorithms for knowledge-based
word-sense disambiguation, exemplified by the work of Moro et al.
\cite{DBLP:journals/tacl/0001RN14}; they perform word-sense disambiguation and
entity linking jointly, searching for a globally coherent solution to both
problems.

\section{Lexical Selection in RBMT}
A purely rule-based approach to lexical selection -- a more declarative version
of the special-cased disambiguation procedures that Bar-Hillel imagined as
futile -- has been tried in many cases, with variants in both direct and
interlingual RBMT systems.
In one recent example, Bick presented a Danish-English RBMT system with purely
rule-based lexical selection \cite{bickdan2eng}. The Dan2eng system includes
roughly $17,000$ individual Constraint Grammar rules for lexical selection
alone, over $70\%$ of all of the rules written. Development took over two
years, and Bick suggests that statistical models (including target-language
LMs) might be used in the future to avoid having to cover all the
idiosyncrasies of lexical transfer.
A similar approach, though not with the same enormous number of lexical
selection rules, was taken by Brandt et al. \cite{brandt2011apertium}, with
their Apertium-based Icelandic-English MT system.

Francis Tyers, in his dissertation work \cite{tyers-dissertation}, develops a
practical approach for improving lexical selection for the Apertium RBMT
system when parallel corpora are available.
As he describes in \cite{tyers-fst}, his software learns finite-state
transducers for lexical selection from parallel corpora.
His dissertation expands on this, additionally learning, via maximum entropy,
weights with which different rules can be applied in different contexts.
He also describes experiments in which linguists wrote lexical selection rules
with the framework. Having human linguists write rules in this formalism can
improve MT output to an extent, but seems tedious, as it took a full day's work
for a linguist to write 156 rules, and this only covered lexical selection for
3\% of the thousand-sentence test corpus.

In this formalism, translations are selected via rules that can reference the
lexical items and parts of speech surrounding the focus word; when a rule
fires, it selects a translation. These rules are stored in an XML encoding, so
that they can be examined, modified, or written from scratch by human
linguists, but are then compiled to compact finite-state transducers for speedy
application.

Tyers' system is intended to be both very fast, for use in practical
translation systems, and to produce easily human-interpretable lexical
selection rules, for modification by linguists. It is currently in use for
some language pairs in the Apertium project.

\section{CL-WSD for Statistical Machine Translation}
While the idea has, for the most part, fallen by the wayside in recent
years,\footnote{There are a number of papers on the topic, but for the past
decade or so, the ``standard" model of phrase-based SMT has not included WSD as
a component. There are a just three paragraphs on the topic in Koehn's book on
SMT \cite[\S 5.3.6]{koehn2010statistical}, and the major open-source SMT
systems have not enabled WSD modules by default.}
some of the earliest work on statistical machine translation at IBM included
an explicit WSD module \cite{Brown91word-sensedisambiguation}.
Brown et al. considered the senses of a word to be its possible translations,
explicitly distancing their work from other WSD efforts that focused on
dictionary senses.
The WSD module described in the 1991 paper used binary classifiers similar to
decision stumps. Each focus word was allocated a single classifier, which could
check a single feature, e.g., the identity of the nearest noun to the right of
the focus word.
These simple classifiers would then bias the decoder to choose a
translation from one of two classes of target-language translations,
which often significantly reduced the decoder's uncertainty. The inclusion of
WSD improved the translation output markedly; the authors deemed 45 of the
output sentences ``acceptable" with WSD enabled, over only 37 without it.
Later versions of IBM's CANDIDE system included a more sophisticated
disambiguation system: contextually-tuned translation models based on
maximum entropy modeling for the most common words in the source language
\cite{Berger:1994:CSM:1075812.1075844}.

The idea of training classifiers on source-language context to help lexical
selection is an attractive one, and has resurfaced several times in the SMT
literature. However, by the early-to-mid 2000s, phrase-based SMT techniques
\cite{koehn2003statistical}, which translate short coherent chunks of text
rather than individual words and thus often get correct word choice ``for
free", seemed to make WSD modules unnecessary.
Work by Cabezas and
Resnik~\cite{cabezas2005using} and early attempts by Carpuat and
Wu~\cite{carpuat-wu:2005:ACL} cast doubt as to whether phrase-based SMT
systems, or even word-based SMT systems trained on large data sets, could
benefit from WSD; Cabezas and Resnik, though expressing enthusiasm for future
developments, did not manage to improve translation performance with WSD, and
initially (in 2005) Carpuat and Wu argued that SMT could be considered a
generalization of word-sense disambiguation. Vickrey et al.
\cite{vickrey-EtAl:2005:HLTEMNLP} noted that lexical selection to translate
phrases as units would be important for applications in MT, though they did not
implement it in their work on the topic.

Not long thereafter, Carpuat and Wu showed how to use classifiers to
improve modern phrase-based SMT systems with a series of papers on the topic
\cite{carpuatpsd,carpuat-wu:2007:EMNLP-CoNLL2007,carpuat2008evaluation,improvingsmtwsd}.
In this work, they show that in order for phrase-based SMT systems to benefit
from classifiers for lexical selection, the system should do
\emph{phrase}-sense disambiguation, rather than word sense. This is to say, the
classifiers should be used to select the translations of source-language
phrases from the corresponding target-language phrases, better matching the
model used by the rest of the pipeline.
Here Carpuat and Wu use an ensemble of classifiers, including naïve Bayes, a
maximum entropy model, an Adaboost classifier, and a nearest-neighbor model in
which feature vectors have been projected into a new space with a kernel and
PCA has been performed to reduce dimensionality.
They used features based on local word context, some syntactic features, and
the position of the focus phrase in the sentence. Their classifiers interact
with the decoder by modifying the phrase table just before decoding, adding
features usable by the decoder's log-linear model.
This approach outperformed the earlier attempts in which individual words had
been labeled either with senses from some sense inventory or with
target-language translations; they posted significant improvements for
Chinese-English SMT on a variety of test sets and metrics.

There have been a number of related efforts in the SMT world recently.
Gimpel and Smith \cite{gimpel-smith:2008:WMT} build phrase table entries in
which the probabilities of target-language phrases are conditioned not only on
the source-language phrase, but also on features extracted from the broader
source-language context; their approach is ``omnivorous" in that it can include
any number of features based on source-language analysis. They tune feature
weights directly with MERT to produce a linear model for the contextual
probability of the target-language phrase. Features used here include the
n-grams in the surrounding context, the parts of speech of the focus word and
the surrounding context, a number of syntactic features based on a parse of
the input, and the focus phrase's approximate position in the sentence.

Mauser et al. \cite{mauser-hasan-ney:2009:EMNLP} present a similar approach,
although in their work they model translations of individual words
independently, rather than using phrases from the phrase table, and their
classifiers consider input sentences as sets of words. They argue that
phrase-based SMT models are already good at predicting sequences, and many of
the signals that might trigger a different word choice, especially in their
source language, Chinese, can be placed freely in the input text. They also
include a ``trigger-based" probability model similar to IBM Model 1
\cite{DBLP:journals/coling/BrownPPM94}, but conditioned on two source-language
words rather than one. Like IBM Model 1, this model is trained with
Expectation-Maximization.

Žabokrtsk\'{y} et al. \cite{vzabokrtsky-popel-marevcek:2010:WMT} use
discriminative MaxEnt models to adapt the ``forward" translation model
probabilities in an SMT system based on dependency grammars. While it has much
the same effect, this work does not describe itself in terms of word sense
disambiguation.

Recently, Tamchyna et al. -- a team that notably includes Marine Carpuat --
have integrated CL-WSD software into the popular open source Moses SMT system
\cite{tamchyna2014integrating}.
This approach does not use one classifier per source phrase, as had been done
in Carpuat's earlier work, but one enormous classifier for all known source
words, trained with the Vowpal Wabbit (VW) machine learning toolkit. This
approach allows generalizations to be learned across different source-language
phrases, which might be otherwise lost. The software is designed for speed and
is significantly faster than earlier architectures, they report.

\section{WSD for lower-resourced languages}
Scaling WSD techniques to new languages is a particularly thorny issue; the
resource acquisition bottleneck presents a problem even for English.
The problem is alleviated somewhat when we are interested in supervised CL-WSD,
with its reliance on bitext rather than sense-annotated corpora.

There has been some work on using WSD techniques for translation into
lower-resourced languages.  \v{S}pela et al.
\cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012} discuss applying WSD to
machine translation for the English-Slovene language pair, using a graph-based
WSD system and the linked Slovene wordnet to disambiguate English words to aid
in Slovene lexical selection.

Dinu and Kübler~\cite{Dinu07} presented work on monolingual WSD for Romanian.
They use memory-based learning for classification, with a small number of
contextual features. With all of its features included, the system
substantially outperformed the ``most frequent sense" baseline, but adding a
feature selection step caused a significant further improvement.

In other work on lower-resourced languages, Sarrafzdadeh et al. develop
a version of the Extended Lesk algorithm for Farsi \cite{sarrafzdadeh},
Lesk algorithms (due to early work by Michael Lesk \cite{lesk}) being a
knowledge-based approach to WSD that relies on dictionaries, counting the
occurrences of words that occur in a particular sense's dictionary definition;
there have been many variations on this theme.

One point that should be made about this dissertation is that it is ultimately
not about word-sense disambiguation for an under-resourced language.  Here we
are focusing on WSD for Spanish, for which we have very large data sets and a
number of good NLP tools. While our sense inventory happens to be extracted
from bitext corpora where the other languages involved are under-resourced, the
decisions to be made are about the meanings of Spanish words.

\section{CL-WSD at Senseval and SemEval}
There have been many related Senseval and SemEval shared tasks, covering
different variations on cross-lingual WSD.

%% 2001 senseval2
Senseval-2 featured a Japanese Translation Task \cite{kurohashi:2001:SENSEVAL},
in which Japanese words were to be labeled with their most appropriate English
translations from the provided translation memory (TM); the task thus combined
aspects of word sense disambiguation and example-based machine translation.

At Senseval-3, there was an English-Hindi translation task
\cite{chklovski-EtAl:2004:Senseval-3}, in which participants were to label a
lexical sample of English words with their Hindi translations. Here the gold
standard translations were created collaboratively online, using the Open Mind
Word Expert software developed by Chklovski and Mihalcea, but the set of
possible translations were extracted from a bilingual dictionary. There were
two subtasks provided; in the more general subtask, systems were provided with
unannotated text in English. For the ``translation-and-sense" subtask, the
provided text was previously-used Senseval-2 data, already annotated with its
WordNet senses. These annotations provided a significant boost to
classification accuracy for the participating systems.

%% 2007 semeval
In SemEval 2007, there were two related tasks covering translation between
English and Chinese. Task 5 \cite{jin-wu-yu:2007:SemEval-2007} was a lexical
sample task in which Chinese texts were manually annotated with English
translations from a predefined dictionary, and participants had to predict
these labels. Task 11 \cite{ng-chan:2007:SemEval-2007}, in contrast, had its
training and test data gathered from automatically word-aligned parallel
corpora, and the direction of translation was from English to Chinese.

%% 2010 semeval
%% 2013 semeval
In the 2010 and 2013 SemEval CL-WSD tasks
\cite{lefever-hoste:2010:SemEval,task10}, both organized by Lefever and Hoste,
participants were asked to translate twenty different polysemous English nouns
into five different European languages (Dutch French, German Italian, and
Spanish), given their containing source-language sentence as context. There
were no explicit bilingual dictionaries provided as sense inventories; the
possible target-language labels were those found in Europarl \cite{europarl}
through automatic word alignment, though they were lemmatized by hand. The test
set was manually annotated with the appropriate (lemmatized) translations from
the sense inventory for each of the five target languages.

%% 2014 semeval
Most recently, for SemEval 2014, there was a variant on the CL-WSD task: the
Writing Assistant task \cite{vangompel-EtAl:2014:SemEval}. In this setting,
rather than complete source-language sentences, short source-language text
segments were included in nearly complete target-language sentences,
representing the task faced by an agent assisting a writer trying to compose
sentences in a second language but facing gaps in their vocabulary.

\section{Translation into Morphologically Rich Languages}
In Section \ref{sec:terere}, we describe a complete, if elementary, MT system
for the Spanish-Guarani language pair. As Guarani has a very rich morphology,
which we will need to predict and synthesize. As such, here we will quickly
mention some of the work on morphological prediction and generation for
statistical and hybrid MT systems.

Koehn and Hoang describe \emph{factored} phrase-based SMT models for predicting
target-language morphology \cite{koehn-hoang:2007:EMNLP-CoNLL2007}. In this
kind of approach, instead of having one translation model containing surface
forms, there can be several different translation models. As a basic strategy,
they suggest having separate models for the translation of lemmas and another
for translating parts-of-speech and morphological features. This approach
allows the MT system to learn more generalized models, both for the translation
of lemmas and for mapping the morphological features of the source language to
the target.
Yeniterzi and Oflazer \cite{yeniterzi-oflazer:2010:ACL} describe an
application of this model for SMT on English-Turkish.
They situate their work in terms of the earlier strategy of considering Turkish
inflectional and derivational morphemes as ``words" to be handled by the SMT
system as normal and then recombined in a post-processing step, which would
sometimes fail when the morphemes were not generated in the correct order.
Contrastingly, their factored model produces Turkish lemmas, then finds the
relevant inflectional features based on English syntax.

Toutanova et al. describe another technique for generating rich morphology in
SMT \cite{toutanova-suzuki-ruopp:2008:ACLMain}. In their model, they consider
inflection prediction separately from the task of producing the target-language
text. One version of their system produces fully-inflected text, which may be
changed by their inflection-prediction system; the other produces uninflected
target-language text, relying on the morphology prediction. In either case, the
morphology of all of the target-language lemmas is predicted with a Maximum
Entropy Markov Model (MEMM), a discriminative sequence prediction model that
can take into account both any features extracted from the analysis of the
source sentence and its own previous morphology predictions. They apply this
model to both English-Russian and English-Arabic translation, posting
improvements for both phrase-based and tree-based SMT.

Chahuneau et al. have recently presented yet another method for SMT into
morphologically rich languages \cite{chahuneau-EtAl:2013:EMNLP}. Here they
also uses discriminative models to predict target-language inflections based on
the available source-language annotations, but instead of inflecting the text
after the decoder has been run, their system generates new phrase-table entries
with the appropriate inflections included, just before the decoder executes,
allowing the decoder to optimize for many concerns jointly, informed by the
language model. Their software is described in more detail in Schlinger et al.
\cite{DBLP:journals/pbml/SchlingerCD13}, and has had an open-source release.

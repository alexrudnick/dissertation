\chapter{Integration Into Machine Translation Systems}
\label{chap:integration}

This can take a lot of text from the SALTMIL paper.


Concretely, we will apply the CL-WSD techniques we develop to a number of
different MT systems with different designs, translating several different
language pairs.  These will include, at least:
\begin{itemize}
\item Tereré, a hybrid SCFG-based system that makes use of both bilingual
transfer rules and a monolingual language model, translating from Spanish to
Guarani.
\item SQUOIA, a classic transfer-based system translating Spanish to Quechua.
\end{itemize}

We would also like to integrate Chipa into:

\begin{itemize}
\item L3, a more sophisticated RBMT system based on constraint solving and
synchronous dependency grammars 
\item Apertium, a system based on shallow transfer, which has been
applied to a large number of language pairs.
\end{itemize}


\section{Tereré}
\label{sec:terere}
Since we hope that our ideas about lexical selection will make sense in several
different contexts, we will develop a new machine translation system out of
open-source SMT components, particularly relying on the cdec decoder and its
associated tools.
\cite{dyer-EtAl:2010:Demos}


This new system is called ``Tereré"
\footnote{\url{http://github.com/alexrudnick/terere}; 
Tereré is a cold variety of yerba mate brewed with ice water; it is a
specifically Paraguayan specialty.}.
Tereré will make use of modern hybrid MT techniques; our current design is
fairly similar to the Stat-XFER approach \cite{DBLP:conf/cicling/Lavie08}
developed by researchers at CMU.
Like Stat-XFER, Tereré will make use of bilingual transfer rules, a lexical
transfer stage, a target-language LM, and statistical decoding.

While the initial transfer rules will likely be written by hand, based the
respective grammars of Spanish and Guarani, we may also include
automatically-extracted rules, perhaps via Thrax \cite{weese-EtAl:2011:WMT} or
a forthcoming tool for extracting Inversion Transduction Grammars, from Dekai
Wu's team at HKUST \cite{saers-addanki-wu:2013:HyTra}.
The use of automatically-extracted transfer rules would make the system more
similar to the SAMT approaches of Zollmann and Venugopal
\cite{zollmann-venugopal:2006:WMT}.

In order to integrate our CL-WSD systems into Tereré, we will automatically
produce a SCFG rules just before decoding, in which features that encodes the
preferences of the WSD system are added to each lexical transfer rule. Then
the weights for all of the features provided to the system (translation
probabilities, LM scores, CL-WSD scores, and perhaps others) can be tuned with
MERT \cite{och:2003:ACL}, and the decoder will use these to search the space of
licensed translations.

We will approach the rich morphology of Guarani and the associated data
sparsity by having the system produce uninflected Guarani stems, which we will
then inflect in a second pass.
In the second pass, we will predict the appropriate morphological features will
with a discriminative sequence-labeling approach based on work at Microsoft
Research \cite{toutanova-suzuki-ruopp:2008:ACLMain}.
Thus both the transfer rules and the language model will be in terms of stemmed
Guarani.
As an alternative, we could adapt the techniques in
\cite{chahuneau:2013:emnlp} to generate translation rules that contain the
appropriately inflected target forms, just before running the decoder.
Rule-based approaches may also be sensible for generating Guarani morphology,
in some cases, and these will have to be investigated. In any case, once the
appropriate morphological features have been predicted, surface forms of
Guarani words will be generated with the FST-based morphological analyzer and
generator developed by Michael Gasser and described in
\cite{rudnick-gasser:2013:HyTra-2013}.

\section{SQUOIA}
SQUOIA\footnote{Described in detail at
\url{http://www.cl.uzh.ch/research/maschinelleuebersetzung/hybridmt_en.html}; 
Code available at \url{http://code.google.com/p/squoia/}}
is a project for MT from Spanish to Quechua, another relatively large
indigenous American language. SQUOIA is being developed by a team at the
University of Zurich. For the most part, it is a classical rule-based transfer
system, although the team has recently developed techniques for predicting verb
morphology with machine learning methods, in cases when rules cannot reliably
disambiguate \cite{riosgonzales-gohring:2013:HyTra}. It does not currently use
machine learning for lexical selection, although we have been in contact with
the team and are planning to collaborate to add CL-WSD features.

SQUOIA's architecture is based on the Matxin system \cite{matxin2005}, which
was originally intended for translating from Spanish to Basque.
It consists of a pipeline of scripts, each of which passes along a tree
describing the current input sentence, in XML form. Adding CL-WSD to this
system will involve adding another script somewhere in the pipeline that
extracts features from the current annotated sentence, makes lexical choices,
and then passes these choices to subsequent scripts.

\section{L3}
L3 is an RBMT system based on synchronous dependency grammars and constraint
solving \cite{gasser:sxdg,gasser:aflat2012}.  It makes use of syntactic
knowledge about the source and target languages and can also include abstract
semantic representations as an intermediate stage in processing. Notably, L3
does not use a pipeline architecture: all of the constraints about the source
syntax, target syntax, semantic representation, and their relationships are
instantiated in one step, then solved jointly. L3 integrates morphological
analysis and generation for use in translating morphologically rich languages,
such as Guarani and Ethiopian semitic languages;
the morphological analyzers and generators to be used in Tereré were originally
developed for L3.

However, the constraints in L3 are currently unweighted, and it could use a way
to rank the licensed translations of an input sentence. It faces syntactic and
lexical ambiguity both in its analysis of the input sentence and in the
construction of output sentences. Ideally, a good lexical selection module
would constrain its other choices, yielding the higher-quality translations
first.

\section{Apertium}
%%Apertium\footnote{\url{http://www.apertium.org/}} \cite{Forcada_theapertium} is
another transfer-based RBMT system, originally designed for translating between
the languages of Spain but now handling over thirty language pairs. Some
language pairs are quite mature, while others are in prototype stages.

Apertium is a ``shallow transfer" system, meaning that it does not depend on
complete syntactic analysis of input sentences, but typically works by
chunking.
In his dissertation work (\cite{tyers-fst} \cite{tyers-dissertation}), Francis
Tyers has
been developing a new lexical selection system for Apertium, one that learns
lexical selection rules in the form of finite-state transducers from available
bitext. These rules are also human readable and editable, which seems like a
useful feature so that users can debug and tweak a translation system as
desired.
Tyers' lexical selection system is a strong baseline against which we should
compare our new CL-WSD approaches, and time and ingenuity permitting, we would
also like to integrate our system into Apertium.


\section{Integrating into SQUOIA}

Lexical ambiguity is a significant problem facing rule-based machine
translation systems, as many words have several possible translations in a
given target language, each of which can be considered a sense of the word from
the source language.
The difficulty of resolving these ambiguities is mitigated for 
statistical machine translation systems for language pairs with large bilingual
corpora, as large n-gram language models and phrase tables containing common
multi-word expressions can encourage coherent word choices.
For most language pairs these resources are not available, so a primarily
rule-based approach becomes attractive.
In cases where some training data is available, though, we can
investigate hybrid RBMT and machine learning approaches, leveraging small and
potentially growing bilingual corpora. In this paper we
describe the integration of statistical cross-lingual word-sense disambiguation
software with SQUOIA, an existing rule-based MT system for the Spanish-Quechua
language pair, and show how it allows us to learn from the available bitext to
make better lexical choices, with very few code changes to the base system. We
also describe Chipa, the new open source CL-WSD software used for these
experiments.



\section{Introduction}
Here we report on the development of Chipa, a package for statistical
lexical selection, and on integrating it into
SQUOIA,\footnote{\url{http://code.google.com/p/squoia/}} a primarily rule-based
machine translation system for the Spanish-Quechua language pair.  With very
few code changes to SQUOIA, we were able to make use of the lexical suggestions
provided by Chipa.

The integration enables SQUOIA to take advantage of any available bitext
without significantly changing its design, and to improve its word choices as
additional bitext becomes available. Our initial experiments also suggest that
we are able to use unsupervised approaches on monolingual Spanish text to
further improve results.

In this paper, we describe the designs of the Chipa and SQUOIA systems, discuss
the data sets used, and give results on both how well Chipa is able to learn
lexical selection classifiers in isolation, and to what extent it is able to
improve the output of SQUOIA on a full Spanish-to-Quechua translation task.

In its current design, SQUOIA makes word choices based on its bilingual
lexicon; the possible translations for a given word or multi-word expression
are retrieved from a dictionary on demand. If there are several possible
translations for a lexical item, these are passed along the pipeline so
that later stages can make a decision, but if the ambiguity persists,
then the first entry retrieved from the lexicon is selected. While there are
some rules for lexical selection, they have been written by hand and only cover
a small subset of the vocabulary in a limited number of contexts.

In this work, we supplement these rules with classifiers learned from
Spanish-Quechua bitext. These classifiers make use of regularities that may not
be obvious to human rule-writers, providing improved lexical selection for
any word type that has adequate coverage in the training corpus.

Quechua is a group of closely related indigenous American languages spoken in
South America. There are many dialects of Quechua; SQUOIA focuses on the
Cuzco dialect, spoken around the Peruvian city of Cuzco.  Cuzco Quechua has
about 1.5 million speakers and some useful available linguistic resources,
including a small treebank \cite{rios2009quechua}, also produced by the SQUOIA
team.

\section{SQUOIA}
SQUOIA is a deep-transfer RBMT system based on the
architecture of MATXIN \cite{matxin2005,matxin}.
The core system relies on a classical transfer approach and is mostly
rule-based, with a few components based on machine learning.
SQUOIA uses a pipeline approach, both in an abstract architectural sense and in
the sense that its pieces are instantiated as a series of scripts that communicate
via UNIX pipes. Each module performs some transformation on its input and
passes along the updated version to the next stage. Many modules focus on very
particular parts of the representation, leaving most of their input unchanged.

In the first stages, Spanish source sentences are analyzed with off-the-shelf
open-source NLP tools. To analyze the input Spanish text,
SQUOIA uses FreeLing \cite{padro12} for morphological analysis and named-entity
recognition,
Wapiti \cite{lavergne2010practical} for tagging,
and DeSr \cite{attardi-EtAl:2007:EMNLP-CoNLL2007} for parsing.
All of these modules rely on statistical models.

In the next step, the Spanish verbs must be disambiguated in order to assign
them a Quechua verb form for generation: a rule-based module tries to assign a
verb form to each verb chunk based on contextual information. If the rules fail to
do so due to parsing or tagging errors, the verb is marked as ambiguous and
passed on to an SVM classifier, which assigns a verb form even if the context
of that verb does not unambiguously select a target form. This is among the
most difficult parts of the
translation process, as the grammatical categories encoded in verbs differ
substantially between Spanish and Quechua. In the next step, a lexical transfer
module inserts all possible translations for every word from a bilingual dictionary.
Then a set of rules disambiguates the forms with lexical or morphological
ambiguities. However, this rule-based lexical disambiguation is very limited,
as it is not feasible to cover all possible contexts for every ambiguous word
with rules.

The rest of the system makes use of a classical transfer procedure. A following module
moves syntactic information between the nodes and the chunks in the tree, and
finally, the tree is reordered according to the basic word order in the target
language. In the last step, the Quechua surface forms are morphologically
generated through a finite state transducer.

\section{CL-WSD with Chipa}
Chipa is a system for cross-lingual word sense disambiguation (CL-WSD).
\footnote{Chipa the software is named for chipa the snack food, popular in many
  parts of South America. It is a cheesy bread made from cassava flour, often
  served in a bagel-like shape in Paraguay.  Also \emph{chipa} means 'rivet,
  bolt, screw' in Quechua, something for holding things together.  The software
is available at \\ \url{http://github.com/alexrudnick/chipa} under the GPL.} By
CL-WSD, we mean the problem of assigning labels to polysemous words in
source-language text, where each label is a word or phrase type in the target
language.


\subsection{System Integration}
In order to integrate Chipa into SQUOIA, we added an additional lexical
selection stage to the SQUOIA pipeline, occurring after the rule-based
disambiguation modules. This new module connects to the Chipa server to request
translation suggestions -- possibly several per word, ranked by their
probability estimates -- then looks for words that SQUOIA currently has marked
as ambiguous.

For each word with multiple translation possibilities, we consider each of the
translations known to SQUOIA and take the one ranked most highly in the
results from the classifiers. If there are no such overlapping translations, we
take the default entry suggested by SQUOIA's dictionary.
Notably, since Chipa and SQUOIA do not share the same lexicon and bitext alignments
may be noisy, translations
observed in the bitext may be unknown to the SQUOIA system, and lexical entries in the
SQUOIA dictionary may not be attested in the training data.







\section{Experiments}
Here we report on two basic experimental setups, including an \emph{in-vitro}
evaluation of the CL-WSD classifiers themselves and an \emph{in-vivo}
experiment in which we evaluate the translations produced by the SQUOIA system
with the integrated CL-WSD system.

\subsection{Classification Evaluation}
To evaluate the classifiers in isolation, we produced a small Spanish-Quechua
bitext corpus from a variety of sources, including the Bible, some government
documents such as the constitution of Peru and several short folktales and
works of fiction. The great majority of this text was the Bible.
We used Robert Moore's sentence aligner \cite{DBLP:conf/amta/Moore02}, with the
default settings to get sentence-aligned text.
Initially there were just over 50 thousand sentences; 28,549 were included
after sentence alignment.

\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|c|}
    \hline
    system                    & \multicolumn{5}{|l|}{accuracy} \\
    \hline
    MFS baseline              &  \multicolumn{5}{|l|}{54.54} \\
    chipa, only word features &  \multicolumn{5}{|l|}{65.43} \\
    \hline
           & $C=100$ & $C=200$ & $C=500$ & $C=1000$ & $C=2000$ \\
    \hline
    chipa, +clusters from training bitext &
    66.71 & 67.43 & 68.41 & 69.00 & 69.43 \\
    chipa, +clusters from europarl        &
    66.60 & 67.18 & 67.83 & 68.25 & 68.58 \\
    \hline
  \end{tabular}
  \end{center}
\caption{Results for the \emph{in-vitro} experiment; classification accuracies
over tenfold cross-validation including null-aligned tokens, as percentages. }
\label{fig:integrationresults1}
\end{figure*}

\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|c|}
    \hline
    system                    & \multicolumn{5}{|l|}{accuracy} \\
    \hline
    MFS baseline              &  \multicolumn{5}{|l|}{53.94} \\
    chipa, only word features &  \multicolumn{5}{|l|}{68.99} \\
    \hline
           & $C=100$ & $C=200$ & $C=500$ & $C=1000$ & $C=2000$ \\
    \hline
    chipa, +clusters from training bitext &
    71.53 & 72.62 & 73.88 & 74.29 & 74.78 \\
    chipa, +clusters from europarl        &
    71.27 & 72.08 & 73.04 & 73.52 & 73.83 \\
    \hline
  \end{tabular}
  \end{center}
\caption{Classification accuracies over tenfold cross-validation, excluding
null-aligned tokens.}
\label{fig:integrationresults2}
\end{figure*}


During preprocessing, Spanish multi-word expressions identifiable with FreeLing
were replaced with special tokens to mark that particular expression, and both
the Spanish and Quechua text were lemmatized. We then performed word-level
alignments on the remaining sentences with the Berkeley aligner
\cite{denero-klein:2007:ACLMain}, resulting in one-to-many alignments such that
each Spanish word is aligned to zero or more Quechua words, resulting in a
label for every Spanish token.

With this word-aligned bitext, we can then train and evaluate classifiers.
We evaluate here classifiers for the 100 most common Spanish lemmas appearing
in the aligned corpus. For this test, we performed 10-fold cross-validation for
each lemma, retrieving all of the instances of that lemma in the corpus,
extracting the appropriate features, training classifiers, then testing on
that held-out fold.

We report on two different scenarios for the \emph{in-vitro} setting; in one
case, we consider classification problems in which the word in question may be
aligned to NULL, and in the other setting, we exclude NULL alignments. While
the former case will be relevant for other translation systems, in the
architecture of SQUOIA, lexical selection modules may not make the decision to
drop a word. In both cases, we show the average classification accuracy across
all words and folds, weighted by the size of each test set.

Here we compare the trained classifiers against the ``most-frequent sense"
(MFS) baseline, which in this setting is the most common translation for a
given lemma, as observed in the training data.

We additionally show the effects on classification accuracy of adding features
derived from Brown clusters, with clusters extracted from both the Europarl
corpus and the Spanish side of our training data.
We tried several different
settings for the number of clusters, ranging from $C=100$ to $C=2000$.
In all of our experimental settings, the addition of Brown cluster features
substantially improved classification accuracy. We note a consistent upward
trend in performance as we increase the number of clusters, allowing the
clustering algorithm to learn finer-grained distinctions.
The training algorithm takes time quadratic in the number of clusters,
which becomes prohibitive fairly quickly, so even finer-grained distinctions
may be helpful, but will be left to future work. On a modern Linux
workstation, clustering Europarl (~2M sentences) into 2000 clusters took
roughly a day.

The classifiers using clusters extracted from the Spanish side of our bitext
consistently outperformed those learned from the Europarl corpus. We had an
intuition that the much larger corpus (nearly two million sentences) would
help, but the clusters learned in-domain, largely from the Bible, reflect
usage distinctions in that domain. Here we are in fact cheating slightly, as
information from the complete corpus is used to classify parts of that corpus.

Figures \ref{fig:theresults1} and \ref{fig:theresults2} show 
summarized results of these first two experiments.

\subsection{Translation Evaluation}
In order to evaluate the effect of Chipa on lexical selection in a live
translation task, we used SQUOIA to translate two Spanish passages for which we
had reference Quechua translations. The first is simply a thousand sentences
from the Bible; the second is adapted from the Peruvian government's public
advocacy website,\footnote{\emph{Defensoría del Pueblo},
\url{http://www.defensoria.gob.pe/quechua.php}} which is bilingual and
presumably contains native-quality Quechua. We collected and hand-aligned
thirty-five sentences from this site.

Having prepared sentence-aligned and segmented bitexts for the evaluation,
we then translated the Spanish side with SQUOIA, with various CL-WSD settings
to produce Quechua text. In comparing the output Quechua with the reference
translations, BLEU scores were quite low. The output often contained no 4-grams
that matched with the reference translations, resulting in a geometric mean of
0. So here we report on the unigram-BLEU scores, which reflect some small
improvements in lexical choice.
See Figure \ref{fig:translatioresults} for the numerical results.

\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|c|c|}
    \hline
    system                           & web test set & bible test set  \\
    \hline
    squoia without CL-WSD            & 28.1         & 24.2            \\
    squoia+chipa, only word features & 28.1         & 24.5            \\
    squoia+chipa, +europarl clusters & 28.1         & 24.5            \\
    squoia+chipa, +bible    clusters & 28.1         & 24.5            \\
    \hline
  \end{tabular}
  \end{center}
  \caption{BLEU-1 scores (modified unigram precision) for the various CL-WSD
  settings of SQUOIA on the two different Spanish-Quechua test sets.}
\label{fig:translatioresults}
\end{figure*}

On the web test set, unfortunately very few of the Spanish words used were both
considered ambiguous by SQUOIA's lexicon and attested in our training corpus.
Enabling Chipa during translation, classifiers are only called on six of the
thirty-five sentences, and then the classifiers only disagree with the default
entry from the lexicon in one case.

We do see a slight improvement in lexical selection when enabling Chipa on the
Bible test set; the three feature settings listed actually all produce
different translation output, but they are of equal quality. Here the in-domain
training data allowed the classifiers to be used more often; 736 of the
thousand sentences were influenced by the classifiers in this test set.

\section{Related Work}
Rios and G\"{o}hring \cite{riosgonzales-gohring:2013:HyTra} describe
earlier work on extending the SQUOIA MT system with machine learning modules.
They used classifiers to predict the target forms of verbs in cases where the
system's hand-crafted rules cannot make a decision based on the current
context.

\section{Conclusions and Future Work}
We have described the Chipa CL-WSD system and its integration into SQUOIA,
a machine translation system for Spanish-Quechua.
Until this work, SQUOIA's lexical choices were based on a small number of
hand-written lexical selection rules, or the default entries in a bilingual
dictionary. 

We have provided a means by which the system can make some use of
the available training data, both bilingual and monolingual, with very few
changes to SQUOIA itself. We have also shown how Brown clusters, either when
learned from a large out-of-domain corpus or from a smaller in-domain corpus,
provide useful features for a CL-WSD task, substantially improving
classification accuracy.

In order make better use of the suggestions from the CL-WSD module, we may
need to expand the lexicon used by the translation system, so that mismatches
between the vocabulary of the available bitext, the translation system itself,
and the input source text do not hamper our efforts at improved lexical
selection. Finding more and larger sources of bitext for this language pair
would of course help immensely.

We would like to learn from the large amount of monolingual Spanish text
available; while the Europarl corpus is nontrivial, there are much larger
sources of Spanish text, such as the Spanish-language Wikipedia. We plan
to apply more clustering approaches and other word-sense discrimination
techniques to these resources, which will hopefully further improve CL-WSD
across broader domains.

Better feature engineering outside of unsupervised clusters may also be useful.
In the future we we will extract features from the already-available POS tags
and the syntactic structure of the input sentence.

We also plan to apply the Chipa system to other machine translation systems and
other language pairs, especially Spanish-Guarani, another important language
pair for South America.

\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work, our target languages are under-resourced, we have many
language resources available for the source language. We would like to use
these to make better sense of the input text, giving our classifiers clearer
signals for lexical selection in the target language. The approaches considered
in this chapter, in general, make additional features available to the CL-WSD
classifiers.

For Spanish, perhaps most saliently, we have abundant monolingual text
available to us.
Given large amounts of Spanish-language text, we can use
unsupervised methods to discover regularities that will give additional
purchase to our classifiers.
In this work we apply Brown clustering
\cite{brown1992class}, which has been used successfully in a variety
of text classification tasks \cite{turian-ratinov-bengio:2010:ACL} and provides
a straightforward mechanism to add annotations to our source-language text.
We also try learning neural word embeddings (XXX citation needed), which map
from individual word types to high-dimensional spaces in which similar word
types are in some sense clustered. %% XXX explain better
% we can also try topic models (say with MALLET)


In addition to large quantities of monolingual text, we have available to us
excellent off-the-shelf NLP tools such as POS taggers and syntactic parsers,
which can be used to

In this chapter, we want to write about all of the things that we can do with
just monolingual data in the source language.
This is probably going to be one of the most important chapters, and really one
of the nicest results in the dissertation.
We can probably integrate some text from the SALTMIL paper into this chapter.


\section{Brown Clustering}
The Brown clustering algorithm takes as input unannotated text and produces a
mapping from word types in that text to clusters, such that words in the same
cluster have similar usage patterns according the corpus's bigram statistics.
We can then use this mapping from words to clusters in our classifiers, adding
an additional annotation for each word that allow the classifiers to find
higher-level abstractions than surface-level words or particular lemmas.
The desired number of clusters must be set ahead of time, but is a tunable
parameter.
We use a popular open source implementation of Brown clustering,
\footnote{\url{https://github.com/percyliang/brown-cluster}} described by
Liang \cite{Liang05semi-supervisedlearning}, running on both the Spanish
side of our bitext corpus and on the Europarl corpus \cite{europarl} for
Spanish.

%% XXX Write about how Brown clustering works
%% XXX it's hierarchical too, so we don't have to use the leaf clusters...

So far, we have really good evidence that we can learn, in an unsupervised way,
with Brown Clustering coarse-grained categories for word types in the source
language that help us make lexical selection distinctions.

There are actually several open questions here, though:

\begin{itemize}
  \item Does more source-language text help us learn ``better" Brown clusters,
    with respect to the CL-WSD task? It seems like it would, but we haven't
    demonstrated that yet.
  \item What's the relationship between the genre of the monolingual text we
    use and the source-language side of the test set? It seems to help to have
    in-domain text to train our clusters, but is there a size at which you
    learn a ``better" clustering that overtakes the in-domain-ness?
  \item We tried clustering on (lemmatized) Europarl in the es-qu experiments for SALTMIL...
  \item Does it matter if you lemmatize the Spanish first, or not? It seems
    like the point of Brown Clustering is that it's picking up on syntax in
    some sense.
  \item Similar open questions for word2vec clusters.
\end{itemize}

It would be good to include some nice examples of the clusters that Brown
Clustering learns, based on the different source-language corpora that we
cluster on.


\begin{figure*}[t!]
  \begin{tabular}{|r|p{10cm}|}
    \hline
    category  & top twenty word types by frequency \\
    \hline
    countries & francia irlanda alemania grecia italia españa rumanía portugal polonia suecia bulgaria austria finlandia hungría bélgica japón gran\_bretaña dinamarca luxemburgo bosnia \\
    \hline
    more places & kosovo internet bruselas áfrica iraq lisboa chipre afganistán estrasburgo oriente\_próximo copenhague asia chechenia gaza oriente\_medio birmania londres irlanda\_del\_norte berlín barcelona \\
    \hline
    mostly people & hombre periodista jefes\_de\_estado individuo profesor soldado abogado delincuente demócrata dictador iglesia alumno adolescente perro chico economista gato jurista caballero bebé \\
    \hline
    infrastructure & infraestructura vehículo buque servicio\_público cultivo edificio barco negocio motor avión monopolio planta ruta coche libro aparato tren billete actividad\_económica camión \\
    \hline
    common verbs & pagar comprar vender explotar practicar soportar exportar comer consumir suministrar sacrificar fabricar gobernar comercializar cultivar fumar capturar almacenar curar beber \\
    \hline
  \end{tabular}
\caption{Some illustrative clusters found by the Brown clustering algorithm on
the Spanish Europarl data. These are five out of $C=1000$ clusters, and
were picked and labeled by hand. The words listed are the
top twenty terms from that cluster, by frequency.}
\label{fig:clusters}
\end{figure*}

Figure \ref{fig:clusters} shows some illustrative examples of clusters that
we found in the Spanish Europarl corpus.  Examining the output of the
clustering algorithm, we see some intuitively satisfying results; there are
clusters corresponding to the names of many countries, some nouns referring to
people, and common transitive verbs. Note that the clustering is unsupervised,
and the labels given are not produced by the algorithm.

\subsection{Maybe explain how Brown Clustering works here?}
Not sure how important this is to the argument...


\section{Other Unsupervised Word Representations}
We can try applying the other stuff in the Turian et al Word Representations
paper. Such as word2vec \cite{mikolovword2vec}

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
word2vec results in high-dimensional representations for each word.

Open question here:
Can we reproduce that ``Learning to Understand Phrases by Embedding the
Dictionary" paper too? Could we learn representations for the cross-language
case too?

Definitely try using word2vec. We want to train word2vec on Spanish wikipedia,
and maybe that's going to give us useful features.

\section{Topic models with MALLET}

\section{Monolingual features from other NLP tools}
In the Chipa software, we can make use of all kinds of annotations for our
text (see Section \ref{sec:annotations}), so adding more features is
straightforward.

Notably, we can use POS tags, features extracted from parsers... we could even
use sense annotations from a monolingual WSD system, if we had a good one on
hand.

For POS tagging and parsing, we run the open source FreeLing text analysis
suite \cite{padro12} on both the training data and the text input sentences.
FreeLing can perform a number of analysis tasks for Spanish, including
lemmatization, POS tagging, named entity recognition, and dependency parsing.

\section{Experiments}
Here we repeat the experiments from Chapter \ref{chap:evaluation}
As described in the previous

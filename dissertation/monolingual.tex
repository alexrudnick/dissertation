\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work, our target languages are under-resourced, we have many
resources available for the source languages. We would like to use these to
make better sense of the input text, giving our classifiers clearer signals for
lexical selection in the target language.
The approaches considered in this chapter make additional features available to
the CL-WSD classifiers, based on our knowledge of the source language.
In short, since we have relatively little bitext available for Spanish-Guarani
and Spanish-Quechua, we will need to lean more on our resources, including
tools and data, for Spanish so that we can better make sense of the input text.

Perhaps most saliently for Spanish, we have abundant monolingual text
available. Given large amounts of Spanish-language text, we can use
unsupervised methods to discover useful regularities and thus additional
features for our text classification task. This has been a broadly successful
approach in the literature \cite{turian-ratinov-bengio:2010:ACL}, and here we
adapt some of the methods of Turian et al. to our particular task. Concretely,
in this chapter we explore using Brown clustering \cite{brown1992class}, also
known as IBM clustering, since it was developed by the Candide group at IBM.

There are actually several open questions here, though:

\begin{itemize}
  \item Does more source-language text help us learn ``better" Brown clusters,
    with respect to the CL-WSD task? It seems like it would, but we haven't
    demonstrated that yet.
  \item What's the relationship between the genre of the monolingual text we
    use and the source-language side of the test set? It seems to help to have
    in-domain text to train our clusters, but is there a size at which you
    learn a ``better" clustering that overtakes the in-domain-ness?
  \item How much does the size of the input text for clustering matter in the
  downstream task?
  \item Does it matter if you lemmatize the Spanish first, or not? It seems
    like the point of Brown Clustering is that it's picking up on syntax in
    some sense. But maybe lemmatizing is OK; we used the lemmatized version of
    Europarl in the es-qu experiments for SALTMIL.
  \item Similar open questions for word2vec.
\end{itemize}

It's also important to consider *what kinds of features* we can extract from
the brown clusters of the words in a sentence. We tried, for example, dropping
a bag of all the clusters that occur in the sentence, and prefix features for
all the clusters that appear too! This did not help classification results!
That's probably too many extraneous features.
It might be the case that we really just need to add the very local context --
three words on a side? In Turian et al, they do two words on a side.


used successfully in a variety of text classification tasks
and provides a straightforward mechanism
to add annotations to our source-language text.  We also try learning neural
word embeddings \cite{mikolovword2vec}, which map individual word types into a
multidimensional, continuous space in which words with similar co-occurrence
patterns, and thus similar meanings, have nearby representations.
These approaches make use of the distributional hypothesis to 

% we can also try topic models (say with MALLET)

In addition to large quantities of monolingual text, we have excellent
off-the-shelf NLP tools for Spanish, such as POS taggers and syntactic parsers,
which both capture abstractions about particular word types (e.g., perhaps a
noun in the window surrounding a focus word is indicative of some particular
meaning) and help us disambiguate meanings based on the structure of the
current sentence.

%% XXX: can we characterize "excellent" here? How good is FreeLing versus other
%% off-the-shelf Spanish parsers?

In this chapter, we want to write about all of the things that we can do with
just monolingual data in the source language.
This is probably going to be one of the most important chapters, and really one
of the nicest results in the dissertation.
We can probably integrate some text from the SALTMIL paper into this chapter.


\section{Monolingual features from other NLP tools}
In the Chipa software, we can make use of all kinds of annotations for our
text (see Section \ref{sec:annotations}), so adding more features is
straightforward.

Notably, we can use POS tags, features extracted from parsers... we could even
use sense annotations from a monolingual WSD system, if we had a good one on
hand.

For POS tagging and parsing, we run the open source FreeLing text analysis
suite \cite{padro12} on both the training data and the text input sentences.
FreeLing can perform a number of analysis tasks for Spanish, including
lemmatization, POS tagging, named entity recognition, and dependency parsing.




\section{Brown Clustering}
The Brown clustering algorithm takes as input unannotated text and produces a
mapping from word types in that text to clusters, such that words in the same
cluster have similar usage patterns according the corpus's bigram statistics.
We can then use this mapping from words to clusters in our classifiers, adding
an additional annotation for each word that allow the classifiers to find
higher-level abstractions than surface-level words or particular lemmas.
The desired number of clusters must be set ahead of time, but is a tunable
parameter.
We use a popular open source implementation of Brown clustering,
\footnote{\url{https://github.com/percyliang/brown-cluster}} described by
Liang \cite{Liang05semi-supervisedlearning}, running on both the Spanish
side of our bitext corpus and on the Europarl corpus \cite{europarl} for
Spanish.

The Brown clustering algorithm uses greedy optimization to try to maximize the
score for an assignment of all word types seen in the input text into a fixed
number of word classes. As the original intention of the approach was for
class-based language models, the scoring function is the mutual information
between two immediately subsequent tokens (ie, a bigram).

Concretely, the formula is...

%% XXX insert the formula here

Finding a globally optimal assignment for this would be computationally
difficult. But we have a greedy approach for finding a local optimum.

%% XXX Write about how Brown clustering works
%% XXX it's hierarchical too, so we don't have to use the leaf clusters...

So far, we have really good evidence that we can learn, in an unsupervised way,
with Brown Clustering coarse-grained categories for word types in the source
language that help us make lexical selection distinctions.

We can also try mkcls! Just Franz's word classes \cite{och1999efficient}.
Interestingly, mkcls is optimizing the same thing as Brown Clustering
approaches.


It would be good to include some nice examples of the clusters that Brown
Clustering learns, based on the different source-language corpora that we could
cluster on.


\begin{figure*}[t!]
  \begin{tabular}{|r|p{10cm}|}
    \hline
    category  & top twenty word types by frequency \\
    \hline
    countries & francia irlanda alemania grecia italia españa rumanía portugal polonia suecia bulgaria austria finlandia hungría bélgica japón gran\_bretaña dinamarca luxemburgo bosnia \\
    \hline
    more places & kosovo internet bruselas áfrica iraq lisboa chipre afganistán estrasburgo oriente\_próximo copenhague asia chechenia gaza oriente\_medio birmania londres irlanda\_del\_norte berlín barcelona \\
    \hline
    mostly people & hombre periodista jefes\_de\_estado individuo profesor soldado abogado delincuente demócrata dictador iglesia alumno adolescente perro chico economista gato jurista caballero bebé \\
    \hline
    infrastructure & infraestructura vehículo buque servicio\_público cultivo edificio barco negocio motor avión monopolio planta ruta coche libro aparato tren billete actividad\_económica camión \\
    \hline
    common verbs & pagar comprar vender explotar practicar soportar exportar comer consumir suministrar sacrificar fabricar gobernar comercializar cultivar fumar capturar almacenar curar beber \\
    \hline
  \end{tabular}
\caption{Some illustrative clusters found by the Brown clustering algorithm on
the Spanish Europarl data. These are five out of $C=1000$ clusters, and
were picked and labeled by hand. The words listed are the
top twenty terms from that cluster, by frequency.}
\label{fig:clusters}
\end{figure*}

Figure \ref{fig:clusters} shows some illustrative examples of clusters that
we found in the Spanish Europarl corpus.  Examining the output of the
clustering algorithm, we see some intuitively satisfying results; there are
clusters corresponding to the names of many countries, some nouns referring to
people, and common transitive verbs. Note that the clustering is unsupervised,
and the labels given are not produced by the algorithm.


\section{Neural Word Embeddings}

We can try applying the other stuff in the Turian et al Word Representations
paper. Such as word2vec \cite{mikolovword2vec}

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
word2vec results in multidimensional representations for each word.

Open question here:
Can we reproduce that ``Learning to Understand Phrases by Embedding the
Dictionary" paper too? Could we learn representations for the cross-language
case too?

Definitely try using word2vec. We want to train word2vec on Spanish wikipedia,
and maybe that's going to give us useful features.

We could also try some dimensionality reduction, other distributional semantics
approach. LSA or something.

\section{Topic models with MALLET}
Maybe skip topic models?

\section{Experiments}
Here we repeat the experiments from Chapter \ref{chap:evaluation}
As described in the previous

There are lots of combinations here...

\begin{itemize}
  \item es-gn, es-qu with pos tags
  \item es-gn, es-qu with syntactic heads
  \item es-gn, es-qu with syntactic children
  \item es-gn, es-qu with all syntactic features
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl brown clusters
  \item es-gn, es-qu with wikipedia brown clusters
  \item es-gn, es-qu with europarl and wikipedia brown clusters
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl embeddings
  \item es-gn, es-qu with wikipedia embeddings
  \item es-gn, es-qu with europarl and wikipedia embeddings
  \item es-gn, es-qu with pos tags and syntactic heads
\end{itemize}

\begin{itemize}
  \item COMBINE THEM ALL!!
\end{itemize}

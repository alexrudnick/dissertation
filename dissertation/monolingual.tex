\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work, our target languages are under-resourced, we have many
resources available for the source languages. We would like to use these to
make better sense of the input text, giving our classifiers clearer signals for
lexical selection in the target language.
The approaches considered in this chapter make additional features available to
the CL-WSD classifiers, based on our knowledge of the source language;
since we have relatively little bitext available for Spanish-Guarani
and Spanish-Quechua, we will need to lean more on our resources, including
tools and data, for Spanish so that we can better make sense of the input text.

Perhaps most saliently for Spanish, we have abundant monolingual text
available, which suggests that we could use unsupervised methods to discover
regularities in the language, and use these as features for our classifiers.
This approach has been broadly successful in the literature
\cite{turian-ratinov-bengio:2010:ACL}
%% XXX: add some more references
, and here we adapt some of the methods explored in previous work on text
classification to our task. Concretely, in this chapter we explore
Brown clustering \cite{brown1992class}, neural word embeddings, and labels
learned from other NLP tools such as off-the-shelf taggers and parsers for
Spanish. There are of course other related methods that one could investigate,
making use of the deep available literature on distributional semantics, but
these will be left to future work, either by this author or other researchers.

\section{Brown Clustering}
The Brown clustering algorithm, also known as IBM clustering, since it was
developed by the Candide group at IBM, takes an unannotated text corpus as
input and assigns each word type in the corpus into hierarchical clusters such
that types in the same cluster have similar usage patterns according to the
corpus's bigram statistics.

We can then use this clustering to extract features to our classifiers, adding
an annotation to each token describing a more abstract, less sparse
representation than than surface-level words or lemmas. The desired
number of clusters must be set ahead of time, but is a tunable parameter.

In this work we use Percy Liang's popular open source implementation of Brown
clustering, described in his masters work \cite{Liang05semi-supervisedlearning}
\footnote{And available at \url{https://github.com/percyliang/brown-cluster}},
which for Spanish we have run on the Europarl corpus \cite{europarl}, and also
on a dump of the entirety of the Spanish-language Wikipedia, taken in 2014.

Brown clustering uses greedy optimization to try to maximize the score for an
assignment of the word types present in the input text into a fixed number of
word classes. As the original intention of the approach was for class-based
language models, the scoring function is the mutual information between two
immediately subsequent tokens (i.e., a bigram).

Concretely, the optimization process is looking for a clustering $C$ that
maximizes the probability of the corpus $\boldsymbol{w}$, according to the
following formula.

\begin{equation} \label{eq:brownclassprob}
P(\boldsymbol{w}; C) = \prod_{w_i} p(w_i | C(w_i)) p(C(w_i) | C(w_{i-1}))
\end{equation}

We can use these clusters to extract a number of different kinds of features,
and we can use a variety of inputs to the clustering software in the first
place.
We tried, for example, dropping a bag of all the clusters that occur in the
sentence, and prefix features for all the clusters that appear too!
This did not help classification results!
That's probably too many extraneous features.
It might be the case that we really just need to add the very local context --
three words on a side? In Turian et al, they do two words on a side.


Finding a globally optimal assignment for this would be computationally
difficult. But we have a greedy approach for finding a local optimum.
Interestingly, mkcls is optimizing the same thing as Brown Clustering
approaches. That's Franz's word classes \cite{och1999efficient}.

When thinking about applying Brown clustering to this task, we would like to
answer several relevant questions.

\begin{itemize}
  \item Can we use unannotated text to learn useful information about the
  Spanish language and meanings of Spanish words, and apply this to our task?
  \item Does more source-language text help us learn ``better" Brown clusters,
    with respect to the CL-WSD task?
  \item How does the genre of the input text affect our performance, and how
  does this relate with the size of the text? Is there a size at which we
    can learn a ``better" clustering that overcomes genre mismatches?
  \item How much does the size of the input text for clustering matter in the
  downstream task?
  \item Does it matter if you lemmatize the Spanish first, or not? It seems
    like the point of Brown Clustering is that it's picking up on syntax in
    some sense. But maybe lemmatizing is OK; we used the lemmatized version of
    Europarl in the es-qu experiments for SALTMIL. If it's true, that's a
    worthwhile thing to note -- to use Brown clustering on Spanish text, maybe
    you need to lemmatize it first.
  \item Similar open questions for word2vec.
\end{itemize}


\section{Clustering Spanish text for this work}

%% TODO: include examples of the clusters that we learn from different corpora
%% specify lemmatized versus not lemmatized

Here's an interesting thing to note: when we do clustering on lemmas, we seem
to pick up more on ``semantic" categories of things. Note the ``infrastructure"
cluster below -- it's got words that all pertain to infrastructure!

When we don't use lemmas, the cluster that contains ``infraestructura" contains
these words: \emph{formación educación tecnología infraestructura publicidad
enseñanza ocupación religión vivienda ética propaganda banda fusión huelga
creatividad medicina manipulación tribuna cualificación comida}. It seems like
this is more about grammatical gender: those are all singular feminine nouns.

In fact, the words in many of the clusters all seem to have the same
grammatical gender and number. To get more semantic similarity, maybe we should
cluster on lemmas -- otherwise the morphological features of Spanish could
overwhelm the clustering approach.

\TODO{replace with current example}
\begin{figure*}[t!]
  \begin{tabular}{|r|p{10cm}|}
    \hline
    category  & top twenty word types by frequency \\
    \hline
    countries & francia irlanda alemania grecia italia españa rumanía portugal polonia suecia bulgaria austria finlandia hungría bélgica japón gran\_bretaña dinamarca luxemburgo bosnia \\
    \hline
    more places & kosovo internet bruselas áfrica iraq lisboa chipre afganistán estrasburgo oriente\_próximo copenhague asia chechenia gaza oriente\_medio birmania londres irlanda\_del\_norte berlín barcelona \\
    \hline
    mostly people & hombre periodista jefes\_de\_estado individuo profesor soldado abogado delincuente demócrata dictador iglesia alumno adolescente perro chico economista gato jurista caballero bebé \\
    \hline
    infrastructure & infraestructura vehículo buque servicio\_público cultivo edificio barco negocio motor avión monopolio planta ruta coche libro aparato tren billete actividad\_económica camión \\
    \hline
    common verbs & pagar comprar vender explotar practicar soportar exportar comer consumir suministrar sacrificar fabricar gobernar comercializar cultivar fumar capturar almacenar curar beber \\
    \hline
  \end{tabular}
\caption{Some illustrative clusters found by the Brown clustering algorithm on
the Spanish Europarl data. These are five out of $C=1000$ clusters, and
were picked and labeled by hand. The words listed are the
top twenty terms from that cluster, by frequency.}
\label{fig:clusters}
\end{figure*}

Figure \ref{fig:clusters} shows some illustrative examples of clusters that
we found in the Spanish Europarl corpus.  Examining the output of the
clustering algorithm, we see some intuitively satisfying results; there are
clusters corresponding to the names of many countries, some nouns referring to
people, and common transitive verbs. Note that the clustering is unsupervised,
and the labels given are not produced by the algorithm.

We additionally show the effects on classification accuracy of adding features
derived from Brown clusters, with clusters extracted from both the Europarl
corpus and the Spanish side of our training data.
We tried several different
settings for the number of clusters, ranging from $C=100$ to $C=2000$.
In all of our experimental settings, the addition of Brown cluster features
substantially improved classification accuracy. We note a consistent upward
trend in performance as we increase the number of clusters, allowing the
clustering algorithm to learn finer-grained distinctions.
The training algorithm takes time quadratic in the number of clusters,
which becomes prohibitive fairly quickly, so even finer-grained distinctions
may be helpful, but will be left to future work. On a modern Linux
workstation, clustering Europarl (~2M sentences) into 2000 clusters took
roughly a day.

The classifiers using clusters extracted from the Spanish side of our bitext
consistently outperformed those learned from the Europarl corpus. We had an
intuition that the much larger corpus (nearly two million sentences) would
help, but the clusters learned in-domain, largely from the Bible, reflect
usage distinctions in that domain. Here we are in fact cheating slightly, as
information from the complete corpus is used to classify parts of that corpus.


\subsection{Learning Clusters from Europarl}

\subsection{Learning Clusters from Wikipedia}


\section{Neural Word Embeddings}

%% XXX: find some related work on word embeddings meant to capture syntactic
%% relationships versus semantic relationships; will lemmatizing here matter,
%% similarly to how it affects Brown clustering?

%% XXX: explain skip-grams and CBOW




Another rich source of features that has been useful for many text
classification problems in recent years is neural word embeddings, perhaps most
famously embodied in the work of Mikolov et al. \cite{mikolovword2vec} and
their associated open source implementation of the technique,
word2vec\footnote{Available at
\url{https://code.google.com/archive/p/word2vec/}}.

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
word2vec results in multidimensional representations for word types, turning
the very sparse ``one hot" representation in which a words with similar uses or
meanings do not have any obvious similarity into a much denser continuous space
in which similar words are placed nearby. These ``embeddings", or placements of
word types into a continuous space, are learned during some other
classification task, and are performed by the early layers of a neural network.

%% 
We could also try some dimensionality reduction or other distributional
semantics approaches, but representations learned during discriminative
classification tasks have been found generally more effective for text
classification problems by Baroni et al \cite{baroni2014don}.


We also try learning neural word embeddings \cite{mikolovword2vec}, which map
individual word types into a multidimensional, continuous space in which words
with similar co-occurrence patterns, and thus similar meanings, have nearby
representations.  These approaches make use of the distributional hypothesis to 

\section{Neural Document Embeddings}
%% XXX working here
Tomas Mikolov, Quoc Le and collaborators have also developed approaches that
model not only individual words, but produce representations for sentences,
paragraphs, or entire documents at once, described in 
\cite{dai-document-embedding-2015,quocle-distributed-representations-2014}.
This approach, typically called ``paragraph vectors" or sometimes ``doc2vec"
takes unlabeled text and learns not only representations for word types found
in the training data, but also representations for each document in the
training set. Here a ``document" could be as short as an individual sentence,
or could be scaled to paragraphs or much longer documents, depending on the
intended application.

This doc2vec approach is an extension of the word2vec approach. Here during
training, the single hidden layer of the network used to predict words in a
document contains not only a representation of the embedding for a word type,
but also an embedding that is learned for the current document.

Then at inference time, when we want to produce a representation for a new
document, the embeddings for the word types are held constant and we run the
neural network optimizer (stochastic gradient descent) to learn a new embedding
for the current document.

As such, Paragraph Vectors provide a straightforward approach for learning
representations of new documents based on an unlabeled training corpus. This is
a very convenient technique for this work.

\section{Monolingual features from other NLP tools}
%% XXX: can we characterize "excellent" here? How good is FreeLing versus other
%% off-the-shelf Spanish parsers?
In addition to large quantities of monolingual text, we have excellent
off-the-shelf NLP tools for Spanish, such as POS taggers and syntactic parsers,
which both capture abstractions about particular word types (e.g., perhaps a
noun in the window surrounding a focus word is indicative of some particular
meaning) and help us disambiguate meanings based on the structure of the
current sentence.


In the Chipa software, we can make use of all kinds of annotations for our
text (see Section \ref{sec:annotations}), so adding more features is
straightforward.

As a first step, and perhaps an obvious one, we can use part-of-speech tags and
features extracted from parsers, such as the heads and children of the focus
words.
We could even use sense annotations from a monolingual WSD system, if we had a
good one on hand. This idea is akin to one that we will explore in Chapter
\ref{chap:multilingual}

For POS tagging and parsing, we run the open source FreeLing text analysis
suite \cite{padro12} on both the training data and the text input sentences.
FreeLing can perform a number of analysis tasks for Spanish, including
lemmatization, POS tagging, named entity recognition, and dependency parsing.

\section{Experiments}
Here we repeat the experiments from Chapter \ref{chap:evaluation}...

There are lots of combinations here...

\begin{itemize}
  \item es-gn, es-qu with pos tags
  \item es-gn, es-qu with syntactic heads
  \item es-gn, es-qu with syntactic children
  \item es-gn, es-qu with all syntactic features
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl brown clusters
  \item es-gn, es-qu with wikipedia brown clusters
  \item es-gn, es-qu with europarl and wikipedia brown clusters
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl embeddings
  \item es-gn, es-qu with wikipedia embeddings
  \item es-gn, es-qu with europarl and wikipedia embeddings
  \item es-gn, es-qu with pos tags and syntactic heads
\end{itemize}

\begin{itemize}
  \item COMBINE THEM ALL!!
\end{itemize}

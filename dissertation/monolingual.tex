\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work, our target languages are under-resourced, we have many
resources available for the source languages. We would like to use these to
make better sense of the input text, giving our classifiers clearer signals for
lexical selection in the target language.
The approaches considered in this chapter make additional features available to
the CL-WSD classifiers, based on our knowledge of the source language.
In short, since we have relatively little bitext available for Spanish-Guarani
and Spanish-Quechua, we will need to lean more on our resources, including
tools and data, for Spanish so that we can better make sense of the input text.

Perhaps most saliently for Spanish, we have abundant monolingual text
available, which suggests that we could use unsupervised methods to discover
helpful regularities and thus good features for our classifiers. This approach
has been broadly successful in the literature
\cite{turian-ratinov-bengio:2010:ACL}
%% XXX: add some more references
, and here we adapt some of the methods explored in previous work on text
classification to our particular task. Concretely, in this chapter we explore
Brown clustering \cite{brown1992class}, neural word embeddings, and labels
learned from other NLP tools such as off-the-shelf taggers and parsers for
Spanish.


\section{Brown Clustering}
The Brown clustering algorithm, also known as IBM clustering, since it was
initially developed by the Candide group at IBM, takes as input unannotated
text and produces a mapping from word types in that text to clusters, such that
words in the same cluster have similar usage patterns according the corpus's
bigram statistics.
We can then use this mapping from words to clusters in our classifiers, adding
an additional annotation for each word that allow the classifiers to find
higher-level abstractions than surface-level words or particular lemmas.
The desired number of clusters must be set ahead of time, but is a tunable
parameter.
We use a popular open source implementation of Brown clustering,
\footnote{\url{https://github.com/percyliang/brown-cluster}} described by
Liang \cite{Liang05semi-supervisedlearning}, running on both the Spanish
side of our bitext corpus and on the Europarl corpus \cite{europarl} for
Spanish.

It's also important to consider *what kinds of features* we can extract from
the brown clusters of the words in a sentence. We tried, for example, dropping
a bag of all the clusters that occur in the sentence, and prefix features for
all the clusters that appear too! This did not help classification results!
That's probably too many extraneous features.
It might be the case that we really just need to add the very local context --
three words on a side? In Turian et al, they do two words on a side.



The Brown clustering algorithm uses greedy optimization to try to maximize the
score for an assignment of all word types seen in the input text into a fixed
number of word classes. As the original intention of the approach was for
class-based language models, the scoring function is the mutual information
between two immediately subsequent tokens (ie, a bigram).

Concretely, the formula is...

\begin{equation} \label{eq:brownclassprob}
P(\boldsymbol{w}; C) = \prod_{w_i} p(w_i | C(w_i)) p(C(w_i) | C(w_{i-1}))
\end{equation}

Finding a globally optimal assignment for this would be computationally
difficult. But we have a greedy approach for finding a local optimum.
Interestingly, mkcls is optimizing the same thing as Brown Clustering
approaches. That's Franz's word classes \cite{och1999efficient}.

When thinking about applying Brown clustering to this task, we would like to
answer several relevant questions.

\begin{itemize}
  \item Can we use unannotated text to learn useful information about the
  Spanish language and meanings of Spanish words, and apply this to our task?
  \item Does more source-language text help us learn ``better" Brown clusters,
    with respect to the CL-WSD task?
  \item How does the genre of the input text affect our performance, and how
  does this relate with the size of the text? Is there a size at which we
    can learn a ``better" clustering that overcomes genre mismatches?
  \item How much does the size of the input text for clustering matter in the
  downstream task?
  \item Does it matter if you lemmatize the Spanish first, or not? It seems
    like the point of Brown Clustering is that it's picking up on syntax in
    some sense. But maybe lemmatizing is OK; we used the lemmatized version of
    Europarl in the es-qu experiments for SALTMIL. If it's true, that's a
    worthwhile thing to note -- to use Brown clustering on Spanish text, maybe
    you need to lemmatize it first.
  \item Similar open questions for word2vec.
\end{itemize}


\section{Clustering Spanish text for this work}

It would be good to include some nice examples of the clusters that Brown
Clustering learns, based on the different source-language corpora that we could
cluster on.

Here's an interesting thing to note: when we do clustering on lemmas, we seem
to pick up more on ``semantic" categories of things. Note the ``infrastructure"
cluster below -- it's got words that all pertain to infrastructure!

When we don't use lemmas, the cluster that contains ``infraestructura" contains
these words: \emph{formación educación tecnología infraestructura publicidad
enseñanza ocupación religión vivienda ética propaganda banda fusión huelga
creatividad medicina manipulación tribuna cualificación comida}. It seems like
this is more about grammatical gender: those are all singular feminine nouns.

In fact, the words in many of the clusters all seem to have the same
grammatical gender and number. To get more semantic similarity, maybe we should
cluster on lemmas -- otherwise the morphological features of Spanish could
overwhelm the clustering approach.

\TODO{replace with current example}
\begin{figure*}[t!]
  \begin{tabular}{|r|p{10cm}|}
    \hline
    category  & top twenty word types by frequency \\
    \hline
    countries & francia irlanda alemania grecia italia españa rumanía portugal polonia suecia bulgaria austria finlandia hungría bélgica japón gran\_bretaña dinamarca luxemburgo bosnia \\
    \hline
    more places & kosovo internet bruselas áfrica iraq lisboa chipre afganistán estrasburgo oriente\_próximo copenhague asia chechenia gaza oriente\_medio birmania londres irlanda\_del\_norte berlín barcelona \\
    \hline
    mostly people & hombre periodista jefes\_de\_estado individuo profesor soldado abogado delincuente demócrata dictador iglesia alumno adolescente perro chico economista gato jurista caballero bebé \\
    \hline
    infrastructure & infraestructura vehículo buque servicio\_público cultivo edificio barco negocio motor avión monopolio planta ruta coche libro aparato tren billete actividad\_económica camión \\
    \hline
    common verbs & pagar comprar vender explotar practicar soportar exportar comer consumir suministrar sacrificar fabricar gobernar comercializar cultivar fumar capturar almacenar curar beber \\
    \hline
  \end{tabular}
\caption{Some illustrative clusters found by the Brown clustering algorithm on
the Spanish Europarl data. These are five out of $C=1000$ clusters, and
were picked and labeled by hand. The words listed are the
top twenty terms from that cluster, by frequency.}
\label{fig:clusters}
\end{figure*}

Figure \ref{fig:clusters} shows some illustrative examples of clusters that
we found in the Spanish Europarl corpus.  Examining the output of the
clustering algorithm, we see some intuitively satisfying results; there are
clusters corresponding to the names of many countries, some nouns referring to
people, and common transitive verbs. Note that the clustering is unsupervised,
and the labels given are not produced by the algorithm.

\subsection{Learning Clusters from Europarl}

\subsection{Learning Clusters from Wikipedia}


\section{Neural Word Embeddings}
Another rich source of features that has been useful for many text
classification problems in recent years is neural word embeddings, perhaps most
famously embodied in the work of Mikolov et al. \cite{mikolovword2vec} and
their associated open source implementation of the technique,
word2vec\footnote{Available at
\url{https://code.google.com/archive/p/word2vec/}}.

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
word2vec results in multidimensional representations for word types, turning
the very sparse ``one hot" representation in which a words with similar uses or
meanings do not have any obvious similarity into a much denser continuous space
in which similar words are placed nearby. These ``embeddings", or placements of
word types into a continuous space, are learned during some other
classification task, and are performed by the early layers of a neural network.

%% 
We could also try some dimensionality reduction, other distributional semantics
approach. LSA or something.


We also try learning neural word embeddings \cite{mikolovword2vec}, which map
individual word types into a multidimensional, continuous space in which words
with similar co-occurrence patterns, and thus similar meanings, have nearby
representations.  These approaches make use of the distributional hypothesis to 

\section{Monolingual features from other NLP tools}
In the Chipa software, we can make use of all kinds of annotations for our
text (see Section \ref{sec:annotations}), so adding more features is
straightforward.


%% XXX: can we characterize "excellent" here? How good is FreeLing versus other
%% off-the-shelf Spanish parsers?
In addition to large quantities of monolingual text, we have excellent
off-the-shelf NLP tools for Spanish, such as POS taggers and syntactic parsers,
which both capture abstractions about particular word types (e.g., perhaps a
noun in the window surrounding a focus word is indicative of some particular
meaning) and help us disambiguate meanings based on the structure of the
current sentence.

Notably, we can use POS tags, features extracted from parsers... we could even
use sense annotations from a monolingual WSD system, if we had a good one on
hand. (nb: this is what we're going to do in the multilingual chapter, in some
sense)

For POS tagging and parsing, we run the open source FreeLing text analysis
suite \cite{padro12} on both the training data and the text input sentences.
FreeLing can perform a number of analysis tasks for Spanish, including
lemmatization, POS tagging, named entity recognition, and dependency parsing.


\section{Experiments}
Here we repeat the experiments from Chapter \ref{chap:evaluation}
As described in the previous

There are lots of combinations here...

\begin{itemize}
  \item es-gn, es-qu with pos tags
  \item es-gn, es-qu with syntactic heads
  \item es-gn, es-qu with syntactic children
  \item es-gn, es-qu with all syntactic features
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl brown clusters
  \item es-gn, es-qu with wikipedia brown clusters
  \item es-gn, es-qu with europarl and wikipedia brown clusters
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl embeddings
  \item es-gn, es-qu with wikipedia embeddings
  \item es-gn, es-qu with europarl and wikipedia embeddings
  \item es-gn, es-qu with pos tags and syntactic heads
\end{itemize}

\begin{itemize}
  \item COMBINE THEM ALL!!
\end{itemize}

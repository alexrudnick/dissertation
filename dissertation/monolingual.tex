\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work our target languages are under-resourced, we have many
resources available for the source languages. We would like to use these to
make better sense of the input text, giving our classifiers clearer signals and
better representations for lexical selection in the target language. The
approaches considered in this chapter make additional features or different
representations available to the CL-WSD classifiers based on knowledge of the
source language, either gleaned through unsupervised methods or baked into
extant tools. Since we have relatively little bitext available for
Spanish-Guarani and Spanish-Quechua, we will need to lean more on our Spanish
resources, software and data, in order to make better sense of the input text.

Perhaps most saliently for Spanish, we have abundant monolingual text
available, which suggests that we could use unsupervised methods to discover
regularities in the language, yielding better features for our classifiers.
This approach has been broadly successful in the literature
\cite{turian-ratinov-bengio:2010:ACL,baroni2014don}, and here we adapt some of
the methods explored in previous work on text classification to our task.

Concretely, in this chapter we explore labels learned from existing NLP tools
such as off-the-shelf taggers and parsers for Spanish, Brown clustering
\cite{brown1992class}, and a few related approaches to neural embeddings. There
are of course other related methods that one could investigate, especially
making use of the broader literature on distributional semantics, but we
consider them to be outside the scope of this work.
First we will describe the methods used in some
detail, and then towards the end of the chapter, in Sections
\ref{sec:monolingual-experiments} and \ref{sec:monolingual-results}
respectively, we describe experiments and present experimental results.

\section{Monolingual features from existing NLP tools}
There are a large number off-the-shelf NLP tools available for Spanish. Here we
will look into part of speech taggers and syntactic parsers specifically.
Tagging can help us capture abstractions over particular word types, and
provides some disambiguation on its own. For example, in Spanish, \emph{poder}
can be the infinitive verb ``to be able", or it can be a noun, meaning ``power,
ability". And if these different meanings are surfaced as different words in
the target language (as they are in English), then simply having an accurate
POS tag makes the CL-WSD task much easier. More generally, perhaps a noun in
the window surrounding a focus word could be indicative of a particular
meaning.

Similarly, syntactic structure may also provide useful features.  Verbs
especially may have drastically different translations in the target language
based on their objects. There are many familiar examples of this phenomenon,
especially among the ``light verbs" and ``phrasal verbs" with noncompositional
meanings in English and Spanish.

In the Chipa software, we can make use of arbitrary annotations for the input
text (see Section \ref{sec:annotations}), so adding more features based on
analysis by external tools is straightforward.

As a first step, we synthesize features based on the part-of-speech tags of the
tokens surrounding the focus word, and using a syntactic parser,
the heads and children of the current focus word. In principle, we could use
other annotations, such as sense annotations from a monolingual WSD system,
given a good one. This idea is akin to one that we will explore in Chapter
\ref{chap:multilingual}, in which we use Chipa itself (trained for other
language pairs, and with larger data sets) to provide annotations, effectively
using some other target language as a sense inventory for WSD.

For POS tagging, we run the open source FreeLing text analysis suite
\cite{padro12} on input sentences. FreeLing can perform a number of analyses
for Spanish, including POS tagging, dependency parsing, lemmatization, and
named entity recognition, of which the latter two are part of the standard
preprocessing done for all experiments in this work. When all the text for an
experiment is known beforehand, as in the experiments reported in this chapter,
we can run FreeLing during the data annotation step (see Section
\ref{sec:datasetsandpreprocessing}) and simply record FreeLing's output as text
annotations. When running on novel test sentences, as in server mode, we must
run it on those sentences just before inference time.

Similarly, for syntactic features, we use MaltParser\cite{Nivre06maltparser:a}
\footnote{Available at \url{http://maltparser.org/} ; in this work we use
version 1.9.0 of the parser, and the ``espmalt" pretrained model, which is
available at \url{http://www.iula.upf.edu/recurs01_mpars_uk.htm}, and was
trained on the IULA Treebank\cite{MARIMON12.519}, by researchers from the IULA
group at Universitat Pompeu Fabra.} to get dependency parses of the input
sentence. This is also performed as a corpus annotation, making the syntactic
relationships for each token available during feature extraction. The parser
here depends on the POS tags produced by FreeLing. Conveniently, FreeLing and
the espmalt parser model assume the same tag set, but as with any pipeline of
NLP systems, using the inferred output from the tagger as input to the parser
carries the risk that errors at early stages in the pipeline could propagate
and cause problems at the later stages of processing. Here we note this concern
but move on; it is a very general problem, and solutions to it are an open area
of research. The parser also depends on the coarse-grained ``universal" POS
tags \cite{PETROV12.274}, which we map from the IULA tagset to universal coarse
tags during corpus annotation.

Operationally, before training our CL-WSD system, we parse all of the training
data with the ``espmalt" model, using the tags inferred by FreeLing during
preprocessing, and store all of MaltParser's dependency parses on disk.
Then with a corpus annotation script, we walk those dependency
graphs to find each token's immediate syntactic head (or the root of the
sentence) and every token's immediate syntactic children, if any. All of these
dependency heads and children are annotated in the training data, to be made
available during feature extraction. Then at feature extraction time, chipa
turns these stored annotations into sparse binary features. The list of all
syntactic features made available to the classifier is given in Figure
\ref{fig:syntacticfeatures}.

To give a concrete example, consider the dependency graph for an English
sentence in Figure~\ref{fig:syntacticfeatures:example}, and the scenario in
which we want to classify the main verb of the sentence, ``chased". The
syntactic features that we would extract for this situation would be as
follows: \texttt{postag(VERB)}, \texttt{postag\_left(NOUN)},
\texttt{postag\_right(DET)} (assuming that the English sentence has been
labeled with Universal POS tags), \texttt{head\_lemma(ROOT)},
\texttt{head\_surface(ROOT)} (since the main verb is attached to the ROOT node
of the dependency tree), \texttt{child\_lemma(dog)},
\texttt{child\_lemma(ball)} (the lemmas of the subject and object of the verb),
and finally \texttt{child\_surface(dogs)}, and \texttt{child\_surface(balls)}
(the surface forms of the subject and object of the verb).

\begin{figure*}
  \begin{centering}
  \begin{tabular}{|r|p{11cm}|}
    \hline
    name          & description  \\
    \hline
    \texttt{postag}    & part-of-speech tag for the focus token \\
    \hline
    \texttt{postag\_left}  & POS tag for the token to the left of focus token \\
    \hline
    \texttt{postag\_right} & \emph{ibid.}, for the right \\
    \hline
    \texttt{head\_lemma} & lemma of the focus word's syntactic head, or ROOT if
    it has no head). \\
    \hline
    \texttt{head\_surface} & \emph{ibid.}, but for the syntactic head's surface
    form. \\
    \hline
    \texttt{child\_lemma} & lemma of the syntactic child or children of the
    focus word. Feature appears multiple times for multiple children. \\
    \hline
    \texttt{child\_surface} & \emph{ibid.}, but for the children's surface
    forms. \\
    \hline
  \end{tabular}
  \end{centering}
  \caption{Additional syntactic features}
  \label{fig:syntacticfeatures}
\end{figure*}

\begin{figure*}
  \begin{centering}
  \includegraphics{monolingual-ex1.pdf}
  \end{centering}
  \caption{An example dependency graph for an English sentence.}
  \label{fig:syntacticfeatures:example}
\end{figure*}

\section{Brown Clustering}
The Brown clustering algorithm\cite{brown1992class}, also known as IBM
clustering, as it was developed by the Candide group at IBM, finds a
hierarchical clustering for all the word types in a corpus, such that words
that are clustered nearby have similar usage patterns, and thus presumably
similar meanings.
The tree of clusters is binary-branching, so the identity of a cluster is
simply its path from the root of the tree, and clusters that are close in the
tree have similar usage patterns. The desired number of ``leaf" clusters must
be set ahead of time as a tunable parameter.

Each of the word types present in the input corpus is allocated to one of the
leaf clusters, such that the assignment attempts to maximize the probability of
the input corpus.  These clusters were originally intended for use in
class-based language modeling, and as such the scoring function is the mutual
information between subsequent tokens. Concretely, the optimization process is
searching for a clustering $C$ that maximizes the probability of the corpus
$\boldsymbol{w}$, according to the formula in Figure~\ref{fig:brownprob};
intuitively, word types that occur in similar bigrams should be in similar
clusters.

\begin{figure*}

  \begin{equation} \label{eq:brownclassprob}
  P(\boldsymbol{w}; C) = \prod_{w_i} p(w_i | C(w_i)) p(C(w_i) | C(w_{i-1}))
  \end{equation}

  \caption{The Brown clustering expression for the probability of a corpus with
  a specific clustering $C$. It is the product, for each token, of the
  probability of that token given its cluster, and the probability of that
  current cluster given the previous cluster. This is analogous to the
  ``emission" and ``transition" probabilities used in an HMM-based tagger.}
  \label{fig:brownprob}
\end{figure*}

Finding an optimal assignment of all word types in the corpus into these
hierarchical clusters is an intractable problem, but several greedy approaches
that find local optima have been explored in the literature.  Notably, in
addition to Liang's approach, Franz Och's \texttt{mkcls} package (familiar to
Moses users, and described in \cite{och1999efficient}) optimizes the same
function when assigning words to classes. In any case, with the available
software running on modern hardware, we can find a clustering for the corpora
used in this work in a fairly short time, on the order of hours.

Here our immediate use for these clusters is to create more features for our
classifiers, interpreting the clusters into which a word type falls as a tag
for instances of that type. These annotations give a more abstract, less sparse
representation than surface forms or lemmas, hopefully providing useful
semantic and syntactic generalizations. We may also preprocess the input to
the clustering algorithm beforehand, as long as we can reproduce the
preprocessing for unseen input text at query time.

In applying Brown clusters to this task, we would like to answer several
questions.

\begin{itemize}
  \item Can learning Brown clusters from monolingual source text improve our
  performance on this CL-WSD task?
  \item Does more source-language text help us learn more helpful Brown
  clusters, with respect to the CL-WSD task? Can a large enough monolingual
  corpus help us overcome domain mismatches?
  \item What kinds of preprocessing should we do on the source text?
  Particularly, should Spanish source text be lemmatized?
\end{itemize}

\subsection{Clustering in practice}
In this work we use Percy Liang's implementation \footnote{Available at
\url{https://github.com/percyliang/brown-cluster}} of Brown clustering
\cite{Liang05semi-supervisedlearning}. We ran this tool on two different
monolingual corpora, the Spanish section of the Europarl corpus \cite{europarl}
(2 million sentences) and a dump of Spanish-language Wikipedia (20 million
sentences). In all cases, we set the number of leaf clusters to 1000, which is
the default for the package.
When working with Spanish, considering its relative morphological richness, we
may consider lemmatizing our input text before clustering. This decision will
be more significant than it would be for English, given the inflections present
in Spanish, especially verb conjugations and adjective-noun agreement.
We might expect that Brown clustering on surface forms would help us find
abstractions over syntax, and that lemmatization will turn it towards more
semantic abstractions. This is an empirical question, and we briefly look into
it here, first by qualitatively examining the clusters that we learned, but
later extrinsically evaluating the clusters to see how they affect the CL-WSD
task.

\input{clusterfigures.tex}

Figures \ref{fig:clusters-europarl-surface}, \ref{fig:clusters-europarl-lemma},
\ref{fig:clusters-wikipedia-surface} and \ref{fig:clusters-wikipedia-lemma}
show some hand-selected illustrative examples of clusters that we found in our
monolingual Spanish corpora, Europarl and Wikipedia, in both surface-form and
lemmatized versions. Examining the output of the clustering algorithm, we see
some intuitively satisfying results; there are clusters corresponding to the
names of many countries, nouns referring to people by profession, and some
semantically similar verbs. The words listed in these figures are the top
twenty terms from that cluster, by frequency in the corpus. Note that the names
given for the clusters were also chosen by hand, rather than through some
automatic process. It is reassuring that the clustering process, over all of
these corpora, is finding some interpretable generalizations in the text.

Comparing the lemmatized versus the surface-form versions of the clustering, we
note that, as expected, the clusters capture more syntactic regularities when
given the inflected forms. Consider, for example the cluster containing
\emph{infraestructura} in Figure~\ref{fig:clusters-europarl-lemma}, as compared
to its cluster in Figure~\ref{fig:clusters-europarl-surface}. In the lemmatized
version, we see that \emph{infraestructura} falls into a cluster primarily
composed of words pertaining to transportation-related concepts, such as
\emph{vehículo} 'vehicle', \emph{billete} 'ticket' and \emph{avión} 'airplane'.
In the Wikipedia lemma clusters, shown in Figure
\ref{fig:clusters-wikipedia-lemma}, we see some satisfying semantic clusters as
well, such as adjectives describing political positions, verbs about making
choices, and a cluster of locations, mostly cities.

Contrastingly, on the surface-form side, the cluster that contains
\emph{infraestructura} does not immediately reveal the same conceptual
similarity: we see \emph{formación} 'education (formal)' \emph{creatividad}
'creativity', and \emph{comida} 'food' in the same cluster. While a few of the
words have similar meanings, the clustering seems to have found that these
words cluster together as singular feminine nouns, which is more of a
syntactic commonality than a semantic one. This is not to say that the
surface-form clusters do not exhibit semantic relatedness; in the ``profession
nouns" cluster in Figure~\ref{fig:clusters-wikipedia-surface}, we see nouns
that refer to people's professions, although it should also be noted that these
are all singular masculine nouns. Also in the Wikipedia surface clusters, we
see a cluster containing almost entirely words referring to cardinal
directions\footnote{With the inclusion of \emph{voivodato} (English:
'voivodeship' or Polish 'województwo') which, I have just learned from
Wikipedia, is an administrative region in eastern Europe, similar to a
historical duchy or modern province.}.

So a qualitative look at the clusters found in these corpora shows some
promising regularities found in the text. As expected, when we cluster on the
surface forms, we find both syntactic and semantic similarities, but using the
lemmatized forms seems to push the clusters towards conceptual similarity. It
is still an empirical question which of these approaches will help more for our
CL-WSD task, so we will run experiments using each one.

\subsection{Classification features based on Brown clusters}
Given a clustering learned from a large monolingual corpus, we must still
decide how to extract classification features from those clusters. If we know,
for example, that the token to the left of the focus word is in cluster
``010111111100", what information do we provide to the classifier? We could
interpret this cluster atomically, as a tag for that token, and create sparse
features for an input sentence based on the clusters present in it, perhaps
focusing on the window surrounding the focus word. However, following the work
of Turian \emph{et al.} \cite{turian-ratinov-bengio:2010:ACL}, we also take the
prefixes of each cluster's bit strings at 4, 6, and 10 bits, taking advantage
of the hierarchical structure in the clusters, and used these separate sparse
features.

As a concrete example using the lemmatized Wikipedia clusters, if we find that
the lemma \emph{rioplatense} is in a three-token window around the current
focus word, and that this lemma is in cluster ``111110111111111110", then we
extract the following features, setting each of them to $1$.

\begin{itemize}
\item \texttt{brown\_window\_wikipedia\_lemma(111110111111111110)}
\item \texttt{brown\_window\_wikipedia\_lemma\_4(1111)}
\item \texttt{brown\_window\_wikipedia\_lemma\_6(111110)}
\item \texttt{brown\_window\_wikipedia\_lemma\_10(1111101111)}
\end{itemize}

We additionally extract analogous features for all tokens in the sentence;
these would be called \texttt{brown\_bag\_wikipedia\_lemma},
\texttt{brown\_bag\_wikipedia\_4}, and so on. If we are using clusters based on
the surface forms, these simply do not have \texttt{lemma} in their name, and
features based on Europarl, are marked with \texttt{europarl} rather than
\texttt{wikipedia}.

\begin{figure*}
  \begin{centering}
  \begin{tabular}{|r|p{11cm}|}
    \hline
    name          & description  \\
    \hline
    \texttt{brown\_bag}    & Bag of all Brown clusters for the entire sentence \\
    \hline
    \texttt{brown\_window}  & All Brown clusters for the three-token window around the focus word \\
    \hline
  \end{tabular}
  \end{centering}
  \caption{Features extracted from Brown clusters. These came in surface-form
  and lemma variants, and were trained on both the Europarl for Spanish, and
  our Wikipedia dump. Additionally, we add variants for the 4, 6, and 10-bit
  prefixes of the clusters.}
  \label{fig:brownfeatures}
\end{figure*}

In cases of unknown words, where we do not have a cluster assignment for a word
in the input text, we simply do not extract cluster features for that word.
This will happen in practice, even for a fairly large monolingual corpus, but
may be especially common in this setting due to domain mismatches. Many
biblical names, for example, are not present in Europarl.

\section{Neural Word Embeddings}
Another rich source of features that has proved useful for many text
classification problems in recent years is neural word embeddings, perhaps most
famously developed in the work of Mikolov et al. \cite{mikolovword2vec} and
their associated open source implementation of the technique,
word2vec\footnote{Available at
\url{https://code.google.com/archive/p/word2vec/}}. In this work we investigate
the use of ``word2vec"-style word embeddings, as well as the related technique,
``doc2vec" or ``paragraph vectors"
\cite{dai-document-embedding-2015,quocle-distributed-representations-2014}.
These techniques let our classifiers operate on lower-dimensional dense vectors
(of a few hundred dimensions, typically), as opposed to the high-dimensional
sparse vectors typically used in earlier NLP literature\footnote{
What we might consider to be ``symbolic" binary features used in machine
learning systems for NLP in recent decades -- such as ``the word `dog` appears
in the sentence" -- are effectively equivalent to these very sparse ``one hot"
representations of words, in which, for a vocabulary of size $|V|$, words are
represented by a length-$V$ vector in which all but one of the elements are
0.}.

There is a rich literature on continuous representations for words; the idea
has a long lineage in NLP, and we could try any number of dimensionality
reduction or distributional semantics approaches here. However, recent
empirical work by Baroni et al \cite{baroni2014don} (summed up by the title of
the paper, ``Don't Count, Predict!") has shown that representations built
during discriminative learning tasks are typically more effective for text
classification problems similar to ours.

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
but like other embedding techniques, word2vec learns multidimensional
representations for word types, turning the very sparse ``one hot"
representation in which words with similar uses or meanings do not have any
obvious similarity into a much denser continuous space, wherein words with
related meanings are placed nearby. These ``embeddings", or placements of word
types in a continuous space, are learned latently during some other
classification task, and are performed by the early layers of a neural network.

The word2vec model in particular has two variations, useful in different
contexts. One, called Continuous Bag-of-Words (CBOW) learns a classifier that
can predict individual words based on their context, while the ``skip-grams"
variant does the reverse and learns to predict context words based on an
individual focus word. The received wisdom\footnote{According to the TensorFlow
tutorial on word2vec, available at
\url{https://www.tensorflow.org/tutorials/word2vec}} is that CBOW is more
appropriate when training on smaller data sets, but this is an empirical
question, so here we run experiments with both variants.

In either case, running a word2vec training process results in a static mapping
from word types to their embeddings in a vector space of a given size,
typically a few hundred dimensions. These embeddings have been shown to be
helpful as features in a number of different NLP tasks \cite{baroni2014don},
allowing us to learn richer representations of word types from plentiful
unannotated monolingual text. This effectively turns what was a purely
supervised task, requiring labeled training data, into a semi-supervised task,
in which the representation-learning phase can be carried out on unlabeled data
by synthesizing a supervised learning task that can be performed with simple
monolingual text as its training data.

We trained a variety of word2vec embeddings for our text classification tasks,
again training on the Spanish-language section of the Europarl corpus (roughly
57 million words or 2 million sentences) and a dump of Spanish Wikipedia (449
million words, 20 million sentences). We learned embeddings using both CBOW and
skip-gram approaches, in 50, 100, 200, and 400 dimensions. We also
used the associated \texttt{word2phrase} tool, distributed with the public
\texttt{word2vec} implementation, which finds collocations in the input text
and treats them as individual tokens. The same collocations must be identified
in the input text for their embeddings to be used. At lookup time, we keep a
list of all the multiword expressions present in the dictionary of embeddings,
sorted from longest to shortest, first considering number of tokens, then
breaking ties by considering number of characters, and greedily replace any of
these multiword expressions (starting with the longest) found in the input text
with the token representing that MWE.

\subsection{From word embeddings to classification features}
We should note that, like Brown clusters, the embeddings learned for a word
type are fixed for that word type, and do not reflect the context of a
particular token, so a focus word's embedding vector will not provide useful
classification features on its own. This leaves us with the problem of how to
make use of the word vectors in a given sentence. Concretely, we must decide
how to combine several word vectors together, and which word vectors in a
sentence to choose for combination. A typical approach is to take element-wise
sums or averages over the embedding vectors for the tokens in a context
\cite[Chapter 8]{Goldberg17}.

There are a variety of approaches we could take, even within summing and
averaging. We could sum (element-wise) all of the word vectors for all the
tokens in the entire sentence. We could only consider the words in a narrow
window around the focus word. Or we could perform a weighted sum, in which
words nearer to the focus word are given more weight, but these weights drop
off according to some function, scaling with the distance from the focus word.
We might expect the tokens closest to the current focus word to be most
important for building a representation of the meaning of that word -- which
would lead us to expect the best results from summing uniformly over the
context window, or a weighted sum with gradually dropping weights -- or perhaps
we might expect that we want to represent the meaning of the entire sentence,
and so could sum over all the tokens.
How well any of these approaches work in practice is an empirical question, so
we run experiments using each of these these different approaches, resulting in
three different feature sets.

In the first feature set, called ``window", we present the classifier with the
element-wise sum of the vectors for the focus word (or the token containing the
focus word, if it is inside a multi-word expression) and the three tokens on
either side. The resulting vector thus has the same dimensionality as the word
embeddings that we learned. Secondly, in the ``fullsent" feature set, we
perform an unweighted element-wise sum of the word embeddings for the entire
input sentence. Finally, in the ``pyramid" feature set, we sum the embeddings
for each token in a broad window around the focus word, but before summing,
scale each embeddings by $max(0, 1.0 - (0.1 * d))$, where $d$ is the distance
between that token and the focus word.

For all of the variations considered here, whether we are using skipgrams or
CBOW, for whichever dimensionality we choose, and for any of the methods of
combining the vectors, when using these word embeddings without other
resources, we end up representing a CL-WSD problem as a dense vector
of the same dimensionality as the current set of word embeddings;
these dense vectors can be passed along to any machine learning algorithm
that we care to use.

\section{Neural Document Embeddings}
Rather than coming up with representations for individual words and then
combining them to get a representation of a sentence or a local context, we
might want a technique that directly finds representations of sequences of
text. One such technique, called ``paragraph vectors" or ``doc2vec", does just
this. doc2vec was developed by some of the same researchers that produced
word2vec, and it can build representations for arbitrary sequences of text,
rather than individual word types
\cite{dai-document-embedding-2015,quocle-distributed-representations-2014}.
This approach takes unlabeled text and learns representations for word types
jointly with representations for each \emph{document}. Here a ``document" is an
arbitrary sequence of tokens, perhaps a single sentence, or perhaps much
longer, depending on the intended application.

Similar to word2vec, here doc2vec uses a neural network with a single hidden
layer to predict words in the current context. Here however, the hidden layer
contains not only an embedding trained for word types, but also an embedding
for the current document. Training proceeds by loading the current
representation for the current word and the current document, attempting to
predict some other word in the context (analogous to training for word2vec),
and then updating both of these representations based on the gradients between
the desired output and the actual output. 

Also similar to word2vec, there are two variants of the approach that can be
used for training. One variant of doc2vec, the ``Distributed Memory Model of
Paragraph Vectors", uses a paragraph embedding combined with word embeddings
for several words in the current context to predict a single individual word.
The other variant, the ``Distributed Bag of Words model of Paragraph Vectors",
simply uses the current paragraph vector to predict all of the words in the
current context, which despite containing ``bag of words" in its name, is more
closely analogous to the skip-gram artitecture for word2vec
\cite{quocle-distributed-representations-2014}, since the model is trained to
predict the entire context based on a single embedding.

In either case, the resulting stored model consists of the embeddings for
individual word types. The embeddings for any particular document in the
training set are not stored, since that document is unlikely to appear again at
inference time, but the embeddings for individual word types provide a useful
generalization, since they constrain embeddings for new documents.

At inference time, doc2vec produces new vectors representing previously unseen
documents based on these fixed word embedding vectors. In our work, as in many
text classification tasks, documents will be input sentences, or perhaps
sentence-length Bible verses, or smaller windows of text around a
word-in-context. To produce a representation for a new document, the embeddings
for word types are held constant and we run stochastic gradient descent
optimization to infer an embedding for the current document, based on the fixed
word embeddings, such that it helps predict words in the current context.

doc2vec thus provides a straightforward approach for learning to produce
representations of new documents, based on an unlabeled training corpus. Here
we use the implementation of doc2vec provided by the gensim package
\footnote{Available at \url{https://radimrehurek.com/gensim/}}, with its
default settings, which uses the ``distributed memory" model
\cite{rehurek-lrec}.

In order to use doc2vec for our experiments, we train and save a doc2vec model
on our 20 million sentences from Spanish-language Wikipedia. Then, during
training data annotation for the CL-WSD model, we pass over our bitext corpus,
take the source-language sentence from each training pair, and infer a new
embedding for it based on that doc2vec model. These sentence-level embeddings
are stored as a token annotation on the first token of each sentence, so they
can be made available during feature extraction. We can also show the gensim
software a small context window around the focus word, rather than the entire
sentence to generate a more focused representation.

In either case, the result is a dense vector of the specified dimensionality,
again typically a few hundred, which we can pass to whichever machine learning
algorithm we like, as we did previously with the vectors based on word2vec.

\section{Experiments}
\label{sec:monolingual-experiments}
Here we repeat the experiments from Chapter \ref{chap:baseline} for
Spanish-Guarani and Spanish-Quechua, using the features extracted from
syntactic tools, Brown clusters, and the various kinds of neural embeddings
presented in this chapter.
These experiments were carried out with the top classification algorithms that
we used in Chapter \ref{chap:baseline}, focusing on maximum entropy (using both
L1 and L2 regularization) and random forests. While most of the experiments
here are a combination of the baseline features with the features introduced in
this chapter, we also used the neural embeddings features on their own,
as a replacement for the baseline features. While there are many possible
combinations of the features, we did not run experiments for all (exponentially
many) possibilities exhaustively.

Specifically, we experimented with the following feature sets, based on the
techniques described in this chapter, for both Spanish-Guarani and
Spanish-Quechua.

\begin{itemize}
  \item POS tag features, with baseline features
  \item POS tag and dependency parse features, with baseline features

  \item Brown cluster features, with baseline features, in combinations of
  \dots{}
    \begin{itemize}
      \item trained on Europarl or Wikipedia
      \item based on lemmas or surface forms
      \item context-window only, or including both context-window and all
      clusters for the sentence as a bag.
    \end{itemize}

  \item word2vec embeddings on their own, in combinations of \dots{}
    \begin{itemize}
      \item trained on Europarl and Wikipedia
      \item skipgrams and CBOW
      \item using multi-word expression embeddings or not
      \item dimensions: 50, 100, 200, 400
      \item combination strategies: full sentence, context window, and
      ``pyramid" (summing embeddings with a weight that decreases with distance
      from the focus word)
    \end{itemize}

  \item doc2vec embeddings on their own, trained on Wikipedia\dots{}
    \begin{itemize}
      \item embeddings for a full sentence or embeddings for a context window
    \end{itemize}

  \item Combination of promising features: baseline features, with syntactic
  features and Brown clusters (context window, surface form, Wikipedia).
  \item Combination of promising features: ``pyramid" word2vec embeddings, with
  syntactic features and Brown clusters (also context window, surface form,
  Wikipedia).
\end{itemize}

In the next section, we present and discuss the results from these experiments.

\section{Experimental Results} 
\label{sec:monolingual-results}

This section contains many tables of numbers; in general, for each set of
experimental results presented, the top result for each setting is presented in
\emph{italics}, and the top result for a setting presented in the whole chapter
is presented in \textbf{bold}.

\subsection{Results: adding syntactic features}

\begin{figure*}
  \begin{centering}
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\
    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
    +pos tags features & 0.464 & 0.510 & 0.449 & 0.420 \\
    \hline
    +all syntactic features & 0.465 & 0.511 & 0.450 & 0.422 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
    +pos tags features & 0.479 & 0.527 & 0.462 & 0.438 \\
    \hline
    +all syntactic features & 0.480 & \textbf{0.530} & 0.465 & \emph{0.441} \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
    +pos tags features & 0.485 & 0.523 & 0.467 & 0.430 \\
    \hline
    +all syntactic features & \emph{0.486} & 0.527 & \emph{0.471} & 0.434 \\
    \hline
  \end{tabulary}
  \end{centering}
  \caption{Classification results for adding syntactic features to the default
  feature set, as compared with the MFS baseline.}
  \label{fig:syntactic-results}
\end{figure*}

Taking a look at the scores from the features based on easily available
Spanish-language taggers and parsers, which are presented in Figure
\ref{fig:syntactic-results}, we see small but fairly consistent improvements
over the baseline features as we add syntactic features. In all of our
settings, we see a few tenths of a percentage point gain by adding the POS tag
features, and then another small gain on top of that by adding syntactic
dependency features.

These gains seem to be of roughly the same (small) magnitude; it is not obvious
whether any of the classifiers are more able to make use of the additional
features than others, but this is somewhat reassuring, and it looks like we are
easily able to add some helpful features when we have syntactic analysis tools
available for our source language.

\subsection{Results: adding clustering features}

\begin{figure*}
  \begin{centering}
  {\footnotesize
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    features & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\
    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline
    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
    +Europarl clusters, surface & 0.446 & 0.491 & 0.429 & 0.399 \\
    \hline
    +Europarl clusters, lemmatized & 0.448 & 0.492 & 0.432 & 0.401 \\
    \hline
    +Wikipedia clusters, surface & 0.446 & 0.490 & 0.428 & 0.399 \\
    \hline
    +Wikipedia clusters, lemmatized & 0.448 & 0.490 & 0.429 & 0.401 \\
    \hline
    +Europarl, surface, window & 0.460 & 0.505 & 0.444 & 0.415 \\
    \hline
    +Europarl, lemmas, window & 0.460 & 0.506 & 0.444 & 0.415 \\
    \hline
    +Wikipedia, surface, window & 0.460 & 0.506 & 0.445 & 0.416 \\
    \hline
    +Wikipedia, lemmas, window & 0.460 & 0.506 & 0.443 & 0.415 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
    +Europarl clusters, surface & 0.462 & 0.512 & 0.445 & 0.420 \\
    \hline
    +Europarl clusters, lemmatized & 0.462 & 0.512 & 0.445 & 0.419 \\
    \hline
    +Wikipedia clusters, surface & 0.461 & 0.512 & 0.445 & 0.420 \\
    \hline
    +Wikipedia clusters, lemmatized & 0.463 & 0.512 & 0.445 & 0.419 \\
    \hline
    +Europarl, surface, window & 0.476 & \emph{0.525} & 0.458 & 0.435 \\
    \hline
    +Europarl, lemmas, window & 0.475 & \emph{0.525} & 0.459 & 0.435 \\
    \hline
    +Wikipedia, surface, window & 0.475 & \emph{0.525} & 0.460 & \emph{0.436} \\
    \hline
    +Wikipedia, lemmas, window & 0.475 & 0.524 & 0.459 & 0.434 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
    +Europarl clusters, surface & 0.476 & 0.519 & 0.461 & 0.423 \\
    \hline
    +Europarl clusters, lemmatized & 0.476 & 0.518 & 0.459 & 0.420 \\
    \hline
    +Wikipedia clusters, surface & 0.477 & 0.518 & 0.460 & 0.423 \\
    \hline
    +Wikipedia clusters, lemmatized & 0.478 & 0.517 & 0.459 & 0.418 \\
    \hline
    +Europarl, surface, window & 0.483 & 0.524 & 0.466 & 0.430 \\
    \hline
    +Europarl, lemmas, window & 0.483 & 0.524 & 0.466 & 0.428 \\
    \hline
    +Wikipedia, surface, window & \emph{0.484} & 0.524 & \emph{0.468} & 0.430 \\
    \hline
    +Wikipedia, lemmas, window & 0.483 & 0.524 & 0.466 & 0.427 \\
    \hline
  \end{tabulary}
  } %% end footnotesize
  \end{centering}
  \caption{Classification results for adding Brown cluster features to the
  default feature set.}
  \label{fig:brown-results}
\end{figure*}

We see the experimental results for adding our Brown Cluster features
(explained in Figure~\ref{fig:brownfeatures}) in Figure
\ref{fig:brown-results}.
In general, we do not see gains from adding all of the Brown Cluster features
together; this typically seems to diminish performance by a few tenths of a
percentage point, in many cases making the performance as a whole dip below the
MFS baseline. While they are intended to be helpful abstractions, we may be
adding too many irrelevant features, perhaps overwhelming the useful signals
present in the baseline feature set. There does not seem to be a consistent or
significant change due to using the lemmatized or surface forms.

However, we get substantially better results when we limit the Brown cluster
features to a narrow context words of three words on either side of the focus
word; this seems to lead to roughly the same results as the baseline features,
for the maximum entropy classifiers. But for the random forest classifiers, we
see consistent gains of three to six tenths of a percentage point. So for
random forest classifiers, it seems worthwhile to add these window-based Brown
cluster features. Within the ``window only" variants of the Brown cluster
features, the scores are fairly similar, but by a small margin we see the
strongest results coming from using the clusters trained on the surface form of
our Spanish Wikipedia.

Neither of these corpora, Europarl or the Spanish Wikipedia, are similar in
genre to the Bible verses in our test set, but we can get a small benefit from
adding the Brown cluster features here. The substantially larger size of the
Wikipedia corpus does not seem to confer clusters that are much more helpful;
if there had been a large performance gap between using the Wikipedia and
Europarl clusters, we could have run additional experiments with clusters
trained on a 2-million sentence sample of Wikipedia to determine whether it was
the additional size or the genre of the text that had improved performance.

\subsection{Results: combining sparse features}
\begin{figure*}
  \begin{centering}
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\

    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline
    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
    +syntactic features, maxent l1 & 0.465 & 0.511 & 0.450 & 0.422 \\
    \hline
    combined sparse features & 0.464 & 0.510 & 0.448 & 0.422 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
    +syntactic features, maxent l2 & 0.480 & \textbf{0.530} & 0.465 & 0.441 \\
    \hline
    combined sparse features & 0.479 & \textbf{0.530} & 0.465 & \textbf{0.443} \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
    +syntactic features & 0.486 & 0.527 & 0.471 & 0.434 \\
    \hline
    combined sparse features & \textbf{0.488} & 0.528 & \textbf{0.472} & 0.437 \\
    \hline
  \end{tabulary}
  \end{centering}
  \caption{Results for baseline features with the sparse features introduced in
  this chapter: features from a POS tagger, a dependency parser, and Brown
  clustering.}
  \label{fig:sparse-combination-results}
\end{figure*}

We present the experimental results for combinations of all of the sparse
features presented in this chapter in
Figure~\ref{fig:sparse-combination-results}. Here in general we see some good
gains over the baseline features, from a few tenths of a percentage point to
just over a full percentage point; we see that as before, the maximum entropy
classifiers do not seem to benefit from the addition of Brown cluster features,
but they do benefit from the syntactic features. The random forest classifiers
benefit from both additional kinds of features; our best performance with the
sparse feature sets is with random forests given these combined features, and
this is consistent for both language pairs, and in the regular and non-null
settings.

\subsection{Results: word2vec embeddings alone}

\begin{figure*}
  \begin{centering}
  {\footnotesize
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\

    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline
    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
fullsent, europarl & 0.421 & 0.465 & 0.406 & 0.369 \\
    \hline
fullsent, europarl, mwes & 0.421 & 0.465 & -     & -     \\
    \hline
fullsent, wikipedia & 0.428 & 0.468 & 0.408 & 0.374 \\
    \hline
fullsent, wikipedia, mwes & 0.428 & 0.469 & -     & -     \\
    \hline
pyramid, europarl & 0.463 & 0.507 & 0.448 & 0.418 \\
    \hline
pyramid, europarl, mwes & 0.462 & 0.505 & 0.447 & 0.418 \\
    \hline
pyramid, wikipedia & \emph{0.469} & \emph{0.512} & \emph{0.453} & \emph{0.424} \\
    \hline
pyramid, wikipedia, mwes & \emph{0.469} & \emph{0.512} & 0.452 & 0.423 \\
    \hline
window, europarl & 0.462 & 0.506 & 0.447 & 0.417 \\
    \hline
window, europarl, mwes & 0.460 & 0.504 & 0.447 & 0.416 \\
    \hline
window, wikipedia & 0.468 & 0.511 & 0.451 & 0.423 \\
    \hline
window, wikipedia, mwes & 0.467 & 0.511 & 0.450 & 0.422 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
fullsent, europarl & 0.421 & 0.469 & 0.406 & 0.375 \\
    \hline
fullsent, europarl, mwes & 0.421 & 0.469 & -     & -     \\
    \hline
fullsent, wikipedia & 0.427 & 0.472 & 0.408 & 0.379 \\
    \hline
fullsent, wikipedia, mwes & 0.427 & 0.472 & -     & -     \\
    \hline
pyramid, europarl & 0.459 & 0.505 & 0.443 & 0.417 \\
    \hline
pyramid, europarl, mwes & 0.457 & 0.504 & 0.443 & 0.418 \\
    \hline
pyramid, wikipedia & 0.465 & \emph{0.512} & 0.449 & \emph{0.424} \\
    \hline
pyramid, wikipedia, mwes & 0.464 & 0.511 & 0.449 & 0.423 \\
    \hline
window, europarl & 0.455 & 0.501 & 0.440 & 0.414 \\
    \hline
window, europarl, mwes & 0.453 & 0.500 & 0.439 & 0.413 \\
    \hline
window, wikipedia & 0.461 & 0.507 & 0.445 & 0.420 \\
    \hline
window, wikipedia, mwes & 0.461 & 0.506 & 0.445 & 0.420 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
fullsent, europarl & 0.432 & 0.474 & -     & -     \\
    \hline
fullsent, europarl, mwes & 0.431 & 0.474 & -     & -     \\
    \hline
fullsent, wikipedia & 0.432 & 0.477 & -     & -     \\
    \hline
fullsent, wikipedia, mwes & 0.433 & 0.476 & -     & -     \\
    \hline
window, europarl & 0.447 & 0.491 & -     & -     \\
    \hline
window, europarl, mwes & 0.448 & 0.489 & -     & -     \\
    \hline
window, wikipedia & 0.453 & 0.494 & -     & -     \\
    \hline
window, wikipedia, mwes & 0.451 & 0.493 & -     & -     \\
    \hline
  \end{tabulary}
  } %% end footnotesize
  \end{centering}
  \caption{Results for classification using only word2vec skipgram embeddings
to create features. For space, here we only show results for 200-dimensional
embeddings.}
  \label{fig:word2vec-alone-results-skipgram}
\end{figure*}

\begin{figure*}
  \begin{centering}
  {\footnotesize
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\
    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline
    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
fullsent, europarl & 0.389 & 0.433 & -     & -     \\
    \hline
fullsent, europarl, mwes & 0.388 & 0.434 & -     & -     \\
    \hline
fullsent, wikipedia & 0.391 & 0.433 & -     & -     \\
    \hline
fullsent, wikipedia, mwes & 0.391 & 0.433 & -     & -     \\
    \hline
pyramid, europarl & -     & -     & \emph{0.401} & 0.374 \\
    \hline
pyramid, wikipedia & -     & -     & 0.401 & 0.375 \\
    \hline
window, europarl & 0.414 & 0.455 & 0.396 & 0.367 \\
    \hline
window, europarl, mwes & 0.411 & 0.454 & -     & -     \\
    \hline
window, wikipedia  & 0.414 & 0.456 & 0.395 & 0.368 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
fullsent, europarl & 0.392 & 0.442 & -     & -     \\
    \hline
fullsent, europarl, mwes & 0.392 & 0.441 & -     & -     \\
    \hline
fullsent, wikipedia & 0.394 & 0.440 & -     & -     \\
    \hline
fullsent, wikipedia, mwes & 0.394 & 0.441 & -     & -     \\
    \hline
pyramid, europarl & -     & -     & 0.398 & 0.375 \\
    \hline
pyramid, wikipedia & -     & -     & 0.398 & \emph{0.377} \\
    \hline
window, europarl & 0.403 & 0.448 & 0.386 & 0.363 \\
    \hline
window, europarl, mwes & 0.402 & 0.447 & -     & -     \\
    \hline
window, wikipedia & 0.408 & 0.451 & 0.389 & 0.366 \\
    \hline
window, wikipedia, mwes & 0.406 & 0.451 & -     & -     \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
fullsent, europarl & 0.432 & 0.472 & -     & -     \\
    \hline
fullsent, europarl, mwes & 0.431 & 0.474 & -     & -     \\
    \hline
fullsent, wikipedia & 0.434 & 0.478 & -     & -     \\
    \hline
fullsent, wikipedia, mwes & 0.434 & 0.478 & -     & -     \\
    \hline
window, europarl & 0.449 & 0.492 & -     & -     \\
    \hline
window, europarl, mwes & 0.448 & 0.489 & -     & -     \\
    \hline
window, wikipedia & \emph{0.452} & \emph{0.494} & -     & -     \\
    \hline
window, wikipedia, mwes & 0.451 & \emph{0.494} & -     & -     \\
    \hline
  \end{tabulary}
  } %% end footnotesize
  \end{centering}
  \caption{Results for classification using only word2vec CBOW embeddings
to create features. For space, here we only show results for 200-dimensional
embeddings.}
  \label{fig:word2vec-alone-results-cbow}
\end{figure*}

Taking a look at Figures \ref{fig:word2vec-alone-results-skipgram} and
\ref{fig:word2vec-alone-results-cbow}, we see the results for the experiments
for using only word2vec embeddings for our features. There were many
variations, but we see a few general trends. We performed experiments for 50,
100, 200, and 400 dimensions, but the dimensionality of the vectors did not
seem to substantially change the results, more than a few tenths of a
percentage point. Here we report the results for 200 dimensions to save space
and for easier comparison; in general the results for 200 dimensions were on
the slightly more favorable side of representative. Also, because there were so
many variations to try, some of the less promising combinations were cut.
Specifically, almost all of the experiments with the CBOW embeddings, the
``fullsent" combination approach, and random forest classifiers gave results
lower than the MFS baseline, so not all combinations were carried out.

However, if we focus on Figure~\ref{fig:word2vec-alone-results-skipgram}, we
can see some fairly promising results. We note that for the maxent classifiers,
we can often surpass the MFS baseline by using the word2vec embedding features
alone, and for some settings, even surpass the baseline features. Particularly,
we note that using the ``window" and ``pyramid" results, we can do fairly well
-- the ``fullsent" approach does not seem to help here -- and that for the
L1-norm maxent classifier, we can outperform the baseline features with
word2vec features alone.

Interestingly, despite its strong performance on the sparse feature sets, the
random forest classifier does not do as well with the dense vectors; it does
not outperform the MFS baseline.  In general, the L1-norm maxent classifier
seems to do best with these feature sets, and the embeddings trained on the
larger (and more general-domain) Wikipedia corpus seem to be most effective;
using the multi-word expression embeddings seems to have little to no effect.
The ``pyramid" combination scheme generally comes out on top. So this is
encouraging, for features for maxent classifiers.

\subsection{Results: doc2vec embeddings alone}

\begin{figure*}
  \begin{centering}
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\
    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline
    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
    fullsent & 0.462 & 0.496 & 0.441 & 0.391 \\
    \hline
    window & 0.463 & 0.502 & 0.445 & 0.395 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
    fullsent & 0.462 & 0.497 & 0.442 & 0.390 \\
    \hline
    window & \emph{0.467} & \emph{0.506} & \emph{0.449} & \emph{0.402} \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
    fullsent & 0.420 & 0.497 & 0.404 & 0.390 \\
    \hline
    window & 0.421 & 0.461 & 0.405 & 0.352 \\
    \hline
    \hline
  \end{tabulary}
  \end{centering}
  \caption{Top results for classification using only doc2vec embeddings to
  create features. For comparison, also included are the MFS baseline and the
  top results from the previous chapter.}
  \label{fig:doc2vec-alone-results}
\end{figure*}

Taking a look at the results for using doc2vec embeddings on their own,
presented in Figure~\ref{fig:doc2vec-alone-results}, we see a few general
trends. As with the word2vec feature sets, the random forests did not
outperform the MFS baseline when given a dense vector as input. Also as we saw
with the word2vec features, trying to build a representation of the
entire sentence was not as effective as building a vector to represent just the
tokens around the focus word.
The full-sentence representations were more effective than the corresponding
full-sentence vector sums from word2vec, however -- in a few instances they
outperform the MFS baseline.

However, when we restrict the doc2vec optimizer to looking at a narrow context
window around the focus word, we outperformed the MFS baseline on all of the
test sets, for maxent classifiers. For some, but not all, of the L1-norm
maxent cases, the doc2vec features surpass the baseline features by a slim
margin. There may be a way to make doc2vec embeddings, or some other kind of
document-level embedding features, work well for this setting, but further
exploration is outside the scope of this work.

\subsection{Results: combining neural embeddings with sparse features}
\begin{figure*}
  \begin{centering}
  \begin{tabulary}{\textwidth}{|R|L|L|L|L|}
    \hline
    classifier & es-gn regular & es-gn non-null & es-qu regular & es-qu non-null \\

    \hline
    MFS    & 0.456 & 0.498 & 0.435 & 0.391 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l1} \\
    \hline
    baseline features & 0.461 & 0.506 & 0.444 & 0.414 \\
    \hline
    word2vec alone  & 0.469 & 0.512 & 0.453 & 0.424 \\
    \hline
    +sparse features & \emph{0.472} & 0.519 & \emph{0.458} & 0.436 \\
    \hline
    \hline

    \multicolumn{5}{|l|}{maxent l2} \\
    \hline
    baseline features & 0.475 & 0.524 & 0.458 & 0.431 \\
    \hline
    word2vec alone & 0.465 & 0.512 & 0.449 & 0.424 \\
    \hline
    +sparse features & \emph{0.472} & \emph{0.522} & \emph{0.458} & \emph{0.438} \\
    \hline
    \hline

    \multicolumn{5}{|l|}{random forest} \\
    \hline
    baseline features & 0.481 & 0.520 & 0.464 & 0.424 \\
    \hline
    word2vec alone & 0.452 & 0.493 & 0.438 & 0.394 \\
    \hline
    +sparse features & 0.461 & 0.504 & 0.449 & 0.410 \\
    \hline
  \end{tabulary}
  \end{centering}
  \caption{Results for combining word2vec embeddings (Wikipedia skipgrams, 200
  dimensions, ``pyramid" strategy) with syntactic features and Brown clusters.
  For comparison, also included are the MFS baseline, word2vec results without
  additional features.}
  \label{fig:pyramid-extras-results}
\end{figure*}

In Figure~\ref{fig:pyramid-extras-results}, we present the experimental results
for combining the most successful word2vec embeddings with the most promising
additional sparse features described in this chapter. In these experiments, we
started with the 200-dimensional Wikipedia embeddings, combined them in the
``pyramid" style, and also made the Brown cluster and syntactic features
available to the classifiers to see if these would improve performance.

And in general, we see that the addition of these features does make an
improvement over using the word2vec features by themselves. In fact, for all of
the classifiers, and each of the test sets, we see an improvement over the
word2vec features; this only results in consistent improvements over the
baseline features for the L1-norm maximum entropy classifer, however. But it is
the best results presented thus far for that classifier, and the most effective
feature set that we have tried, using neural embeddings.

\section{Discussion}
In this chapter, we have shown a number of different approaches for making use
of the available resources for our source language in the CL-WSD task,
including three different ways of learning from unannotated source-language
text -- clustering and two kinds of neural embeddings -- and making use of
existing source-language syntactic analysis tools.

Using each of these different tools, we were able to overcome the relatively
strong most-frequent sense baseline, and with many of the different feature
sets, we were able to show small but consistent gains over the baseline feature
set described in the previous chapter. Overall, the best results posted for
Spanish-Guarani were with the combined sparse features; the random forest
classifier had the strongest results for the ``regular" setting, and the
maximum entropy classifier with L2 regularization came out ahead for the
non-null setting (this result was tied with the one for using the syntactic
features with maximum entropy, L2 regularization). 
For Spanish-Quechua, the top results were similarly posted with the combined
sparse features, with random forests coming out ahead for the ``regular"
setting and maximum entropy, L2, posting the best results for ``non-null".

All of the approaches explored in this chapter make more abstract
representations of the source text available to our classifiers, whether these
representations were learned from the available text, or encapsulate some kind
of syntactic knowledge about the language.

We found, in general, that adding features that represent the text closely
surrounding the focus word was more effective than attempting to represent the
entire source-language sentence at once; we observed this effect for Brown
cluster features, and for features built from both word and document
embeddings.
Additionally, we found that we could not only add some of these features to our
baseline sparse feature set, but also that our new sparse features could be
added to dense feature vectors based on word2vec embeddings, resulting in gains
over using the neural embeddings by themselves.

In general, our random forest classifiers seem to benefit more from adding many
sparse features, like the ones we extracted from Brown clusters, than the
maxent classifiers do. However, the random forest classifiers do not perform as
well when given dense vectors, like those produced by word2vec and doc2vec.
Contrastingly, the maximum entropy classifiers have better performance in this
setting.

In the next chapter, we will make use of another one of our available resources
for a resource-rich language like Spanish: bitext with other language pairs.

\chapter{Learning from Monolingual Data}
\label{chap:monolingual}
While in this work our target languages are under-resourced, we have many
resources available for the source languages. We would like to use these to make
better sense of the input text, giving our classifiers clearer signals and
better representations for lexical selection in the target language.  The
approaches considered in this chapter make additional features or different
representations available to the CL-WSD classifiers, based on knowledge of the
source language; since we have relatively little bitext available for
Spanish-Guarani and Spanish-Quechua, we will need to lean more on our Spanish
resources, including tools and data, in order to make better sense of the input
text.

Perhaps most saliently for Spanish, we have abundant monolingual text
available, which suggests that we could use unsupervised methods to discover
regularities in the language, yielding better features for our classifiers.
This approach has been broadly successful in the literature
\cite{turian-ratinov-bengio:2010:ACL}
%% XXX: add some more references
, and here we adapt some of the methods explored in previous work on text
classification to our task. Concretely, in this chapter we explore Brown
clustering \cite{brown1992class}, two related approaches to neural embeddings,
and labels learned from other NLP tools such as off-the-shelf taggers and
parsers for Spanish. There are of course other related methods that one could
investigate, making use of the deep available literature on distributional
semantics, but these will be left to future work, either by this author or other
researchers.

%% XXX: maybe move the less interesting stuff earlier in the chapter? We kind
%% of want to go out on a high note with the embedding ideas.

\section{Brown Clustering}
%% XXX: add citations here!
The Brown clustering algorithm, also known as IBM clustering, since it was
developed by the Candide group at IBM, takes an unannotated text corpus as
input and assigns each word type in the corpus into hierarchical clusters such
that types in the same cluster have similar usage patterns according to the
bigram statistics of the corpus.

We can then use this clustering to create more features for our classifiers; we
can consider the clusters into which a word falls as a sort of tag.  These
annotations for each token describe a more abstract, less sparse representation
than than that of surface forms or lemmas. The desired number of clusters must
be set ahead of time, but is a tunable parameter.

In this work we use Percy Liang's popular open source implementation of Brown
clustering, described in his masters work \cite{Liang05semi-supervisedlearning}
\footnote{And available at \url{https://github.com/percyliang/brown-cluster}}.
Here for Spanish we have used this tool on a variety of corpora, varying in
size from the Bible, to the rather larger Europarl corpus \cite{europarl}, to
larger still, a dump of the entirety of the Spanish-language Wikipedia.
%% XXX: add explicit sizes to this paragraph. How many words are they?

Brown clustering uses greedy optimization to find a clustering  -- an assignment
of the word types present in the input corpus into a fixed number of
hierarchical clusters -- that maximizes the probability of the input corpus.  As
the original intention of the approach was for class-based language models, the
scoring function is the mutual information between two immediately subsequent
tokens (i.e., a bigram).  Concretely, the optimization process is looking for a
clustering $C$ that maximizes the probability of the corpus $\boldsymbol{w}$,
according to the following formula.

\begin{equation} \label{eq:brownclassprob}
P(\boldsymbol{w}; C) = \prod_{w_i} p(w_i | C(w_i)) p(C(w_i) | C(w_{i-1}))
\end{equation}

Finding a globally optimal assignment turns out to be an intractable problem,
but several greedy approaches that find local optima have been explored in the
literature.  Notably, in addition to Liang's approach, Franz Och's
\texttt{mkcls} package (described in \cite{och1999efficient}), has been
distributed with Moses for many years, and optimizes the same function.

When applying Brown clustering to this task, we would like to answer several
relevant questions.

\begin{itemize}
  \item Can we use unannotated text to learn useful information about the
  Spanish language and meanings of Spanish words, and apply this to our task?
  \item Does more source-language text help us learn ``better" Brown clusters,
    with respect to the CL-WSD task?
  \item How does the genre of the input text affect our performance, and how
  does this relate with the size of the text? Is there a size at which we
    can learn a ``better" clustering that overcomes genre mismatches?
  \item What kinds of preprocessing should we do on the source text? Should the
  source text be lemmatized, for Spanish? It seems as though part of the point
  of Brown clustering is to find abstractions over syntax, but perhaps
  lemmatization helps it find more semantic abstractions in practice.
  If it's true, that's a worthwhile thing to note -- to use Brown clustering on
  Spanish text, maybe you need to lemmatize it first.
  \item Similar open questions for word2vec.
\end{itemize}

We can use such a clustering $C$ (i.e., a mapping from word types to clusters)
to extract a number of different kinds of features for our classifiers;
we may also choose a variety of inputs to the clustering algorithm in the first
place.

%% Some of the features ...

%% XXX: turn this into a figure
We tried, for example, dropping a bag of all the clusters that occur in the
sentence, and prefix features for all the clusters that appear too!

It might be the case that we really just need to add the very local context --
three words on a side? In Turian et al, they do two words on a side.


\section{Clustering Spanish text for this work}

%% TODO: include examples of the clusters that we learn from different corpora
%% specify lemmatized versus not lemmatized

Here's an interesting thing to note: when we do clustering on lemmas, we seem
to pick up more on ``semantic" categories of things. Note the ``infrastructure"
cluster below -- it's got words that all pertain to infrastructure!

When we don't use lemmas, the cluster that contains ``infraestructura" contains
these words: \emph{formación educación tecnología infraestructura publicidad
enseñanza ocupación religión vivienda ética propaganda banda fusión huelga
creatividad medicina manipulación tribuna cualificación comida}. It seems like
this is more about grammatical gender: those are all singular feminine nouns.

In fact, the words in many of the clusters all seem to have the same
grammatical gender and number. To get more semantic similarity, maybe we should
cluster on lemmas -- otherwise the morphological features of Spanish could
overwhelm the clustering approach.

\TODO{replace with current example}
\begin{figure*}[t!]
  \begin{tabular}{|r|p{10cm}|}
    \hline
    category  & top twenty word types by frequency \\
    \hline
    countries & francia irlanda alemania grecia italia españa rumanía portugal polonia suecia bulgaria austria finlandia hungría bélgica japón gran\_bretaña dinamarca luxemburgo bosnia \\
    \hline
    more places & kosovo internet bruselas áfrica iraq lisboa chipre afganistán estrasburgo oriente\_próximo copenhague asia chechenia gaza oriente\_medio birmania londres irlanda\_del\_norte berlín barcelona \\
    \hline
    mostly people & hombre periodista jefes\_de\_estado individuo profesor soldado abogado delincuente demócrata dictador iglesia alumno adolescente perro chico economista gato jurista caballero bebé \\
    \hline
    infrastructure & infraestructura vehículo buque servicio\_público cultivo edificio barco negocio motor avión monopolio planta ruta coche libro aparato tren billete actividad\_económica camión \\
    \hline
    common verbs & pagar comprar vender explotar practicar soportar exportar comer consumir suministrar sacrificar fabricar gobernar comercializar cultivar fumar capturar almacenar curar beber \\
    \hline
  \end{tabular}
\caption{Some illustrative clusters found by the Brown clustering algorithm on
the Spanish Europarl data. These are five out of $C=1000$ clusters, and
were picked and labeled by hand. The words listed are the
top twenty terms from that cluster, by frequency.}
\label{fig:clusters}
\end{figure*}

Figure \ref{fig:clusters} shows some illustrative examples of clusters that
we found in the Spanish Europarl corpus.  Examining the output of the
clustering algorithm, we see some intuitively satisfying results; there are
clusters corresponding to the names of many countries, some nouns referring to
people, and common transitive verbs. Note that the clustering is unsupervised,
and the labels given are not produced by the algorithm.

We additionally show the effects on classification accuracy of adding features
derived from Brown clusters, with clusters extracted from both the Europarl
corpus and the Spanish side of our training data.
We tried several different
settings for the number of clusters, ranging from $C=100$ to $C=2000$.
In all of our experimental settings, the addition of Brown cluster features
substantially improved classification accuracy. We note a consistent upward
trend in performance as we increase the number of clusters, allowing the
clustering algorithm to learn finer-grained distinctions.
The training algorithm takes time quadratic in the number of clusters,
which becomes prohibitive fairly quickly, so even finer-grained distinctions
may be helpful, but will be left to future work. On a modern Linux
workstation, clustering Europarl (~2M sentences) into 2000 clusters took
roughly a day.

The classifiers using clusters extracted from the Spanish side of our bitext
consistently outperformed those learned from the Europarl corpus. We had an
intuition that the much larger corpus (nearly two million sentences) would
help, but the clusters learned in-domain, largely from the Bible, reflect
usage distinctions in that domain. Here we are in fact cheating slightly, as
information from the complete corpus is used to classify parts of that corpus.


\subsection{Learning Clusters from Europarl}

\subsection{Learning Clusters from Wikipedia}


\section{Neural Word Embeddings}
Another rich source of features that has proved useful for many text
classification problems in recent years is neural word embeddings, perhaps most
famously developed in the work of Mikolov et al. \cite{mikolovword2vec} and
their associated open source implementation of the technique,
word2vec\footnote{Available at
\url{https://code.google.com/archive/p/word2vec/}}. In this work we investigate
the use of ``word2vec"-style word embeddings, as well as the related technique,
``doc2vec" or ``paragraph vectors"
\cite{dai-document-embedding-2015,quocle-distributed-representations-2014}.
These techniques let our classifiers operate on lower-dimensional dense vectors
(of a few hundred dimensions, typically), as opposed to the high-dimensional
and sparse vectors typically used in earlier NLP literature\footnote{
What we might consider to be ``symbolic" binary features used in machine
learning systems for NLP in recent decades -- such as ``the word `dog` appears
in the sentence" -- are effectively equivalent to these very sparse ``one hot"
representations of words, in which, for a vocabulary of size $|V|$, words are
represented by a length-$V$ vector in which all but one of the elements are 0}.

There is a rich literature on continuous representations for words; the idea
has a long lineage in NLP, and we could try any number of dimensionality
reduction or distributional semantics approaches here. Recent empirical work by
Baroni et al \cite{baroni2014don} (summed up by the title of the paper, ``Don't
Count, Predict!") has shown that representations built during discriminative
learning tasks are typically more effective for text classification problems
similar to ours.

Unlike Brown Clustering, which infers hierarchical clusters for each word type,
but like other embedding techniques, word2vec learns multidimensional
representations for word types, turning the very sparse ``one hot"
representation in which a words with similar uses or meanings do not have any
obvious similarity into a much denser continuous space in which words with
related meanings are placed nearby. These ``embeddings", or placements of word
types into a continuous space, are learned during some other classification
task, and are performed by the early layers of a neural network.

The word2vec model in particular has two variations, useful in different
contexts. One, called Continuous Bag-of-Words (CBOW) learns a classifier that
can predict individual words based on their context, while the ``skip-grams"
variant does the reverse and learns to predict context words based on an
individual focus word. According to the literature, CBOW is more appropriate
when training on smaller data sets...

%% XXX: it seems like CBOW would actually work better for larger data sets,
%% intuitively? Get a good understanding of this.
In either case, running a word2vec training process results in a static mapping
from word types to their embeddings in a vector space of a given size,
typically a few hundred dimensions. These embeddings have been shown to be
helpful as features in a number of different NLP tasks, allowing us to learn
richer representations of word types from plentiful unannotated monolingual
text. This, in effect, turns what was a purely supervised task, requiring
labeled training data, into a semi-supervised task, in which the
representation-learning phase can be carried out on unlabeled data.


%% from the tensorflow tutorial on word2vec
%% https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html#vector-representations-of-words
% This inversion might seem like an
% arbitrary choice, but statistically it has the effect that CBOW
% smoothes over a lot of the distributional information (by treating an
% entire context as one observation). For the most part, this turns out
% to be a useful thing for smaller datasets. However, skip-gram treats
% each context-target pair as a new observation, and this tends to do
% better when we have larger datasets. We will focus on the skip-gram
% model in the rest of this tutorial.

We trained a variety of word2vec embeddings for our text classification tasks,
training on the Spanish-language section of the Europarl corpus (roughly 57
million words and 2 million sentences) and a dump of Spanish Wikipedia (449
million words, 20 million sentences). We learned embeddings using both CBOW and
skip-gram approaches, in 50, 100, 200, and 400 dimensions. In all cases, we
used the associated \texttt{word2phrase} tool, distributed with the initial
\texttt{word2vec} implementation, which finds collocations in the input text
and treats them as individual tokens. The same collocations must be identified
in the input text for their embeddings to be used.

We can visualize the learned embeddings with t-SNE \cite{van2008visualizing},
a popular technique for projecting high-dimensional data conveniently into two
or three dimensions.

\subsection{From word embeddings to classification features}
We should note that the embeddings learned for a word type are static and do
not reflect the context of their tokens, so it is not particularly useful to
take the word vector for the focus word on its own. This leaves us with the
problem of how to make use of the word vectors in a given sentence.

There are a variety of approaches we could take, even given the assumption that
we want to combine the vectors by summing or averaging. We could sum
(element-wise) all of the word vectors for all the tokens in the entire
sentence. We could only consider the words in a narrow window around the focus
word. Or we could perform a weighted sum, in which words nearer to the focus
word are given more weight, but these weights drop off according to some
function, the farther away they are from the focus word.

%% XXX add some citations for papers where people use word embeddings as
%% features

\section{Neural Document Embeddings}
The same researchers that produced word2vec and their collaborators have also
developed approaches that build representations for sequences of text, rather
than individual word types, described in
\cite{dai-document-embedding-2015,quocle-distributed-representations-2014}.
This approach, called ``paragraph vectors" or ``doc2vec" takes unlabeled text
and learns representations for word types jointly with representations for each
\emph{document}. Here a ``document" is an arbitrary sequence of tokens, perhaps
a single sentence, or perhaps much longer, depending on the intended
application.

Similar to word2vec, during training, a neural network with a single hidden
layer is used to predict words in the current context. In this extension,
however, the hidden layer contains not only an embedding trained for the
current word type, but also an embedding for the document. Training proceeds by
loading the current representation for
%% XXX: what's the current word, in this context? Are we doing skip-grams in
%% the doc2vec case?
the current word and the current document, attempting to predict some
other word in the context (analogous to training for word2vec), and then
updating both representations based on the gradients between the desired output
and the actual output. The stored model is then just the embeddings for
individual word types; the embeddings for any particular document are not
useful since that specific document is unlikely to appear again. Contrastingly,
the embeddings for individual word types provide a useful generalization, since
they constrain embeddings for new documents.

At inference time, doc2vec produces new vectors describing previously unseen
documents, based on these fixed word embedding vectors. In our work, as in many
text classification tasks, documents will be input sentences, or perhaps
sentence-length Bible verses. To produce a representation for a new document,
the embeddings for word types are held constant and we run the optimization
process (stochastic gradient descent or similar) to infer an embedding for the
current document, such that it helps predict words in the current context.

doc2vec thus provides a straightforward approach for learning to produce
representations of new documents, based on an unlabeled training corpus. Here
we use the implementation of doc2vec provided by the gensim package
\cite{rehurek-lrec}.

\section{Monolingual features from other NLP tools}
In addition to large quantities of monolingual text, another important resource
for Spanish is the large number of available off-the-shelf NLP tools, such as
POS taggers and syntactic parsers. These can also help us capture abstractions
about particular word types (e.g., perhaps a noun in the window surrounding a
focus word is indicative of some particular meaning) and syntactic structure
may also provide useful features.  In the Chipa software, we can make use of
all kinds of annotations for our text (see Section \ref{sec:annotations}), so
adding more features based on analysis by external tools is straightforward.

As a first step, we synthesize features based on the part-of-speech tags of the
tokens in a window around the current focus word, and using a syntactic parser,
the heads and children of the current focus word.
In principle, we could use other annotations, such as sense annotations from a
monolingual WSD system, given a good one.
This idea is akin to one that we will explore in Chapter
\ref{chap:multilingual}

For POS tagging and parsing, we run the open source FreeLing text analysis
suite \cite{padro12} on the training data, and at inference time, on new input
sentences. FreeLing can perform a number of analysis tasks for Spanish,
including lemmatization, POS tagging, named entity recognition, and dependency
parsing.
%% XXX: make some nice figures for the features extracted from FreeLing

\section{Experiments}
Here we repeat the experiments from Chapter \ref{chap:evaluation}, with the
addition of features extracted from Brown clusters, neural word embeddings,
document vectors, and FreeLing text annotations.

There are lots of combinations here...

\begin{itemize}
  \item es-gn, es-qu with pos tags
  \item es-gn, es-qu with syntactic heads
  \item es-gn, es-qu with syntactic children
  \item es-gn, es-qu with all syntactic features
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl brown clusters
  \item es-gn, es-qu with wikipedia brown clusters
  \item es-gn, es-qu with europarl and wikipedia brown clusters
  \item es-gn, es-qu with word2vec embeddings of dimensions (50, 100, 200, 400)
  from both europarl and wikipedia
\end{itemize}

\begin{itemize}
  \item es-gn, es-qu with europarl embeddings
  \item es-gn, es-qu with wikipedia embeddings
  \item es-gn, es-qu with europarl and wikipedia embeddings
  \item es-gn, es-qu with pos tags and syntactic heads
\end{itemize}

\begin{itemize}
  \item combine the most promising ones
\end{itemize}

\chapter{Summary and Outlook}
\label{chap:conclusions}

In this work, I have developed approaches for cross-lingual word sense
disambiguation in the setting where we are translating from a resource-rich
language into an under-resourced one, and implemented those approaches in
running software.
We have shown how cross-lingual word sense disambiguation can be applied to
lexical selection in hybrid machine translation for language pairs with
relatively modest resources, particularly when translating from Spanish to two
different indigenous languages of South America, Guarani and Quechua.

We described, in Chapter \ref{chap:baseline}, an initial version of the Chipa
software based on common text classification features, and showed how it
outperforms the relatively strong most-frequent sense baseline. We also
compared Chipa to other systems submitted to recent SemEval events for
cross-lingual word sense disambiguation on the task of translating from English
to other European languages.

In Chapter \ref{chap:monolingual}, we extended our initial approach with
strategies that make use of the available resources for Spanish, including NLP
tools for analyzing Spanish text and unsupervised learning techniques that
allow us to learn representations for our classifiers from the wealth of
available unannotated Spanish text, including Brown clusters and neural
embeddings that represent both individual word types and sequences of text.

In Chapter \ref{chap:multilingual}, we showed how to use Chipa for classifier
stacking, which allows us to learn from bitext corpora that pair our source
language with languages other than our intended target language; this technique
is broadly applicable when we want to translate out of a language with many
available bitext resources, and could also be applied when we have a good
monolingual WSD system available.

Finally, in Chapter \ref{chap:integration}, we demonstrated prototypes for
integrating our CL-WSD classifiers into existing machine translation systems of
two completely different architectures, requiring only small changes to
existing code.  We demonstrated a phrase-based SMT system for
Spanish-Guarani\footnote{Ignoring, for the moment, the admittedly substantial
problem of morphological generation.} that uses Chipa classifiers in a feature
function that guides its decoding process. For Spanish-Quechua, we augmented a
primarily rule-based hybrid machine translation system, using our classifiers
to supplement the existing hand-written lexical selection rules.

\section{Applying techniques like this to neural machine translation}
During the course of this work, the field of machine translation has undergone
a dramatic shift, in which statistical MT research, and in many cases,
practice\cite{gnmt}, has moved from phrase-based and tree-based SMT systems to
models based on recurrent neural networks.

It may not be immediately useful to add explicit CL-WSD classifiers to a neural
MT architecture, although this is an empirical question. In a sense, though,
we can say that NMT systems are already performing CL-WSD; they have a
representation of the whole input sentence available while making output
decisions. This vector representation is similar to the doc2vec
representations discussed in Chapter \ref{chap:monolingual}, but is learned
specifically for the translation task, and uses deeper, and recurrent,
networks. The sentence-encoding process in neural machine translation
automates much of the feature engineering work that would go into building a
CL-WSD system.

However, many of the difficulties addressed in this work still apply in the
neural machine translation setting. We still must come up with good ways to
leverage the available corpora and tools for resource-rich source languages,
when we have little available bitext. Can source-language taggers and parsers
be integrated with NMT? Will this help when translating into under-resourced
languages? How can other existing NLP tools be applied, such as monolingual
WSD?

In our CL-WSD setting, we did not find immediate benefits when using neural
embeddings based on comparatively large source-language corpora. How can
monolingual source-language resources be used for NMT, when only very small
bitext corpora are available?  There has been some recent work, including by
Johnson \emph{et al.} \cite{TACL1081}, on training multilingual neural machine
translation systems that learn to share representations among different
translation tasks; these systems can even perform ``zero shot" translation, in
which a single network trained to translate from, \emph{e.g.}, Portuguese to
English and English to Spanish can also translate reasonably well from
Portuguese to Spanish.
Additionally, there has been work, such as that of Artetxe \emph{et al.}
\cite{artetxe2018unsupervised} and Lample \emph{et al.}
\cite{lample2018unsupervised}, that creatively uses back-translation and
adversarial networks to make up for the lack of bitext for some language pairs,
but to my knowledge this has not yet been practically applied to translating
into under-resourced languages.  Broadly, there do not yet seem to be
widely-accepted approaches for leveraging existing resources to improve neural
translation into under-resourced languages, but neural machine translation is
still in its early stages and the field is developing rapidly.

Hopefully we will see effective strategies for making use of what we do have
for building NMT systems targeting under-represented and under-resourced
languages in the near future.

\section{Building resources for Guarani and other under-represented languages}
\label{sec:crowdsourcing}

While we have explored techniques in this work that allow us to lean
somewhat on the resources available for resource-rich source
languages, a clear way to improve translation for currently under-resourced
languages is to make them less under-resourced. Languages like Guarani and
Quechua have a number of speakers comparable with that of some relatively
resource-rich European languages, as well as their own engaged activist
communities. Ethnologue, for example,
reports\footnote{\url{https://www.ethnologue.com/language/grn}} that there are
about six million Guarani speakers in the world, which is more than the number
of Danish or Norwegian speakers. To support these languages and better serve
these communities, we can as technologists find better ways to work with the
speakers of these languages.

While this work has focused on languages spoken in South America, there are
under-represented languages around the world with millions or possibly many
millions of speakers, but relatively little text on the web thus far, and no
easily available MT or other NLP tools. Many languages in India and Southeast
Asia are quite large by number of speakers, but nonetheless dramatically
under-resourced. However, Internet usage is spreading quickly throughout the
world, and the speakers of these languages are coming online; we are presented
with great opportunities to build great linguistic resources and NLP tools for
these languages.

There have already been fairly successful campaigns by technologists to work
with speakers of these languages on resource-gathering projects for them. For
example, in 2016, Mozilla Paraguay managed to localize the Firefox web browser
for Guarani, with the help of Guarani-speaking volunteers, a local university,
and Guarani-language activists\footnote{See a description of the process, and
the launch announcement, at
\url{http://mozillanativo.org/2016/lanzamiento-oficial-de-firefox-en-guarani.html}
(in Spanish). Firefox in Guarani can be downloaded at
\url{https://www.mozilla.org/gn/firefox/}}. Localizing an application as
complex as a web browser is an enormous task, requiring the translation of
thousands of sentence-length messages, along with the in-browser
documentation. Also notably, this task requires choosing appropriate
Guarani-language terms for technical concepts.

As a success story directly pertinent to machine translation, in 2013, Google
ran a crowdsourcing campaign in New Zealand that managed to collect
enough training data to build a phrase-based SMT system for the Māori
language\footnote{Post on the Google New Zealand blog:
\url{https://newzealand.googleblog.com/2013/12/kua-puta-google-whakamaori-ki-te-reo.html}};
since then, Google Translate has operated a crowdsourcing platform called
Google Translate Community\footnote{https://translate.google.com/community},
on which volunteers can rate and submit translations, including for some
languages that are not currently supported by the main Google Translate
product. For languages that are already supported by Google Translate, the
submissions and ratings are used to improve MT output.

Our research group has started some efforts along these lines, but a more
sustained effort will be needed to provide benefits for language communities in
practice. We (the author, working together with Alberto Samaniego and Taylor
Skidmore, directed by Michael Gasser) prototyped two websites for collecting
Guarani and Spanish-Guarani language resources, but these have not been used
beyond the prototype stage.  The first website is called ``Tahekami", which
means \emph{let's search together} in Guarani. Tahekami\footnote{Prototype code
at \url{http://github.com/hltdi/gn-documents}} is a repository of Guarani and
bilingual documents that allows full-text search and browsing documents by tag.
Our initial version of the site contained a collection of masters theses from
the \emph{Ateneo de Lengua y Cultura Guaraní}.
We developed a second website, called ``Guampa" \footnote{A ``guampa", in
Paraguay, is the cup from which one drinks yerba mate or tereré. The term
``guampa" is also local to Paraguay; in other parts of South America, the
container itself is called a ``mate". The Guampa software is available at
\url{https://github.com/hltdi/guampa}}, which is a bilingual wiki, designed
to be used by Guarani speakers and learners to collaboratively translate
documents from Spanish to Guarani or vice-versa. We presented an initial version
of this software at LREC 2014\cite{RUDNICK14.151}. This site is meant to serve
at least three purposes: helping Guarani-language learners practice and get
feedback on their translations, creating more Guarani-language documents for
the web (we started initially with translating Spanish Wikipedia), and building
bitext corpora for training MT systems.

Michael Gasser is currently, as of 2018, developing successors to these
websites, in the form of \emph{mitmita} and \emph{mainumby}, online
computer-aided translation systems for Amharic and Guarani
respectively\footnote{Code for these sites is being developed at
\url{https://github.com/hltdi/mitmita} and
\url{https://github.com/hltdi/mainumby}}. The goal of these tools is to help
users translate documents by providing both searches over translation memories
and automatic suggestions from an integrated machine translation system.

In any case, while there remains much work to be done --- both technical and
social --- for supporting the under-represented languages of the world, it is
outside the scope of this dissertation, and will be addressed in the future,
hopefully in part by the author.

\chapter{Summary and Outlook}
\label{chap:conclusions}

In this work, I have developed approaches for cross-lingual word sense
disambiguation in the setting where we are translating from a resource-rich
language into an under-resourced one, and implemented these approaches in
running software.
I have shown how cross-lingual word sense disambiguation can be applied to
lexical selection in hybrid machine translation for language pairs with
relatively modest resources, particularly when translating from Spanish to two
different indigenous languages of South America, Guarani and Quechua.

In the initial three chapters, we describe CL-WSD and situate it among efforts
to integrate word-sense disambiguation over the history of work on machine
translation. In Chapter \ref{chap:overview}, we explain how word-sense
disambiguation is relevant to translation and lay out the structure of the rest
of the work.

In Chapter \ref{chap:background}, we provide a broad overview of the field of
word-sense disambiguation, considering some framings for WSD that have been
applied in the past, and the difficulties that these framings cause when our
goal is to support MT, or to process arbitrary running text more generally.
We explain how the CL-WSD framing overcomes two major difficulties in applying
WSD to machine translation in practice. Firstly, CL-WSD provides a concrete
sense inventory for lexical items in the source language, since we consider the
senses of a word to be its possible translations, and secondly, it provides a
source of labeled training data in the form of any available bitext, thus
allowing us to apply supervised machine learning techniques. Also in this
chapter, we briefly describe Guarani and Quechua, two of the largest indigenous
languages in the Americas, and explain some motivations for building hybrid
rule-based and statistical MT systems to support them: for most of the world's
language pairs, there are no large bitext corpora available, so using
rule-based components may provide a useful starting point for NLP systems,
especially where there are well-known linguistic regularities, such as
morphology and syntax.

Chapter \ref{chap:relatedwork} covers some of the relevant related work,
including some of the work most similar to ours.  Especially notable are the
ParaSense work by Els Lefever \cite{lefever-hoste-decock:2011:ACL-HLT2011},
which pioneered the use of multilingual corpora to support CL-WSD, and the
development of learned lexical selection models for Apertium, a mostly
rule-based MT system, by Francis Tyers \cite{tyers-dissertation}.

Having described relevant context and background information, in Chapter
\ref{chap:evaluation}, we describe the evaluation settings for our CL-WSD
systems, as well as the available data sets for our language pairs, and
describe in detail the preprocessing steps that allow us to take sentence- or
verse-aligned bitext and produce training data suitable for training our
classifiers. For our \emph{in vitro} evaluation setting, we can report
classifier accuracy, or report benchmarks based on shared tasks like the 2010
and 2013 SemEval CL-WSD tasks. For \emph{in vivo} evaluation, we want to look
at the downstream task, and see how the addition of our CL-WSD approaches
contributes to machine translation quality, either through automatic metrics
like BLEU, or by examining MT output by hand.

We described, in Chapter \ref{chap:baseline}, an initial version of the Chipa
software based on common text classification features -- the tokens of the
input text, their lemmas, and then also the token and lemmas in a small context
window around the focus word -- and showed how it outperforms the relatively
strong most-frequent sense baseline. We compared a few different machine
learning algorithms on this task, and found that we got the best results with
either random forests or logistic regression classifiers, with L2
regularization, and this held consistently, across language pairs.
We also compared Chipa to other systems presented at recent SemEval events for
CL-WSD on the task of translating from English to other European languages,
using considerably more training data. By applying Chipa to both of these
tasks, we showed that this initial approach is a fairly strong starting point,
comparable with CL-WSD systems developed by other research groups, and that it
can be used in multiple settings fairly successfully.

In Chapter \ref{chap:monolingual}, we extended our initial approach with
strategies that make use of the available resources for Spanish, including NLP
tools for analyzing Spanish text and unsupervised learning techniques that
allow us to learn representations for our classifiers from the wealth of
available unannotated Spanish text. We showed how to extract features based on
syntactic analyses of the input text, done with a POS tagger and a dependency
parser, and how adding these features provided modest, but very consistent,
accuracy improvements. We also showed some gains, especially when using random
forest classifiers, by adding features based on Brown clusters trained on
larger samples of monolingual Spanish text. The best results in the chapter
came from adding these syntactic and Brown clustering features to the baseline
sparse feature set.

We also showed that we could use dense vector representations, built with
word2vec and doc2vec trained on available monolingual Spanish text, with
moderate success. However, these did not outperform the baseline sparse
features for our task, at least not with the data sets and machine learning
techniques used in this work; particularly, our random forest classifiers saw
accuracy drops when using these features. We learned that when using these
dense representations, it is better to focus on the text immediately
surrounding the focus word, either by limiting the context window or by giving
more weight to words close to the focus word. Trying to build a doc2vec
representation of the entire input text, or summing the word embeddings
uniformly across the text was less effective. Also, we found that it was
possible to combine the dense representations based on word embedding vectors
with the sparse features from syntactic analysis and Brown clustering, and that
this gave us performance boosts over using the dense features alone.

In Chapter \ref{chap:multilingual}, we showed how to use Chipa for classifier
stacking, which allows us to learn from bitext corpora that pair our source
language with languages other than our intended target language; this technique
is broadly applicable when we want to translate out of a language with many
available bitext resources, and could also be applied when we have a good
monolingual WSD system available. Here we trained CL-WSD classifiers on both
Bible translations, which are available for many languages and directly match
the domain of our Spanish-Guarani and Spanish-Quechua corpora, and also
Europarl bitext corpora, which are much larger, at roughly 1.8 million sentence
pairs. We showed that we are able to get some gains by adding features based on
the output of the Europarl-trained CL-WSD classifiers, but larger gains when
adding the output of the in-domain classifiers. Furthermore, the gains are
larger when adding classifier output for five different language pairs, rather
than relying on just Spanish-English CL-WSD, and these classifier stacking
features work alongside the syntactic features discussed in Chapter
\ref{chap:monolingual}; our best results came from using them in tandem.

Finally, in Chapter \ref{chap:integration}, we demonstrated prototypes for
integrating our CL-WSD classifiers into existing machine translation systems of
two completely different architectures, requiring only small changes to
existing code.  We presented a phrase-based SMT system for
Spanish-Guarani\footnote{Ignoring, for the moment, the admittedly substantial
problem of morphological generation.} that uses Chipa classifiers in a feature
function that guides its decoding process. For Spanish-Quechua, we augmented a
primarily rule-based hybrid machine translation system, using our classifiers
to supplement the existing hand-written lexical selection rules. This first
integration shows how CL-WSD techniques can reasonably be applied to
phrase-based SMT even in relatively low-data scenarios\footnote{There is prior
work on using CL-WSD for SMT; see Section \ref{sec:clwsd-smt}.}, when
translating between two unrelated languages, and the second is a
proof-of-concept for how to add a CL-WSD into an existing rule-based MT system,
allowing it to learn from any available bitext, with increasing benefits as
more bitext is collected in the future

One interesting difficulty that came up, and is particularly relevant for MT
systems that deal with morphologically rich languages, is the potential for
mismatches between the different inventories of lemmas known to the interacting
NLP systems; this is to say, the recommendations made by the CL-WSD system must
be, for the most part, lexical selections that the MT system can make.
Thankfully, for the most part the Chipa CL-WSD system and SQUOIA have a large
overlap in their inventories of Quechua citation forms, but this need not be
the case, and some care must be taken. For our integration of Chipa into the
Spanish-Guarani phrase-based SMT system, this difficulty was largely avoided,
because the lexical entries were all based on the output of the same
morphological analyzers. In both cases, we showed some modest improvements to
MT output on a small test set, using automatic metrics for Spanish-Guarani and
manual evaluation for Spanish-Quechua.

\section{Applying techniques like this to neural machine translation}
During the course of this work, the field of machine translation has undergone
a dramatic shift, in which statistical MT research, and in many cases,
practice\cite{gnmt}, has moved from phrase-based and tree-based SMT systems to
models based on recurrent neural networks.

It may not be immediately useful to add explicit CL-WSD classifiers to a neural
MT architecture, although this is an empirical question. In a sense, though, we
can say that NMT systems are already performing CL-WSD; they have a vector
representation of the whole input sentence available while making output
decisions, tuned specifically for the translation task. This vector
representation is similar to the doc2vec representations discussed in Chapter
\ref{chap:monolingual}, but uses deeper, and recurrent, networks. The
sentence-encoding process in neural machine translation thus automates much of
the feature engineering work that would go into building a CL-WSD system.

However, many of the difficulties addressed in this work still apply in the
neural machine translation setting. We still must come up with good ways to
leverage the available corpora and tools for resource-rich source languages,
when we have little available bitext. Can source-language taggers and parsers
be integrated with NMT? Will this help when translating into under-resourced
languages? How can other existing NLP tools be applied, such as monolingual
WSD?

In our CL-WSD setting, we did not find immediate benefits when using neural
embeddings based on comparatively large source-language corpora. How can
monolingual source-language resources be used for NMT, when only very small
bitext corpora are available?  There has been some recent work, including by
Johnson \emph{et al.} \cite{TACL1081}, on training multilingual neural machine
translation systems that learn to share representations among different
translation tasks; these systems can even perform ``zero shot" translation, in
which a single network trained to translate from, \emph{e.g.}, Portuguese to
English and English to Spanish can also translate reasonably well from
Portuguese to Spanish.
Additionally, there has been work, such as that of Artetxe \emph{et al.}
\cite{artetxe2018unsupervised} and Lample \emph{et al.}
\cite{lample2018unsupervised}, that creatively uses back-translation and
adversarial networks to make up for the lack of bitext for some language pairs,
but to my knowledge this has not yet been practically applied to translating
into under-resourced languages.  Broadly, there do not yet seem to be
widely-accepted approaches for leveraging existing resources to improve neural
translation into under-resourced languages, but neural machine translation is
still in its early stages and the field is developing rapidly.

Hopefully we will see effective strategies for making use of what we do have
for building NMT systems targeting under-represented and under-resourced
languages in the near future.

\section{Building resources for Guarani and other under-represented languages}
\label{sec:crowdsourcing}

While we have explored techniques in this work that allow us to lean
somewhat on the resources available for resource-rich source
languages, a clear way to improve translation for currently under-resourced
languages is to make them less under-resourced. Languages like Guarani and
Quechua have a number of speakers comparable with that of some relatively
resource-rich European languages, as well as their own engaged activist
communities. Ethnologue, for example,
reports\footnote{\url{https://www.ethnologue.com/language/grn}} that there are
about six million Guarani speakers in the world, which is more than the number
of Danish or Norwegian speakers. To support these languages and better serve
these communities, we can as technologists find better ways to work with the
speakers of these languages.

While this work has focused on languages spoken in South America, there are
under-represented languages around the world with millions or possibly many
millions of speakers, but relatively little text on the web thus far, and no
easily available MT or other NLP tools. Many languages in India and Southeast
Asia are quite large by number of speakers, but nonetheless dramatically
under-resourced. However, Internet usage is spreading quickly throughout the
world, and the speakers of these languages are coming online; we are presented
with great opportunities to build great linguistic resources and NLP tools for
these languages.

There have already been fairly successful campaigns by technologists to work
with speakers of these languages on resource-gathering projects for them. For
example, in 2016, Mozilla Paraguay managed to localize the Firefox web browser
for Guarani, with the help of Guarani-speaking volunteers, a local university,
and Guarani-language activists\footnote{See a description of the process, and
the launch announcement, at
\url{http://mozillanativo.org/2016/lanzamiento-oficial-de-firefox-en-guarani.html}
(in Spanish). Firefox in Guarani can be downloaded at
\url{https://www.mozilla.org/gn/firefox/}}. Localizing an application as
complex as a web browser is an enormous task, requiring the translation of
thousands of sentence-length messages, along with the in-browser
documentation. Also notably, this task requires choosing appropriate
Guarani-language terms for technical concepts.

As a success story directly pertinent to machine translation, in 2013, Google
ran a crowdsourcing campaign in New Zealand that managed to collect
enough training data to build a phrase-based SMT system for the Māori
language\footnote{Post on the Google New Zealand blog:
\url{https://newzealand.googleblog.com/2013/12/kua-puta-google-whakamaori-ki-te-reo.html}};
since then, Google Translate has operated a crowdsourcing platform called
Google Translate Community\footnote{https://translate.google.com/community},
on which volunteers can rate and submit translations, including for some
languages that are not currently supported by the main Google Translate
product. For languages that are already supported by Google Translate, the
submissions and ratings are used to improve MT output.

Our research group has started some efforts along these lines, but a more
sustained effort will be needed to provide benefits for language communities in
practice. We (the author, working together with Alberto Samaniego and Taylor
Skidmore, directed by Michael Gasser) prototyped two websites for collecting
Guarani and Spanish-Guarani language resources, but these have not been used
beyond the prototype stage.  The first website is called ``Tahekami", which
means \emph{let's search together} in Guarani. Tahekami\footnote{Prototype code
at \url{http://github.com/hltdi/gn-documents}} is a repository of Guarani and
bilingual documents that allows full-text search and browsing documents by tag.
Our initial version of the site contained a collection of masters theses from
the \emph{Ateneo de Lengua y Cultura Guaraní}.
We developed a second website, called ``Guampa" \footnote{A ``guampa", in
Paraguay, is the cup from which one drinks yerba mate or tereré. The term
``guampa" is also local to Paraguay; in other parts of South America, the
container itself is called a ``mate". The Guampa software is available at
\url{https://github.com/hltdi/guampa}}, which is a bilingual wiki, designed
to be used by Guarani speakers and learners to collaboratively translate
documents from Spanish to Guarani or vice-versa. We presented an initial version
of this software at LREC 2014\cite{RUDNICK14.151}. This site is meant to serve
at least three purposes: helping Guarani-language learners practice and get
feedback on their translations, creating more Guarani-language documents for
the web (we started initially with translating Spanish Wikipedia), and building
bitext corpora for training MT systems.

Michael Gasser is currently, as of 2018, developing successors to these
websites, in the form of \emph{mitmita} and \emph{mainumby}, online
computer-aided translation systems for Amharic and Guarani
respectively\footnote{Code for these sites is being developed at
\url{https://github.com/hltdi/mitmita} and
\url{https://github.com/hltdi/mainumby}}. The goal of these tools is to help
users translate documents by providing both searches over translation memories
and automatic suggestions from an integrated machine translation system.

In any case, while there remains much work to be done --- both technical and
social --- for supporting the under-represented languages of the world, it is
outside the scope of this dissertation, and will be addressed in the future,
hopefully in part by the author.

\chapter{Learning from Multilingual Data}
\label{chap:multilingual}

\section{Evidence from multilingual corpora}
As discussed in the previous chapter, while our target languages are
under-resourced, we have many useful resources for our source languages. One of
them is a relative abundance of bitext, from which we can learn translations
between resource-rich languages. When this works well, this can give a better
sense about the meanings of words in a context in the resource-rich source
language; translation from \emph{e.g.} Spanish to English can be thought of as
word-sense disambiguation, which can be used to add annotations for use in
translating into the under-resourced target languages with which we're
currently concerned. In this chapter, we will investigate making use of some of
the available multilingual corpora resources.

We would like our CL-WSD system to be able to learn relationships between the
senses of a given source word that are lexicalized in different ways in various
languages.
Two target languages may happen to surface similar sense distinctions, perhaps
due to being related languages, or simply by coincidence. Alternatively, a
combination of translations into several languages may provide evidence for a
certain lexical choice in the target language of interest.

For some languages, such as those of the European Union, we have multiple
bitext corpora available, covering several different language pairs with the
same source language. Through the Europarl corpus \cite{europarl}, for example,
we have bitext corpora in which Spanish is paired with English, German, French,
and 17 other European languages.
We would like to be able to make use of evidence from all of these corpora when
translating into any particular target language, ideally even when the corpora
involved are not all mutually parallel.
Each corpus may contain useful examples of a given source language word,
and senses of that word may be lexicalized in varying, non-overlapping ways in
the different target languages.

The approaches in this chapter are particularly informed by the work of Els
Lefever \emph{et. al} (see especially
\cite{lefever-hoste-decock:2011:ACL-HLT2011}), in which the entire source
sentence is machine-translated into several different target languages just
before inference time. One drawback of this technique is that it requires
multiple complete MT systems to perform CL-WSD, which seems unwieldy when we
want to use CL-WSD as a subcomponent of a machine translation system in the
first place.

So considering the work of Lefever \emph{et. al}, we developed some prototype
CL-WSD systems that made use of multilingual evidence
\cite{rudnick-liu-gasser:2013:SemEval-2013} and produced some of the best
results in a SemEval shared task on CL-WSD \cite{task10}.
Here our systems that took into account evidence from multiple sources had
better performance on the task than the one that used monolingual features; our
top results in every language came from either classifier stacking or the MRF
classifier. This suggests that it is possible to make use of the evidence in
several parallel corpora in a CL-WSD task without translating every word in a
source sentence into many target languages.

In our SemEval paper, we presented two different approaches for
making use of multilingual evidence. The more complex of them was a system
based on graphical models that performed loopy belief propagation over Markov
networks, which we will discuss briefly here, in the next section.
The simpler approach described in that work was a relatively straightforward
classifier stacking approach, where the outputs of several CL-WSD classifiers
were used as input features for one final classifier. This approach is more
immediately applicable to lexical selection when our target language is
under-resourced, as it does not require bitext corpora between all of the
involved languages.

\section{CL-WSD with Markov Networks}
In our SemEval entry, we investigated an approach based on Markov networks
(also known as ``Markov Random Fields"), building a network of interacting
variables (see Figure \ref{fig:pentagram}) to solve the CL-WSD classification
problem for the five target languages of the SemEval 2013 task \cite{task10}.
Given English source sentences with a specific annotated focus word (from a
given set of possible focus words types), the task was to make a correct
lexical selection for translating into Dutch, French, German, Italian and
Spanish, matching the selections made by human annotators familiar with those
languages.

In that work, our Markov network has nodes that correspond to the distributions
produced by language-specific maximum entropy classifiers, given an input
sentence, and edges with pairwise potentials that are derived from the joint
probabilities of target language labels occurring together in the available
bitext.
We frame the task of finding the optimal translations into five languages
jointly as a MAP inference problem, wherein we try to maximize the joint
probability of all five variables, given the evidence of the features extracted
from the source language sentence.
We perform inference with loopy belief propagation
\cite{DBLP:conf/uai/MurphyWJ99}, which is an approximate but tractable
inference algorithm that, while giving no guarantees, often produces good
solutions in practice.
We used the formulation for pairwise Markov networks that passes messages
directly between the nodes rather than first constructing a ``cluster graph",
which is described in \cite[\S 11.3.5.1]{Koller+Friedman:09} of Koller and
Friedman's book on graphical models.

Intuitively, at each time step loopy belief propagation passes messages around
the graph that inform each neighbor about the estimate, from the perspective of
the sender and what it has heard from its other neighbors, of the minimum
penalty that would be incurred if the recipient node were to take a given
label. As a concrete example, when the \emph{nl} node sends a message to the
\emph{fr} node at time step 10, this message is a table mapping from all
possible French translations of the current target word to their associated
penalty values. The message depends on three things: the probability
distribution from a monolingual classifier just for Dutch, joint probabilities
estimated from our Dutch-French bitext, and the messages from the \emph{es},
\emph{it} and \emph{de} nodes from time step 9.

\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \caption{The network structure used in the MRF system: a complete graph with
  five nodes, in which each node represents the random variable for the
  translation into a target language.}
  \label{fig:pentagram}
\end{figure}

While we were able to achieve fairly good results with these MRF-based
classifiers on the Semeval CL-WSD task, our strategy for setting the weights in
the Markov network requires, for each edge in the graph, a parallel corpus for
the corresponding language pair.
%% XXX working here
It thus seems less easily extensible than the classifier stacking approach, in
which it is clear how we can include information from many heterogeneous
sources. The benefit of the Markov network approach is that it takes seriously
the uncertainty present in the predictions of each of the component
classifiers, solving the entire problem jointly.

The Markov network approach may be useful for future CL-WSD systems in other
settings, but we will not make further use of it for now, for the setting of
translating into under-resourced languages, since we do not have many parallel
corpora available for Guarani or Quechua.

\section{Classifier stacking}
The classifier stacking approach, in order to predict the translation of a word
into a given target language, uses translations into other available target
languages as features.
For example, for the SemEval prototype systems, in order to translate an
English word into Spanish, we would give the classifier features encoding that
word's translations into French, Italian, Dutch and German.
At training time during SemEval, we were provided with parallel corpora for six
languages, we found these translations in the given parallel corpora, but
for test sentences, the translations were not available, they were predicted
with the simpler non-multilingual classifiers.
%% XXX: we're not doing this going forward, right? just run the classifiers.

The classifier stacking approach could likely be improved by adding features
derived from more sources.
We would like to try adding classifiers trained on the other Europarl
languages, as well as completely different corpora; this would require that for
the stacked classifier, we would train on predicted translations rather than
This approach only requires that the monolingual classifiers make \emph{some}
prediction based on text in the source language;
they need not be trained from the same source text, depend on the same
features, or even output words as labels.

\section{Experiments}
\label{sec:multilingual-experiments}
... same as before, right?

\section{Experimental Results}
\label{sec:multilingual-results}

\section{Discussion}


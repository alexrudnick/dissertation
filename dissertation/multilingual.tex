\chapter{Learning from Multilingual Data}
\label{chap:multilingual}

\section{Evidence from multilingual corpora}
As discussed in previous chapters, while our target languages are
under-resourced, we have many resources available for our source languages.
For Spanish and many other languages, in addition to the large amounts of
monolingual text and off-the-shelf NLP tools discussed in Chapter
\ref{chap:monolingual}, we have a relative abundance of bitext, pairing Spanish
with other resource-rich languages.
We would like to be able to learn from these bitext corpora when translating
from resource-rich source languages into under-resourced target languages.
Each bitext corpus may contain useful examples of a given source language word,
and senses of that word may be lexicalized in varying ways in different target
languages.

Selecting a contextually correct translation for source-language words is
evidence that we are able to infer the meaning of those words at least
implicitly, in as far as sense distinctions are surfaced in the target
language.
As we have mentioned previously, translation,
\emph{e.g.} from Spanish to English can be thought of as word-sense
disambiguation.
%% XXX working here

We would like our system to be able to learn relationships between the
senses of a given source word that are lexicalized in different ways in
different languages.
Two target languages may happen to surface similar sense distinctions, perhaps
due to being related languages, or simply by coincidence. Alternatively, a
combination of translations into several languages may provide evidence for a
certain lexical choice in the target language of interest.

These translations into \emph{other} target languages can then be used as
annotations when translating into the under-resourced target languages that
concern us in the current work. In this chapter, we will investigate making use
of some of the available multilingual corpora resources in exactly this way.

\section{Multilingual evidence from the Europarl corpus}

For some languages, such as those of the European Union, we have multiple
bitext corpora available, covering several different language pairs with the
same source language. Through the Europarl corpus \cite{europarl}, for example,
we have bitext corpora in which Spanish is paired with English, German, French,
and 17 other European languages.

%% especially when the available corpora are not all mutually parallel.

The approaches in this chapter are particularly informed by the work of Els
Lefever \emph{et. al} (see especially
\cite{lefever-hoste-decock:2011:ACL-HLT2011}), in which the entire source
sentence is machine-translated into several different target languages just
before inference time. One drawback of this technique is that it requires
multiple complete MT systems to perform CL-WSD, which seems unwieldy when we
want to use CL-WSD as a subcomponent of a machine translation system in the
first place.

In earlier work, considering the work of Lefever \emph{et. al}, we developed
some prototype CL-WSD systems that made use of multilingual evidence
\cite{rudnick-liu-gasser:2013:SemEval-2013} and produced some of the top
results in a SemEval shared task on CL-WSD \cite{task10}.
Here our systems that took into account evidence from multiple sources had
better performance on the task than the one that used monolingual features; our
top results in every language came from either classifier stacking or the
Markov network classifier. This suggests that it is possible to make use
of the evidence in several parallel corpora in a CL-WSD task without
translating every word in a source sentence into many target languages.

In this SemEval paper, we presented two different approaches for making use of
multilingual evidence. The more complex of them was a system based on graphical
models that performed loopy belief propagation over Markov networks, which we
will discuss briefly in the next section. The simpler approach described in
that work was a relatively straightforward classifier stacking approach, where
the outputs of several CL-WSD classifiers were used as input features for one
final classifier. This approach is more immediately applicable to lexical
selection when our target language is under-resourced, as it does not require
bitext corpora between all of the involved languages.

\section{CL-WSD with Markov Networks}
In our SemEval entry, we investigated an approach based on Markov networks
(also known as ``Markov Random Fields"), building a network of interacting
variables (see Figure \ref{fig:pentagram}) to solve the CL-WSD classification
problem for the five target languages of the SemEval 2013 task \cite{task10}.
Given English source sentences with a specific annotated focus word (from a
given set of possible focus words types), the task was to make a correct
lexical selection for translating into Dutch, French, German, Italian and
Spanish, matching the selections made by human annotators familiar with those
languages.

In that work, our Markov network has nodes that correspond to the distributions
produced by language-specific maximum entropy classifiers, given an input
sentence, and edges with pairwise potentials that are derived from the joint
probabilities of target language labels occurring together in the available
bitext.
We frame the task of finding the optimal translations into five languages
jointly as a MAP inference problem, wherein we try to maximize the joint
probability of all five variables, given the evidence of the features extracted
from the source language sentence.
We perform inference with loopy belief propagation
\cite{DBLP:conf/uai/MurphyWJ99}, which is an approximate but tractable
inference algorithm that, while giving no guarantees, often produces good
solutions in practice.
We used the formulation for pairwise Markov networks that passes messages
directly between the nodes rather than first constructing a ``cluster graph",
which is described in \cite[\S 11.3.5.1]{Koller+Friedman:09} of Koller and
Friedman's book on graphical models.

Intuitively, at each time step loopy belief propagation passes messages around
the graph that inform each neighbor about the estimate, from the perspective of
the sender and what it has heard from its other neighbors, of the minimum
penalty that would be incurred if the recipient node were to take a given
label. As a concrete example, when the \emph{nl} node sends a message to the
\emph{fr} node at time step 10, this message is a table mapping from all
possible French translations of the current target word to their associated
penalty values. The message depends on three things: the probability
distribution from a monolingual classifier just for Dutch, joint probabilities
estimated from our Dutch-French bitext, and the messages from the \emph{es},
\emph{it} and \emph{de} nodes from time step 9.

\begin{figure}
  \begin{center}
  \includegraphics[width=5cm]{pentagram.pdf}
  \end{center}
  \caption{The network structure used in the MRF system: a complete graph with
  five nodes, in which each node represents the random variable for the
  translation into a target language.}
  \label{fig:pentagram}
\end{figure}

While we were able to achieve fairly good results with these MRF-based
classifiers on the Semeval CL-WSD task, our strategy for setting the weights in
the Markov network requires, for each edge in the graph, a parallel corpus for
the corresponding language pair.
%% XXX working here
It thus seems less easily extensible than the classifier stacking approach, in
which it is clear how we can include information from many heterogeneous
sources. The benefit of the Markov network approach is that it takes seriously
the uncertainty present in the predictions of each of the component
classifiers, solving the entire problem jointly.

The Markov network approach may be useful for future CL-WSD systems in other
settings, but we will not make further use of it for now, for the setting of
translating into under-resourced languages, since we do not have many parallel
%% XXX add accent
corpora available for Guarani or Quechua.

\section{Classifier stacking}
The classifier stacking approach, in order to predict the translation of a word
into a given target language, uses translations into other available target
languages as features.
For example, for our SemEval prototype systems, in order to translate an
English word into Spanish, we predict that word's translations into French,
Italian, Dutch and German, and then encode those predicted translations as
features for our English to Spanish classifier.

%% XXX this needs some editing

We would like to try adding classifiers trained on the other Europarl
languages, as well as completely different corpora; this would require that for
the stacked classifier, we would train on predicted translations rather than...

This approach only requires that the monolingual classifiers make \emph{some}
prediction based on text in the source language; they need not be trained from
the same source text, depend on the same features, or even output words as
labels.

\section{Experiments}
\label{sec:multilingual-experiments}
... same as before, right?

\section{Experimental Results}
\label{sec:multilingual-results}

\section{Discussion}


\chapter{Data Sets, Tasks, Software and Evaluation}
\label{chap:evaluation}
In order to evaluate our CL-WSD approaches and their effects on translation
quality, we will need to have a basis for comparison between our proposed
techniques and sensible baselines, including results reported by other
researchers.

In this chapter, I describe the tasks and data sets that we will investigate
for the rest of the dissertation, as well as the basic version of the Chipa
software, which we extend in a variety of directions in the following chapters.

%% There are three different kinds of evaluation that one *could* do here.
%% There's the typical kind of monolingual evaluation, with predefined sense
%% inventories. There's the semeval-style classification accuracy, and then
%% there's MT evaluation, in-vivo on a real task that we care about.
%% ""consider fine-grained vs coarse-grained evaluation
%% coarse/fine was manually chosen ""
As discussed in Section \ref{sec:background-wsd}, for many years word sense
disambiguation systems have been evaluated \emph{in vitro} and in a monolingual
setting. The test sets for such WSD evaluations include sentences
hand-annotated with a word senses from a particular sense inventory, such as
(typically) the WordNet senses. These monolingual WSD evaluations often include
both coarse-grained and fine-grained distinctions and allow for several possible
correct answers; some word senses identified by lexicographers are more closely
related than others.  While the senses in a sense inventory will contain many
useful and interesting distinctions, they do not necessarily make the
distinctions most appropriate for a given task, and the necessary distinctions
will vary by application. As such, accuracy improvements on these tasks do not
necessarily lead to performance gains for an NLP system making use of
word-sense disambiguations \cite{resnikwsdapplications}.

This style of evaluation relies on a fairly scarce resource: sentences
hand-annotated with a particular sense inventory.
In the CL-WSD setting, where we consider target-language lexical items to be
the salient senses of source-language words, we could also ask annotators to
hand-label each word of our test sentences with their translations.
For many language pairs, however, this annotation task has nearly been
performed by translators. We do not strictly have the labels for each token in
the source document, but with automatic word alignment techniques, we can infer
these labels with high confidence.

%% XXX: working here.

We want to make the argument that higher classification accuracy on CL-WSD
tasks leads to improved translation results. It seems intuitive that a
classifier producing correct word choices would lead an MT system to produce
better results. But we do not have a system that always indicates correct word
choices, nor are we likely to have one in the near future. What we need to
decide empirically is whether the suggestions made by the CL-WSD system,
imperfect as they are sure to be, will improve translation results. We could
imagine these gains failing to materialize, for example, if the CL-WSD system's
correct choices are mostly those that the MT system would have made on its own
-- say, with the guidance of a language model and phrase-table probabilities --
and its incorrect choices are misleading to the point of doing harm to our
translation quality.

%% TODO: Drop in examples of all the different Bible formats as figures.

%% NB: it's OK for this chapter to get pretty long. 40 pages isn't too long.

%% TODO Write a bit more about how we'll use cdec later on.

\section{Measuring CL-WSD Classification Accuracy}
One straightforward \emph{in vitro} approach for evaluating a CL-WSD system is
to measure classification accuracy over pre-labeled data. Here by
classification accuracy, we mean measuring how often the system is able to
correctly choose the appropriate translation of the focus word in a given
context.
%% XXX: we should adopt consistent terminology for the focus word. ``target
% word" is probably confusing in a translation context.
%% TODO: expand a bit about what we mean by classification accuracy.
Strictly speaking, we
do not have pre-labeled data: our bitext does not come with sub-sentential
alignments. But automatic word alignments provide a good approximation, as long
as our sentence alignments (or verse alignments, for Bible text) are accurate.
For the purposes of this work, we will assume that the automatic word-level
alignments are correct, putting in our best effort preprocessing the available
text so that the aligner can produce useful output.

Using our automatically-aligned bitext as labeled data allows us to closely
mirror the lexical selection task faced by an MT system while translating
running text. We can train our classifiers for many different word types, but
we will generally not have training examples available for all words in the
input at test time. This is a problem faced by data-driven NLP systems broadly.

For comparison with other work, we can also run our systems on the SemEval
shared task test sets, which are publicly available. Our task here is framed in
the same way as the CL-WSD shared tasks from 2010 and 2013, so measuring
performance on these test sets allows a straightforward comparison
between the variations explored in this work and several other CL-WSD systems
from recent years. These test sets are limited in scope, however, and will
only demonstrate Chipa's performance for translating individual English nouns.
It is also worth noting that our more immediate practical goal is to aid
translation from Spanish into lower-resourced languages, and the resources that
we can use to analyze an input English sentence will be different from those
for Spanish.

\TODO{this means we have to actually run the latest Chipa on the SemEval test sets too}
%% """if you end up using these data set for evaluation, I'd like to see
%information here about how the data were annotated, where the translation
%candidates came form, how big the data sets were ""

\section{Measuring MT Improvements}
We will additionally want to conduct \emph{in vivo} evaluations of our CL-WSD
techniques as applied to running MT systems. Here we can use standard
approaches for evaluating MT, such as BLEU and METEOR scores, or simply show
that where word choices differ from the application of CL-WSD, the changes are,
for the most part, improvements.
%% TODO "how do you know, the changes don't make the translation worse?"
These experiments will be carried out in Chapter \ref{chap:integration}, in
which we integrate the Chipa software with running MT systems.

For these experiments, we sample sentences from the available bitext --
particularly ones that contain polysemous words for which we can train
classifiers! -- and run the MT systems both with and without Chipa enabled.
These experiments are described in more detail in
Chapter~\ref{chap:integration}, where I also discuss how to integrate Chipa
into the different machine translation packages.

\section{Data Sets and Preprocessing}
%% TODO "If you use europarl, it should be mentioned throughout the chapter,
%% under data, here ..."

The largest corpus that we have available for all of our source and target
languages is the Bible. There are translations available for many different
languages, and while these were not always produced by native speakers, the
translators are often driven by missionary zeal to produce high-quality
translations~\cite{DBLP:journals/lre/ResnikOD99}, so we can be reasonably
confident of the (linguistic) accuracy of our text.

In this work, we focus on one Bible translation per language. For Spanish, we
use the Reina-Valera translation (1995 edition), which I was able to scrape
from a Bible study website. Our Guarani translation, \emph{Ñandejara Ñe'e}
(1997 edition) was scraped from the same site. Our Quechua version (the 2004
translation published by the Peruvian Bible Society) was provided by Chris Loza
and Rada Mihalcea; the preparation of the Quechua corpus is described in Loza's
masters thesis \cite{chrisloza}. For English, we use the public domain World
English Bible translation.
While there are a large number of translations available online, different
``Bible societies" own the associated copyrights and thus redistribution of the
complete texts is often restricted \cite{MAYER14.220.L14-1215}.

\subsection{Source-side annotations}
\label{sec:annotations}
In order to gracefully include a variety of features and build on
any available analysis tools for the source language, Chipa's input format and
feature functions allow for arbitrary annotations on source-language tokens.

Chipa's input file format describes one token per line, with fields split by
tabs.  The first field of a line is a token's lemma, and the second field is
its surface form.  Following this are an arbitrary number of annotations for
that token, which may encode things such as part-of-speech, dependency
relations, Sentence boundaries are indicated by blank lines. An annotated
sentence with one annotation per token might look like Figure
\ref{fig:quickbrownfox}, for example:

\begin{figure*}
\raggedright \texttt{\\
the	The	pos=DET \\
quick	quick	pos=ADJ \\
brown	brown	pos=ADJ \\
fox	fox	pos=NOUN \\
jump	jumped	pos=VERB \\
over	over	POS=ADP \\
the	the	POS=DET \\
lazy	lazy	POS=ADJ \\
sleep sleeping	POS=VERB \\
dog	dog	POS=NOUN \\
.	.	POS=. \\
  }
  \caption{Example annotated sentence.}
  \label{fig:quickbrownfox}
\end{figure*}

The Chipa software has been developed with a number of tools that consume and
produce this format, typically taking in sentences annotated in this way,
calling another NLP tool to generate more annotations, and then adding the new
annotations to those already present. The experiments in this chapter only use
one of these tools, \texttt{freeling\_to\_annotated.py}, which produces Chipa's
input format given output from Freeling.  In the following chapters, we will
make use of these tools and the extra information they make available to our
classifiers; in Chapter \ref{chap:monolingual} we add annotations learnable with
monolingual resources, and in Chapter \ref{chap:multilingual} we describe tools
and annotations that require parallel data.

\subsection{Preprocessing}

There is a nontrivial amount of preprocessing required in order to get our
Bible text into a format suitable for our tools.  This is largely due to the
varying markup formats used by different translation groups.
For each of the formats and translations, we need to understand its formatting
enough to know which text comes from which book, chapter, and verse number.
This triple uniquely identifies a unit of text, and verse numbers are nearly
standardized across translations.  There are a few exceptions, however:
translations produced by different religious traditions may include different
books. Most notably, Catholic editions have a superset of the books found in
modern Protestant editions, and include slightly different chapters.
Additionally, in some translations, such as our Guarani edition, a few verses
have been combined into a single segment for a more natural translation.

In any case, if a particular book/chapter/verse is present in two Bibles, then
the two verses are very likely translations of one another. Once we find all of
the matching verses, we can build a bitext corpus suitable for use in our
pipeline.  In all four of our translations, we find all 66 books of the
Protestant Bible. Furthermore, we find matches for nearly all of the verses
across all of the language pairs considered. For English/Spanish and
Spanish/Quechua, the intersection of book/chapter/verse tuples across Bibles
contains over 99\% of the verses in any given Bible. For the Guarani
translation, however, we are only able to match 95.2\% of the verses. The
Guarani text contains fewer verses, $29,867$ versus the $31,104$ in Spanish,
showing that the translators were more willing here to combine verses and that
the translations may be more interpretive.

\begin{figure*}
\raggedright \begin{verbatim}
\id LAM 25-LAM-web.sfm World English Bible (WEB) 
\ide UTF-8
\h Lamentations 
\toc1 The Lamentations of Jeremiah 
\toc2 Lamentations 
\toc3 Lam 
\mt2 The 
\mt Lamentations 
\mt2 of Jeremiah 
\c 1  
\p
\v 1 How the city sits solitary, 
\q2 that was full of people! 
\q She has become as a widow, 
\q2 who was great among the nations! 
\q She who was a princess among the provinces 
\q2 has become a slave! 
\b
\q
\v 2 She weeps bitterly in the night. 
\q2 Her tears are on her cheeks. 
\q Among all her lovers 
\q2 she has no one to comfort her. 
\q All her friends have dealt treacherously with her. 
\q2 They have become her enemies. 
\end{verbatim}
  \caption{The first two verses of the Book of Lamentations (World English
  Bible translation) in USFM format.}
  \label{fig:usfmsample}
\end{figure*}


Our English translation is distributed in a format called USFM (Unified
Standard Format Markers), which is a markup language developed by a working
group of Bible translators and used widely by the Bible-related software
community.
\footnote{See \url{http://paratext.org/about/usfm} for more about USFM. USFM is
widely deployed; the website from which we scraped the Spanish and Guarani
corpora appears to render its HTML from a USFM source.  There is an entire
community of Bible software developers, and it has a wing that advocates Open
Source software and translations unencumbered by copyright.  One could delve
arbitrarily deeply into the history of religious-text translators and their
relationships with technology and copyright, but one has an NLP dissertation to
write.}
While there are a number of tools that consume this format, I did not find one
that handles our particular use case of stripping metadata and simply mapping
from verses to flat text, so I wrote a short script to parse USFM and produce
text in an easily-alignable format.

%% TODO "Does that mean, the extraction could potentially have introduced
%inconsistencies? If so, which ones?"
% (regarding the HTML extraction for the Spanish/Guarani editions)

Our Spanish and Guarani corpora are extracted from HTML pages scraped from an
online Bible study website. In practice, the scraping was done by predicting
the URLs for each chapter of each book and requesting those URLs
programmatically. We then extract the text from the saved HTML pages with a
Python script and the BeautifulSoup library for HTML parsing.
\footnote{\url{http://www.crummy.com/software/BeautifulSoup/}}
As a side note, since websites change over time and we
downloaded the Guarani translation at an earlier date (from a mobile version of
the site that is no longer available), the HTML takes a significantly different
structure in the Spanish and Guarani editions, so they require different
versions of the script for preprocessing.

Our Quechua translation came in an easily parseable format, with individual
verses already identified by Loza and Mihalcea; for this corpus, relatively
little preprocessing was necessary since lines begin with the book, chapter,
and verse already labeled. See Figure \ref{fig:loza} for a sample of the input
format.

\begin{figure*}
\raggedright \begin{verbatim}
[999,31,1,1] ¡Ay, imaraqmi Jerusalenqa sapan kapushan, haqay hina
  askha runakunayoq llaqta! ¡Ay, imaraqmi viuda warmi hina kapushan,
  lliw suyukunaq uman kashaqqa! Mit'anipina llank'apushan, lliw
  llaqtakunaq ñust'an kashaqqa.
[999,31,1,2] Tutan-tutanmi unuy-parata waqayku- shan, uyapas
  p'aspay-p'aspallaña. Llapan munaqninkunamanta manan hukllapas kanchu
  sonqochay- kuqnin. Llapan reqsinakuq-masinkunapas manan rikuqpas
  tukupunkuchu, llapallanmi awqanman tukupunku.
\end{verbatim}
  \caption{The first two verses of the Book of Lamentations in Quechua, from
  the 2004 Peruvian Bible Society translation. Whitespace changes added here
  for readability.}
  \label{fig:loza}
\end{figure*}

Each of the different formatting schemes uses its own coding to identify the
different books of the Bible, so when producing output for alignment, we must
map to a standard encoding. For example, our Quechua edition marks the book of
Lamentations with the numerical code $31$ (it is the thirty-first book in
modern Catholic editions), but in the USFM markup for the English translation,
we find a header with the string ``Lamentations". For our Spanish and Quechua
editions from the web, we find the code ``LAM" in the markup. In any case, we
map each of these identifiers to a single code ``Lam", so that we can match
books, chapters and verses across translations.

\subsection{Morphological Analysis}
\label{sec:guaranima}

For each of our languages, whether on the source or target side, in addition to
tokenizing, lowercasing and verse-alignment, we extract the lemmas (the
citation forms, normalized with respect to inflection) from each surface word.
Especially when working with smaller data sets and morphologically rich
languages, this is an important step; working primarily with lemmas rather than
surface-form words allows us to group, for example, the plural form of a word
with its singular.

For English and Spanish, we use the FreeLing suite of NLP tools
\cite{padro12} to extract lemmas. FreeLing provides a variety of analyses,
including tokenization, sentence splitting, multi-word expression and named
entity detection, part-of-speech tagging and parsing.  We start out using just
a few of these tools -- initially only tokenization and lemmatization. We
discuss making use of more of them in Chapter \ref{chap:monolingual},
leveraging the quality NLP tools that we have for our resource-rich source
languages.

The use of FreeLing for English is a noted improvement over initial
experiments, which used the WordNet lemmatizer and the default NLTK
part-of-speech tagger.
The WordNet lemmatizer especially has a fairly small vocabulary, did not
provide a good strategy for dealing with unknown words, and could only address
nouns, verbs, and adjectives.
There are a very large number of named entities in the source text, most of
which are not present in WordNet. It cannot, by itself, help us know that
``Zebulonites" is the plural of "Zebulonite".
%% TODO "hm, this is again more anecdotal. Can you give any numbers for
%coverage and accuracy? E.g. take a few verses and check manually how many
%words were recognized and if so correctly analyzed What happens if a word is
%not recognized?"

For Guarani and Quechua, we use Michael Gasser's ParaMorfo and AntiMorfo
analyzers.\footnote{Prof. Gasser's morphology software for Guarani, Quechua,
Spanish, and other languages is available at
\url{http://www.cs.indiana.edu/~gasser/Research/software.html}} Guarani text is
analyzed with ParaMorfo 1.1, and Quechua with AntiMorfo 1.2.  These packages
are based on software originally developed for Ethiopian Semitic languages
\cite{gasser:eacl09}, and use cascades of finite-state transducers (FSTs),
which is a common approach in morphological analysis \cite{beesley+karttunen}.
ParaMorfo and AntiMorfo use weighted FSTs to handle long-distance dependencies
between morphemes, but rather than having weights correspond to probabilities
to be combined with multiplication, the FSTs are weighted with feature
structures that are combined via unification. This approach was originally
introduced by Amtrup \cite{amtrup:03}. In an FST weighted with feature
structures, the result of a successful traversal is the unification of the
feature structure ``weights'' on the traversed arcs, as well as an output
string. All possible paths through the FST are searched, though many of these
paths will not lead to accepting states, or will fail due to unification
conflicts.  Because a feature structure is accumulated during the process of
transduction, each traversal through the transducer retains a memory of where
it has been, permitting the incorporation of long-distance constraints such as
those relating the negative prefix and suffix of Guarani verbs.

Here the result of morphologically analyzing a word is a root and a feature
structure representing the grammatical features of the word.  For the basic
version of Chipa, we are only concerned with the lemmatized forms of the words
in our corpora, so we ignore the grammatical features for now.  In cases of
morphological ambiguity, which is to say cases where a surface form could be an
inflection of multiple distinct lemmas, we maintain this ambiguity rather than
trying to disambiguate from context.  The different possible lemmas are
separated by a slash; there are examples of this in
Section~\ref{sec:exploring}.

%% TODO "Did you run / are you planning to run another experiment where you
%force disambiguation? Either use the highest weights, or the first analysis
%..."
%% Force morphological disambiguation on the gn side, not a bad idea.

%% TODO "why not give a few here?"
%% Examples of morphological ambiguity in gn/qu.

A familiar example of morphological ambiguity in Spanish is that \emph{ir} 'to
go' and \emph{ser} 'to be' share their preterite-tense forms; this is a fairly
common phenomenon. However, the FreeLing analysis tools can typically
(heuristically) resolve these ambiguities for us. When analyzing Guarani and
Quechua, however, the ambiguities are maintained.

\subsection{Alignment}
As a precaution against verses that are not close translations, or perhaps have
combined several parts of the text together into one verse on only one side, we
run the cdec filtering tool (\texttt{filter-length.pl}) with its default
settings, which checks for unusual length disparities in the bitext. This ends
up removing roughly 7\% of the verses\footnote{Specifically, length filtering
removes $7.12\%$ of the verse pairs for Spanish-English and thus
English-Spanish, $6.69\%$ for Spanish-Guarani, and $6.72\%$ for
Spanish-Quechua.}, but manual inspection shows that those verses removed are in
general very loose translations, or that additional material is present on one
side.

Having tokenized, lowercased, verse-aligned and lemmatized, our corpora, we are
ready to extract word-level alignments.
For this we use the \texttt{fast\_align} tool from cdec
\cite{dyer-EtAl:2010:Demos}, with its default settings\footnote{With the
command given at \url{http://www.cdec-decoder.org/guide/fast_align.html}} and
the lemmatized text as input.
\texttt{fast\_align} runs several iterations of a variant of IBM Models 1 and
2, with a prior encouraging diagonal alignments
\cite{dyer-EtAl:2011:ACL-HLT2011}. While running, it tunes the parameters on
the priors, and having learned a translation model, outputs the single most
likely one-to-many word alignments, such that each source language token is
linked with zero or more target-language tokens in the corresponding verse.

%% TODO "explain? example?"
%% XXX: working here

\TODO{add a clear example alignment here, maybe a word aligned to a multi-word
expression}


%% TODO "This whole section sounds like a text I would use for teaching. You
%explain all the fundamental problems on a very general level. What I would
%expect form a thesis is that you tell the reader what exactly you did, or at
%least what the major steps are. Examples would help, too. I mean, I don't need
%to know the exact regular expressions how you identified corresponding verses,
%but I would like to know what kind of segmentation you did, how you dealt with
%run-on verses in Guarani, if you split punctuation from words, cases of
%discrepancies and how you dealt with them ..  "

\subsection{A deeper dive: producing Spanish-Guarani bitext}
To get a concrete idea about the process of producing our bitext and our
annotated source corpus, we will now step through the bitext production process
for our Spanish-Guarani language pair. For the precise process, please see the
source code
\footnote{\url{https://github.com/alexrudnick/terere/tree/master/bibletools}}.

We consider the ``raw materials" for this process to be the scraped Spanish and
Guarani bibles; see Figure \ref{fig:es-html-sample} for an example.  In order
to replicate this work, or to extend it to more languages, one will have to
acquire bitext covering one's preferred language. There are translations in
many different languages available on various Bible websites, however.

The result of the scraping process is a directory full of HTML files, the raw
HTML that was served by the Bible websites. For the sites that we scraped, each
chapter is served as an individual HTML file. From these HTML files, we must
extract the flat text of all of the available Bible verses.  We do this with
the \texttt{get\_verses.py}, a tool that uses BeautifulSoup to parse the input
HTML and navigate its structure, extracting the verse identifiers and verse
text as it goes. We call the output of \texttt{get\_verses.py} a ``Bible file";
it consists of tab-separated values, one per line, in which (book, chapter,
verse) tuples are paired with the text of those verses.

We then call the \texttt{print\_bitext.py} on pairs of Bible files. This script
finds matching verses (book, chapter, verse tuples) and produces bitext files
in the format expected by the cdec alignment tools, which is simply
source-language segments and target-language segments, separated by
\texttt{|||}. The output of \texttt{print\_bitext.py} is called
\texttt{bible.es-gn.unfiltered}, which contains some non-parallel "sentences"
and is untokenized. Individual sentences have not been preprocessed at all.

To achieve well-aligned verses, we then run cdec's length filtering script,
\texttt{filter-length.pl}. It filters out segments where either side is too
long, or where there is a very large mismatch in length ratios between source
length and target length (which indicates poor alignments between verses). Now
we have selected the bitext from which we will train Chipa.

%% TODO: add an example of bad alignments here.

Given the sentence-aligned bitext corpus, which contains only the subset of the
source and target Bibles that we were able to verse-align (roughly, ``sentence
align", in more common MT terms, though a verse need not be a single sentence),
we now want to preprocess the source text to produce the annotated version used
for training. We call cdec's \texttt{cut-corpus.pl} to select just the source
side of the bitext, in this case the Spanish. Then we call Freeling to do basic
analysis on the input Spanish text: it provides tokenizing (for example,
splitting out punctuation adjacent to words), lemmatizing and tagging
\footnote{See the checked-in Freeling configuration file for the exact settings
used:
\url{https://github.com/alexrudnick/terere/blob/master/bibletools/freeling-config/es.cfg}}.
Then we run a Python script, \texttt{freeling\_to\_annotated.py}, to convert
the FreeLing output format into Chipa's source-language input format.

Now on the target language side, we do something analogous, although less
analysis of the target text is required, and Freeling does not support our
target languages. We again use \texttt{cut-corpus.pl} to select just the target
side, in this case the Guarani, and cdec's \texttt{tokenize-anything.sh} and
\texttt{lowercase.pl} to tokenize and lowercase the Guarani text. Then we run
Chipa's lemmatizer script over the Guarani text. It calls the Paramorfo
morphological analyzer on the Guarani tokens, producing Guarani lemmas. In
cases of morphological ambiguity, we produce tokens that maintain that
ambiguity, simply joining the possible lemmas with a slash.  Once lemmatization
is complete, we have produced the target-language lemmatized text.

Then we join together each line of the source-language lemmatized text with its
corresponding target-language lemmatized text, using cdec's
\texttt{paste-files.pl} script. The output of this is the sentence-aligned,
tokenized, lowercased, lemmatized bitext corpus, which is ready for word
alignment. To generate the word alignments, we call cdec's \texttt{fast\_align}
tool.

%% TODO: put an example here.

After all of this preprocessing and alignment, we have produced three files of
interest to Chipa: first, we have the lemmatized bitext corpus. Second, we have
the one-to-many word alignments between the two sides of the bitext. These two
files taken together constitute the ground truth for the Chipa system -- the
source lemmas, and the subsequences of the target text that are considered to
be their translations. Note that these alignments could be NULL -- a source
word could correspond with an empty subsequence of target-language tokens. At
test time, we will want to predict those translations, whatever they happen to
be.

The third file contains the annotated version of the source text, which
includes all of the information of the source side of the preprocessed bitext,
in addition to the original surface forms and the annotations extracted from
FreeLing analysis. Further annotations can be added by other tools; in general,
annotated source-language corpora could contain whatever kinds of signals we
might like to add to aid classifiers in predicting translations. In Chapter
\ref{chap:monolingual}, for example, we discuss features learnable with
monolingual resources such as part of speech tags, Brown clusters, and
embeddings learned with neural networks. In Chapter \ref{chap:multilingual}, we
discuss features that require parallel corpora, such as CL-WSD predictions into
languages other than the current target language. But in all of these cases,
the additional features are passed to the classifiers via this
corpus-annotation approach.

\begin{figure*}
\raggedright \begin{verbatim}
<div id="reader" class="content text_ltr">
  <a name="1"></a>
  <a href="/bible/more/Lam.1.1"
     style="color:black;text-decoration:none">
    <h2>El profeta </h2>
    <span class="verse" id="Lam_1_1">
    <strong class="verseno">1</strong>
    &nbsp;¡Pobrecita de ti, Jerusalén! <br />
    Antes eras la más famosa <br />
    de todas las ciudades. <br />
    ¡Antes estabas llena de gente, <br />
    pero te has quedado muy sola, <br />
    te has quedado viuda!  <br />
    ¡Fuiste la reina de las naciones, <br />
    pero hoy eres esclava de ellas! <br /> <p> </p></span>
  </a>
  <a name="2"></a>
  <a href="/bible/more/Lam.1.2"
     style="color:black;text-decoration:none">
  <span class="verse" id="Lam_1_2">
  <strong class="verseno">2</strong>
  &nbsp;Olvidada y bañada en lágrimas <br />
  pasas todas las noches. <br />
  Muchos decían que te amaban, <br />
  pero hoy nadie te consuela. <br />
  Los que se decían tus amigos <br />
  hoy son tus enemigos. <br />
  <p> </p></span>            </a>
\end{verbatim}
  \caption{The first two verses of the Book of Lamentations, \emph{Traducción en
  Lenguaje Actual} (TLA) version, in HTML as scraped from the web. Whitespace
  changes added here for readability.}
  \label{fig:es-html-sample}
\end{figure*}

\section{Exploring the Bitext}
\label{sec:exploring}
Even using only the Bible as bitext, we find a substantial number of
training examples for many of the most common word types, which, due to the
Zipfian distribution of word types in most texts, constitute a significant
fraction of the tokens exhibited. See Figures \ref{fig:mostcommon-lemmas} and
\ref{fig:mostcommon-surface} for more detail.
%% TODO: quantify this, per language!!
Some of these most common words
in the text are proper names or exhibit only one translation in the target
language, but for the most part we see, through the aligned bitext, that
the most common words are polysemous.
To take a few salient examples from the Spanish-English aligned text,
the verb \emph{hacer} can translate in a number of different ways: 'do',
'make', 'cause'; for \emph{decir}, we must choose between 'say', 'tell' or
'speak'; \emph{mujer} can mean either 'wife' or 'woman'; and \emph{reina} can
translate to 'reign' or 'queen'.  

\begin{figure*}
TODO: plot of rank of each word type versus the cumulative fraction of the text
covered by that word type. Do this for both surface forms and lemmas, for each
of en, es, qu, gn.
  \caption{Word ranks versus fraction of Bible tokens covered, for lemmas.}
  \label{fig:mostcommon-lemmas}
\end{figure*}

\begin{figure*}
TODO: plot of rank of each word type versus the cumulative fraction of the text
covered by that word type. Do this for both surface forms and lemmas, for each
of en, es, qu, gn.
  \caption{Word ranks versus fraction of Bible tokens covered, for
  (case-insensitive) surface forms.}
  \label{fig:mostcommon-lemmas}
\end{figure*}

There are analogous lexical ambiguities when translating from Spanish to
Guarani and Quechua.
For example, the Spanish \emph{pueblo} 'town' or 'people' can translate into
Quechua as \emph{llaqta} 'town/city', or \emph{runa} 'person'.
For translating from Spanish to Guarani, we see the verb \emph{volver}
'turn/return/repeat' translated as \emph{jey} 'again', but also as \emph{jere}
'turn'.

Figure~\ref{fig:mostcommon-en-es} gives the most common lemmas used in our
corpus for the four languages, along with their counts. In
Figure~\ref{fig:mostcommon-es-translations}, we see common lemmas from
Spanish along with their most likely translations into English, Guarani and
Quechua (as extracted from the bitext). For the purposes of this work, we will
consider words to be in-vocabulary if they appear at least fifty times in the
training corpus; we also do not include punctuation marks and stopwords (as
defined by the default NLTK stopword lists for English and Spanish) other than
common verbs.
%% XXX: magic number. Do we need to argue about fifty? What if we went down to
%% forty? Lower than that and this looks pretty ridiculous.

\TODO{update based on most recent preprocessing pipeline}

\begin{figure*}
  \begin{tiny}
  \begin{centering}
  \begin{tabular}{|r|c|c|}
    \hline
    rank & word type & count \\
    \hline
1 & be & 23542 \\
2 & have & 9389 \\
3 & say & 6664 \\
4 & yahweh & 6508 \\
5 & shall & 4413 \\
6 & god & 4112 \\
7 & come & 3603 \\
8 & son & 3303 \\
9 & go & 2985 \\
10 & do & 2799 \\
11 & king & 2744 \\
12 & one & 2547 \\
13 & israel & 2476 \\
14 & make & 2349 \\
15 & day & 2246 \\
16 & man & 2190 \\
17 & people & 2052 \\
18 & house & 2050 \\
19 & give & 1997 \\
20 & child & 1902 \\
21 & take & 1843 \\
22 & hand & 1793 \\
23 & father & 1782 \\
24 & land & 1758 \\
25 & men & 1518 \\
26 & also & 1456 \\
27 & let & 1386 \\
28 & bring & 1384 \\
29 & thing & 1375 \\
30 & lord & 1374 \\
31 & know & 1328 \\
32 & us & 1326 \\
33 & may & 1290 \\
34 & behold & 1289 \\
35 & city & 1233 \\
36 & therefore & 1220 \\
37 & word & 1195 \\
38 & speak & 1187 \\
39 & even & 1107 \\
40 & like & 1106 \\
41 & servant & 1073 \\
42 & name & 1060 \\
43 & see & 1037 \\
44 & offering & 1000 \\
45 & away & 1000 \\
46 & place & 993 \\
47 & david & 992 \\
48 & great & 936 \\
49 & among & 932 \\
50 & jesus & 930 \\
    \hline
  \end{tabular}
  \quad
  \begin{tabular}{|r|c|c|}
    \hline
    rank & word type & count \\
    \hline
1 & ser & 8436 \\
2 & haber & 8418 \\
3 & jehová & 6515 \\
4 & decir & 6006 \\
5 & hijo & 4955 \\
6 & dios & 4164 \\
7 & estar & 3925 \\
8 & hacer & 3872 \\
9 & tierra & 2832 \\
10 & rey & 2676 \\
11 & israel & 2445 \\
12 & hombre & 2418 \\
13 & tener & 2276 \\
14 & día & 2146 \\
15 & casa & 2056 \\
16 & dar & 2053 \\
17 & entonces & 2026 \\
18 & ir/ser & 1967 \\
19 & pueblo & 1918 \\
20 & pues & 1909 \\
21 & si & 1690 \\
22 & vosotros & 1623 \\
23 & así & 1597 \\
24 & mano & 1535 \\
25 & padre & 1521 \\
26 & señor & 1411 \\
27 & delante & 1292 \\
28 & ciudad & 1259 \\
29 & ver & 1241 \\
30 & poner & 1238 \\
31 & venir & 1182 \\
32 & palabra & 1178 \\
33 & tomar & 1105 \\
34 & cosa & 992 \\
35 & david & 976 \\
36 & responder & 957 \\
37 & mujer & 950 \\
38 & volver & 944 \\
39 & poder & 939 \\
40 & grande & 926 \\
41 & hermano & 912 \\
42 & ir & 904 \\
43 & corazón & 883 \\
44 & sacerdote & 873 \\
45 & lugar & 868 \\
46 & nombre & 867 \\
47 & salir & 833 \\
48 & sino & 831 \\
49 & hablar & 829 \\
50 & año & 823 \\
    \hline
  \end{tabular}
  \end{centering}
  \end{tiny}
  \caption{Some of the most common (lemmatized, non-stopword) word types in our
  English and Spanish Bibles}
  \label{fig:mostcommon-en-es}
\end{figure*}

\begin{figure*}
  \begin{tiny}
  \begin{centering}
  \begin{tabular}{|r|p{4.2cm}|p{4.2cm}|p{4.2cm}|}
    \hline
    es & translations (en)                    & translations (gn) & translations (qu) \\
    \hline
ser & be,  will be, it be, shall be              &   hína, niko, mba'e, ramo                                                              &  ka, kanqa, chayqa, kanki \\
haber & have,  there, i have, i                  &   vaekue, {\textlangle}hague, che, {\textlangle}ha'e                                   &  qan, chay, ma/mana, ni \\
%jehová & yahweh,  yahweh s, to, s                &  ñandejára,  jára, tupã, opa/pa                                                        & señor,  señor diosqa, señor diosmi, señor diospa \\
decir & say,  tell, speak, say to                &  'e,  'e ha'e, ha'e, 'e/ha'e                                                           &  ni, chaymi, hina/hinan, ni/nina/ninaku \\
dios & god,  lord, gods, yahweh                  &  tupã,  jára, momba'e, tupã jára                                                       &  diospa, diosqa, dios, diosmi \\
estar & be,  stand, now, behold                  &   \~{i}{i}, ime, hína, iko/ko                                                          &  ka/kasha, kasha, ka, kaq \\
hacer &  do, make, cause, he                     &   {\textlangle}japo, mba'e, haguã, japouka                                             &  ruwa, chay, ruwa/ruway, ruwa/ruwana \\
tierra & land, earth, ground,  country           &   yvy, {\textlangle}hetã, ko yvy, yvy ári                                              &  suyu, hallp'a, ka/kay pacha \\ %% , ka/kay \\
ir/ser & be,  go, come, depart                   &   ha, vaekue, upe, kuéra                                                               &  karqan, ri, ripu, hina/hinan \\
pueblo & people, among,  nation, multitude       &   {\textlangle}hetã, {\textlangle}hetã gua, opavave, israelgua                         & llaqta,  runa, llaq/llaqta, israel runa \\
pues &  for, therefore, so, then                 &   upe, niko, aipórõ, che                                                               &  chay, chaymi, chay hinaqa, chhaynaqa \\
si & if,  if you, but if, whether                &  ramo,  rire, ime, pende                                                               &  chayqa, ma/mana, chaypas, icha \\
así &  so, thus, this, therefore                 &   upe, péicha, kóicha, avei                                                            &  chay, ahi/ahina, chay hina, hina \\
%%padre & father,  parent, his father, father s    &  ru,  ru ypy, ypy, vaekue                                                              & tayta,  yaya, ñawpa tayta, tayta/taytay \\
padre & father,  parent, his father              &  ru,  ru ypy, ypy, vaekue                                                              & tayta,  yaya, ñawpa tayta, tayta/taytay \\
señor & lord, master,  sir, ruler                &  ñandejára,  jára, karai, che jára                                                     &  señor, apu, señorpa, señorníy \\
poner &  put, set, lay, make                     &   mo\~{i}{i}/\~{i}{i}, mo\~{i}{i}, mo\~{i}{i}/ñemo\~{i}{i}/\~{i}{i}, upéi              &  chura, hina, hinaspa, chay \\
%venir & come,  will come, behold, will           &   ju, guah\~{i}{i}{e}, aju/ju, rendápe                                                 &  hamu, chaya/chayamu, hamu/hamuq, chaya \\
%tomar & take,  shall take, took, he take         &   raha, momba'e, {\textlangle}hupi, japyhy                                             &  hap'i, hina, hina/hinan, apa \\
%responder & answer,  say, then, but              &  'e,  'e ha'e, katu, katu 'e                                                           &  kuti/kutichi, ni, chaymi, hina/hinan \\
mujer & woman, wife,  a wife, a woman            &  kuña, {\textlangle}hembireko,  menda, kuñakarai                                       & warmi,  war, warmi/warmiy, qhari \\
volver & return,  turn, again, back              &   jey, jeýta, ju jey, jere                                                             &  kutipu, wakmanta, kuti, kuti/kutimu \\
poder & can,  power, able, could                 &  katu,  pu'aka, ndaikatu, mbarete                                                      &  ati, ati/atiy, ma/mana, atiyniyoq \\
ir & go,  come, will, let                        &   ha, s\~{i}{i}{e}, ha ha, {\textlangle}hasa                                           &  ri, ripu, ri/rina, ri/risa \\
%% lugar & place,  instead, in, place where         &   {\textlangle}henda, {\textlangle}hendague/{\textlangle}hendaguépe/hendague, {\textlangle}henda/ha'e, {\textlangle}hendague/{\textlangle}hendaguépe/rendague  &  ranti, cheqasta, ma, cheqasman \\
salir & go out, come out,  out, go               &  s\~{i}{i}{e},  upe, ha, s\~{i}{i}{e} okápe                                            &  ri, lloqsispa, puriri, hina/hinan \\
%siervo & servant,  slave, male servant, young    &  {\textlangle}hembiguái,  mburuvicha, che, {\textlangle}huvicha                        &  kamachi, kama/kamachi, kama/kamachi/kamachiy, kama \\
judá & judah, jew,  belongs, of judah            &  judá, judagua,  judápe, judágui                                                       & judá, judá suyu,  judapi, judá ayllu \\
mismo &  same, himself, own, myself              &   voi, avei, upe, pete\~{i}{i}                                                         &  kikin, kaq, chay, hina/hinalla \\
% oír & hear, heard, listen,  when                 &  {\textlangle}hendu,  kuaa, {\textlangle}hendu/ndu, japysaka                           & uyari,  uya, uyari/uyarina, uyari/uyariqkuna \\
llevar & bring,  carry, take, bear               &  raha,  {\textlangle}hupi, raha hikuái, hikuái                                         &  apa, pusa, apa/apamu, apari/apariku \\
% hecho & do,  make, done, work                    &   {\textlangle}japo, {\textlangle}japo vaekue, {\textlangle}hembiapo, mba'e            &  ruwa, ru, kama, allinta \\
dicho & say, speak,  tell, word                  &  'e,  'e/ha'e, kóicha 'e, {\textlangle}he'ika                                          & ni,  diosmi ni, apu, diosmi \\
cielo & heaven, sky,  heavens, cloud             &  yvága,  ára, yvate, yvága pe/pegua                                                    & hana/hanaq pacha,  pacha, hana/hanaq, hana/hanaq pa \\
ojo & eye, sight,  in, his eye                   &   {\textlangle}hesa, {\textlangle}hecha, ma'\~{i}{i}{e}, che                           &  ñawi, qhawari, ri/riku, ruwa \\
llegar & come,  have come, reach, when           &  guah\~{i}{i}{e},  guah\~{i}{i}{e} hikuái, ke, ja/mboja/ñemboja                        &  chaya, chaya/chayamu, hamu, cha \\
%quedar & be,  stay, remain, be leave             &   pyta, {\textlangle}hemby, {\textlangle}heja, ndopyta                                 &  kapu/kapun, kanqa, qhepaq, qhepakurqan \\
%aquel & that,  him, will, everyone               &  upe,  pe, upe upe, umiha                                                              & chay,  pipas, haqay, chaypachan \\
%% cada & each, every,  everyone, every man         &   te\~{i}{i}, opa/pa, {\textlangle}he\~{i}{i}/{\textlangle}he\~{i}{i}me/te\~{i}{i}, pete\~{i}{i} te\~{i}{i}    &  sapanka, sapa, sapa/sapan, sapankanku \\
% medio &  middle, among, through, within          &   rupi, mbytépe, apytépe, pende                                                        &  ukhu, ukhupi, chawpi, qankuna \\
entrar & enter into, come, come into, go into, go&  ke,  guah\~{i}{i}{e}, ndoike, hikuái                                                  & hayku,  hayku/haykuna, ri, hay \\
%% llamar & call, name,  whose name, summon         &  {\textlangle}héra, {\textlangle}henói,  {\textlangle}henoika/henoika, {\textlangle}henói/nói                                          &  waq/waqya, sutiyoq, waqya, suti/suticha \\
llamar & call, name, summon                      &  {\textlangle}héra, {\textlangle}henói,  {\textlangle}henoika/henoika, {\textlangle}henói/nói                                          &  waq/waqya, sutiyoq, waqya, suti/suticha \\
%espíritu & spirit,  breath, by, trouble          &  espíritu,  espíritu santo, py'a, pu'aka                                               &  santo espirituq, santo, espiritun, santo espirituqa \\
%monte & mountain, on,  mount, hill country       &  yvyty,  yvyty {\textlangle}hu'ã, ka'aguy, yvyty rupi                                  & orqo,  orqopi, orqoman, orqokuna \\
subir & go up, come up, up,  ascend              &   jupi, ha, s\~{i}{i}{e}, ju                                                           &  wicha, ri, wichari, chay/chayman \\
obra & work, do,  deed, doings                   &   {\textlangle}hembiapo, {\textlangle}japo, mba'e, mba'e {\textlangle}japo             & ruwa,  llank'a, ruwa/ruwana, ruwa/ruway \\
%% puerta & gate, door, at,  threshold              &  {\textlangle}hok\~{i}{i}{e},  k\~{i}{i}{e}, táva {\textlangle}hok\~{i}{i}{e}, {\textlangle}hok\~{i}{i}{e} nguéra          & punku,  hayku/haykuna, punku/punkuta, pun \\
% pecado & sin, iniquity, trespass,  transgression &  angaipa,  {\textlangle}hembiapo vai, vai, {\textlangle}heja rei                       & hucha, huchalli/huchalliku,  hucha/huchaku, hu \\
hija & daughter,  her, her daughter, woman       &  rajy,  kuña, memby, táva                                                              & ususi,  warmi, warmi wawa, llaq/llaqta \\
% junto & by,  together, beside, at                &   {\textlangle}hembe'y, ypýpe, ykére, \~{i}{i}                                         &  qaylla, patapi, kuska, qayllapi \\
dejar & leave,  let, allow, they                 &   {\textlangle}heja, poi, ja, jei                                                      &  kachari, ama, ma/mana, maña/mañana \\
    \hline
  \end{tabular}
  \end{centering}
  \end{tiny}
  \caption{Selected common Spanish word types with their most likely
translations. These were picked for interesting polysemy from the 100 most
common word types.}
  \label{fig:mostcommon-es-translations}
\end{figure*}

\begin{figure*}
  \begin{tiny}
  \begin{centering}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    es & entropy (en) & entropy (gn) & entropy (qu) \\
    \hline
ser    &     2.49         &      3.90        &       3.25       \\  
haber  &     2.88         &      3.57        &       2.70       \\
decir  &     1.57         &      2.05        &       1.82       \\
dios   &     0.33         &      1.56        &       3.99       \\
estar  &     2.07         &      3.01        &       2.61       \\
hacer  &     4.07         &      2.68        &       2.67       \\
tierra &     2.00         &      2.96        &       3.58       \\
ir/ser &     2.65         &      3.19        &       2.92       \\
pueblo &     0.56         &      3.36        &       3.21       \\
pues   &     3.18         &      3.73        &       2.90       \\
si     &     2.68         &      3.06        &       2.91       \\
así    &     2.95         &      2.73        &       3.27       \\
padre  &     0.53         &      1.48        &       2.60       \\
señor  &     0.78         &      3.15        &       3.38       \\
poner  &     4.32         &      3.05        &       2.93       \\
mujer  &     2.11         &      2.28        &       1.88       \\
volver &     3.89         &      3.59        &       3.65       \\
poder  &     3.51         &      3.12        &       3.36       \\
ir     &     3.49         &      2.96        &       2.77       \\
salir  &     3.43         &      2.40        &       3.91       \\
judá   &     0.23         &      2.07        &       2.81       \\
mismo  &     3.27         &      2.94        &       2.87       \\
llevar &     3.70         &      1.81        &       2.79       \\
dicho  &     1.91         &      2.60        &       2.99       \\
cielo  &     1.32         &      2.21        &       2.13       \\
ojo    &     2.03         &      2.52        &       2.57       \\
llegar &     3.23         &      2.76        &       3.27       \\
entrar &     3.77         &      1.88        &       2.71       \\
llamar &     1.64         &      2.69        &       3.49       \\
subir  &     2.85         &      3.39        &       3.19       \\
obra   &     1.99         &      3.27        &       3.42       \\
hija   &     0.35         &      2.33        &       2.29       \\
dejar  &     4.03         &      2.67        &       2.80       \\
    \hline
  \end{tabular}
  \end{centering}
  \end{tiny}
  \caption{Common Spanish word types and the entropy, in bits, faced by a
  system that must choose among the possible alternatives} 
  \label{fig:mostcommon-es-entropy}
\end{figure*}
%% TODO "I did not find a mention / explanation of this figure in the text."


Using larger bitexts of course will allow us to construct training and test
sets for a broader vocabulary, and with better statistical support for the
words already present. However even when working with the Bible, we
have at least fifty uses of over a thousand different lemmas, for English and
Spanish, as shown in Figure \ref{fig:mostcommon-en-es}.

\TODO{For comparison, do we want to include the stats for Europarl here too? Or
maybe we could put that in the ``multilingual" chapter?}

\section{Baseline Chipa System}
At its core, the Chipa software takes in a word-aligned bitext corpus, with
annotations for the source tokens. It then trains CL-WSD classifiers for
source-language word types on demand. 

The software holds all of the available bitext in memory. On request, it
constructs a training set for learning to disambiguate a given word
by retrieving all sentences that contain that word,
finding the instances of that word and its aligned translations, and extracting
features (see Figure~\ref{fig:baselinefeatures}) from the source side and its
annotations.
If a source-language word has been seen aligned to only one target-language
type, then this is simply noted, and if the source word is not present in the
training data, then that word is marked as out-of-vocabulary. In these two
cases, we do not train a classifier for that word, since there is no
ambiguity present in our examples and the Chipa software cannot provide any
useful guidance to its caller. A machine translation system may, for example,
refuse to translate sentences with OOV words, or more likely, will simply
assume the identity translation, effectively passing them through untranslted.

Since source-language tokens may be NULL-aligned (i.e., not all words in the
source text will have corresponding words in the target text), both in the
training data and in translations, Chipa provides the option to request
classifiers that consider NULL as a valid label for classification, or not, as
appropriate for the translation application. We here report classification
accuracies for both settings.

Memory permitting, Chipa classifiers are kept cached for later usage. Chipa can
also be run as a server, providing an interface whereby client programs can
request CL-WSD decisions over remote procedure calls (RPC).

Chipa's classifiers are trained with the scikit-learn machine learning toolkit
\cite{scikit-learn} for Python and its associated NLTK interface, though in
earlier versions, we used the megam package \cite{daume04cg-bfgs}, also through
NLTK.  By default, we use Logistic Regression classifiers (also known as
Maximum Entropy), with the default scikit-learn settings.
Maximum entropy classifiers are well-suited to this sort of classification
task, as they are robust to adding large numbers of features, even
highly-correlated ones \cite{nigam1999using}.
Classifiers relying on bag-of-words  ...
%% TODO ""Explain for CS committee members that that is often the case when you
%% use words as features?"
%% regarding correlated BOW features

Furthermore, because we have a large number of features but a relatively small
number of samples per classifier, we train with L1 regularization rather than
L2~\cite{ng2004feature}.
We can also set a regularization parameter during training, to encourage
more parsimonious solutions and thus avoid overfitting.
Here by default we set the regularization parameter to $C=1.0$.
%% TODO "If the results were worse, you can just report this in one sentence."

%% XXX working here

We have also tried a variety of classification algorithms; scikit-learn makes
experimentation with different classifiers and parameter settings
straightforward. We have tried random forests and linear support vector
machines, and of course compare these to the most-frequent-sense baseline.

\TODO{Report results with a few different regularization settings}
\TODO{Report results with SVMs and random forests too.}

\begin{figure*}
  \begin{centering}
  \begin{tabular}{|r|p{11cm}|}
    \hline
    name          & description  \\
    \hline
    \texttt{bagofwords}    & a feature counting each lemma in the source sentence \\
    \hline
    \texttt{bagofsurface}  & like \texttt{bagofwords}, but with surface forms \\
    \hline
    \texttt{window}       & a binary feature for each of the lemmas in the immediately surrounding three-word context window \\
    \hline
    \texttt{surfacewindow} & like \texttt{window}, but with surface forms \\
    \hline
  \end{tabular}
  \end{centering}
  \caption{Features for the baseline Chipa system}
  \label{fig:baselinefeatures}
\end{figure*}

\section{Classification Results for the Baseline System}

\TODO{insert a bunch of numbers here}

\begin{itemize}
\item en-es
\item es-en
\item es-gn
\item es-qu
\end{itemize}

\TODO{Words in Spanish for which we gain the most by training a classifier --
measured as the difference in accuracy between MFS}

\chapter{Data Sets, Tasks, Software and Evaluation}
\label{chap:evaluation}

In order to evaluate our CL-WSD approaches and their effects on translation
systems, we will need to have some metrics for comparison between our
techniques, sensible baselines, and results reported by other researchers.

One clear \emph{in vitro} approach for measuring the success of a CL-WSD system
will be the metrics provided by the SemEval shared tasks
\cite{task10}; measuring our performance on these test
sets will provide a clear comparison between our work, several other CL-WSD
systems from the past few years, and the results posted by Els Lefever's
ParaSense system.

These test sets are limited in scope, however, and will only demonstrate our
performance for translating individual polysemous English nouns, ignoring the
problem of translating the rest of the sentence.
Furthermore, our broader practical goal is to use Spanish as a source language,
and the resources that we can use to analyze an input English sentence will be
different from those for Spanish.

An approach that we took recently \cite{rudnick-gasser:2013:HyTra-2013} was to
simply report classification accuracy for the CL-WSD task of labeling input
Spanish sentences with Guarani stems, from a held-out test set sampled from the
Bible.
This will be closer to an accurate assessment of a lexical selection system
trying to translate every word in running text with a realistically large
vocabulary. Collecting larger bitext corpora would allow us to construct test
sets from a broader domain.

We will additionally want to conduct \emph{in vivo} evaluations of our CL-WSD
techniques while they are in use for lexical selection in running MT systems.
Here we can use standard approaches for evaluating MT, such as BLEU and METEOR
scores.

\section{Data Sets and Preprocessing}

The largest corpus that we have available for all of the languages is the
Bible. It has been translated into many different languages...

We got the Quechua Bible from Chris Loza and Rada Mihalcea; the preparation
of the Quechua corpus is described in Loza's masters thesis \cite{chrisloza}.

\subsection{Preprocessing}

\section{Morphological Analysis for Guarani}
\label{sec:guaranima}
We analyze the Spanish and Guarani Bible using our in-house morphological
analyzer, originally developed for Ethiopian Semitic languages 
\cite{gasser:eacl09}.

For this work, I used ParaMorfo 1.1 and AntiMorfo 1.2.

As in other, more familiar, modern
morphological analyzers such as \cite{beesley+karttunen}, analysis in our
system is modeled by cascades of finite-state transducers (FSTs).  To solve the
problem of long-distance dependencies, we extend the basic FST framework using
an idea introduced by Amtrup \cite{amtrup:03}.  Amtrup starts with the
well-understood framework of weighted FSTs, familiar from speech recognition.
For speech recognition, FST arcs are weighted with probabilities, and a
successful traversal of a path through a transducer results in a probability
that is the product of the probabilities on the arcs that are traversed, as
well as an output string as in conventional transducers.  Amtrup showed that
probabilities could be replaced by feature structures and multiplication by
unification.  In an FST weighted with feature structures, the result of a
successful traversal is the unification of the feature structure ``weights'' on
the traversed arcs, as well as an output string.  Because a feature structure
is accumulated during the process of transduction, the transducer retains a
sort of memory of where it has been, permitting the incorporation of
long-distance constraints such as those relating the negative prefix and suffix
of Guarani verbs.

In our system, the output of the morphological analysis of a word is a root and
a feature structure representing the grammatical features of the word.  We
implemented separate FSTs for Spanish verbs, for Guarani nouns, and for the two
main categories of Guarani verbs and adjectives.  Since Spanish nouns and
adjectives have very few forms, we simply list the alternatives in the lexicon
for these categories.  For this paper, we are only concerned with the roots of
words in our corpora, so we ignore the grammatical features that are output
with each word.


\footnote{Michael Gasser's morphological analyzers for Guarani, Quechua,
Spanish, and other languages are available at
\url{http://www.cs.indiana.edu/~gasser/Research/software.html} }

\section{Tasks and Evaluation}



\section{Baseline System}
Chipa is a 

\begin{figure*}
  \begin{centering}
  I'll show \emph{you} a feature set.
  \end{centering}
  \caption{Yo, these are the features we use.}
  \label{fig:baselinefeatures}
\end{figure*}



Since source-language tokens may be NULL-aligned (\emph{i.e.,} not all words in
the source text will have corresponding words in the target text), both in the
training data and in translations, we allow users to request classifiers that
consider NULL as a valid label for classification, or not, as appropriate for
the application.

The software holds all of the available bitext in a database, retrieving the
relevant training sentences and learning classifiers on demand.
If a source word has been seen with multiple different translations, then a
classifier will be trained for it. If it has been seen aligned to only one
target-language type, then this is simply noted, and if the source word is not
present in the training data, then that word is marked out-of-vocabulary.
Memory permitting, these classifiers and annotations are kept cached for later
usage. Chipa can be run as a server, providing an interface whereby client
programs can request CL-WSD decisions over RPC.



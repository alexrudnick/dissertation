\chapter{Data Sets, Tasks, Software and Evaluation}
\label{chap:evaluation}
In order to evaluate our CL-WSD approaches and their effects on translation
systems, we will need to have some basis for comparison between the various
techniques that we would like to evaluate, sensible baselines, and results
reported by other researchers.

So in this chapter, I describe the tasks and data sets that we will be
examining for the rest of the dissertation, as well as the basic version of the
Chipa software, which will be extended in subsequent chapters.

\section{Measuring CL-WSD Classification Accuracy}
%% XXX
One clear \emph{in vitro} approach for measuring the success of a CL-WSD system
is to report its classification accuracy on pre-labeled data. Strictly
speaking, we do not have pre-labeled data: our bitext does not
come with sub-sentential alignments, but automatic word alignments will provide
a good approximation, as long as our sentence (or verse, for Bible text)
alignments are accurate. For the purposes of this work, we will assume that our
automatic alignments are correct, putting in our best effort preprocessing
the available text so that the aligner can produce sensible output.

Using our automatically-aligned bitext as labeled data allows us to approximate
the task faced by an MT system trying to do lexical selection for running text,
since we can train our classifiers for many different word types. We do not
have sufficient training data to build classifiers for every possible word, but
this is a problem faced by NLP systems broadly.

For comparison with other work, we can also run our systems on the SemEval
shared task test sets, which are publicly available. Our task here is framed in
the same way as the CL-WSD shared tasks from 2010 and 2013, so measuring our
performance on these test sets will provide a straightforward comparison
between this work, its variations, and several other CL-WSD systems from recent
years.  These test sets are limited in scope, however, and will only
demonstrate Chipa's performance for translating individual English nouns.
It is also worth noting that our more immediate practical goal is to aid
translation from Spanish into lower-resourced languages, and the resources that
we can use to analyze an input English sentence will be different from those
for Spanish.

\section{Measuring MT Improvements}
We will additionally want to conduct \emph{in vivo} evaluations of our CL-WSD
techniques while they are in use for lexical selection in running MT systems.
Here we can use standard approaches for evaluating MT, such as BLEU and METEOR
scores.

For these experiments, we sample sentences from the available bitext --
particularly ones that contain polysemous words for which we can train
classifiers! -- and run the MT systems both with and without Chipa enabled.
These experiments are described in more detail in
Chapter~\ref{chap:integration}, where I also discuss how to integrate Chipa
into machine translation software.

\section{Data Sets and Preprocessing}
The largest corpus that we have available for all of our source and target
languages is the Bible. There are translations available for many different
languages, and while the translations are not always done by native speakers,
the translators are often driven by missionary zeal to get the best possible
translation~\cite{DBLP:journals/lre/ResnikOD99}, so we can be confident that
our Bibles are at least fairly good examples of our under-resourced languages.

Our text came from a number of different sources. For Spanish, we use the
Reina-Valera translation (1995 edition), which we were able to scrape from a
Bible study website.
Our Guarani translation, \emph{Ñandejara Ñe'e} (1997 edition) was scraped from
the same website.
For our Quechua Bible (XXX: what's the name of the translation?) was provided
by Chris Loza and Rada Mihalcea; the preparation of the Quechua corpus is
described in Loza's masters thesis \cite{chrisloza}.
For English, we use the public domain World English Bible translation, which
seems tailor-made for exactly this sort of application.

\subsection{Preprocessing}
%% XXX
There is a nontrivial amount of preprocessing that must be done in order to get
our Bible text into a format usable by our tools.
This is largely due to the various markup formats used for different
translations in different languages. For each of the formats and translations,
we need to understand the markup format enough to know which text comes from
which book, chapter, and verse number. This triple uniquely identifies a
unit of text, and verse numbers are nearly standardized across translations.
There are a few exceptions, however: translations produced by different
religious traditions may include different books (modern Catholic editions have
a superset of the books found in most Protestant editions, for example), and
slightly different chapters. Additionally, in some translations, such as our
Guarani edition, a few verses have been combined into a single segment for a
more natural translation.

In any case, if a particular book/chapter/verse is present in two Bible
corpora, then the two verses can safely be considered to be translations of one
another.  Once we find the matching verses, we can build a bitext corpus
formats amenable to the rest of our tools.

Our English translation is distributed in a format called USFM (Unified
Standard Format Markers), which is a markup language developed by a working
group of Bible translators and used widely by the Bible-related software
community.
\footnote{See \url{http://paratext.org/about/usfm} for more about USFM. USFM is
widely deployed; the website from which we scraped the Spanish and Guarani
corpora appears to render its HTML from a USFM source. 
There is an entire community of Bible software developers, and it has a wing
that advocates Open Source and translations unencumbered by copyright.  One
could delve arbitrarily deeply into the history of religious-text translators
and their relationships with technology and copyright, but one has an NLP
dissertation to write.}
While there are a number of tools that consume this format, I did not find one
that handles our particular use case of stripping metadata and simply mapping
from verses to flat text, so I wrote a short script to parse USFM and produce
text in an easily-alignable format.

%% SECTIONGOODTOHERE
In either case, we do lowercasing, tokenization and lemmatization on the input
text. For lemmatization, we use Mike's morphological analyzer from Paramorfo.  

For Spanish, 
bible chopping: Spanish
For Spanish text, we can try several different Bible translations, all of which
are available electronically.
We have the Traducción en Lenguaje Actual (TLA), and the Reina Valeria (1995
edition), which were both found on the web.


In both cases, we change them into a single standard format, which is used by
scripts further down the pipeline.





bible chopping: Guarani
Our Guarani text is spidered from the web. We use beautifulsoup to pull out the
flat text from the HTML.

Guarani text requires morphological analysis: we want to pick lemmas rather
than fully inflected forms.


bible chopping: Quechua

* We have a Quechua translation of the Catholic Bible, provided by Rada's group.

* We also have a pre-preprocessed version from SQUOIA group.

Using MA from Mike's AntiMorfo.

producing bitext

We just take all the verse numbers that line up and call those matching
sentences. Should we do some heuristic to check to see that they're about the
same length? Probably. Moses and cdec have something like that, yeah?  


\subsection{Morphological Analysis}
%% XXX
\label{sec:guaranima}
Lemmatized with NLTK's wordnet lemmatizer.


We analyze the Spanish and Guarani Bible using our in-house morphological
analyzer, originally developed for Ethiopian Semitic languages 
\cite{gasser:eacl09}.

For this work, I used ParaMorfo 1.1 and AntiMorfo 1.2.

As in other, more familiar, modern
morphological analyzers such as \cite{beesley+karttunen}, analysis in our
system is modeled by cascades of finite-state transducers (FSTs).  To solve the
problem of long-distance dependencies, we extend the basic FST framework using
an idea introduced by Amtrup \cite{amtrup:03}.  Amtrup starts with the
well-understood framework of weighted FSTs, familiar from speech recognition.
For speech recognition, FST arcs are weighted with probabilities, and a
successful traversal of a path through a transducer results in a probability
that is the product of the probabilities on the arcs that are traversed, as
well as an output string as in conventional transducers.  Amtrup showed that
probabilities could be replaced by feature structures and multiplication by
unification.  In an FST weighted with feature structures, the result of a
successful traversal is the unification of the feature structure ``weights'' on
the traversed arcs, as well as an output string.  Because a feature structure
is accumulated during the process of transduction, the transducer retains a
sort of memory of where it has been, permitting the incorporation of
long-distance constraints such as those relating the negative prefix and suffix
of Guarani verbs.

In our system, the output of the morphological analysis of a word is a root and
a feature structure representing the grammatical features of the word.  We
implemented separate FSTs for Spanish verbs, for Guarani nouns, and for the two
main categories of Guarani verbs and adjectives.  Since Spanish nouns and
adjectives have very few forms, we simply list the alternatives in the lexicon
for these categories.  For this paper, we are only concerned with the roots of
words in our corpora, so we ignore the grammatical features that are output
with each word.

\footnote{Michael Gasser's morphological analyzers for Guarani, Quechua,
Spanish, and other languages are available at
\url{http://www.cs.indiana.edu/~gasser/Research/software.html}}



\subsection{Alignment}
%% XXX
To get one-to-many word alignments, we use the \texttt{fast\_align} tool from
cdec \cite{dyer-EtAl:2010:Demos}, with its default settings
\footnote{With the command given at
\url{http://www.cdec-decoder.org/guide/fast_align.html} }
, which runs several
iterations of a Bayesian variant of IBM Models 1 and 2, with a prior
encouraging diagonal alignments. During training, it tunes the parameters on
the priors, %% XXX check up about this

where each source word is aligned to 0 or more target words.

Note that when working with Bible text, we use verses rather than sentences as
the basic unit of text.



\section{Exploring the Bitext}
%% XXX
Even using only the Bible, we can find a usefully large number of training
examples for many common word types. Some of these most common words are proper
names or exhibit only one translation in the other language, but for the most
part, we can see with bitext that they are polysemous.

In Figure~\ref{fig:mostcommon-en-es}, we see the most common lemmas used in our
Bible bitext for our languages, along with their counts. In
Figure~\ref{fig:mostcommon-es-translations}, we see the most common lemmas from
Spanish, along with their most likely translations into English, Guarani and
Quechua. For the purposes of this work, we will consider words to be
in-vocabulary if they appear at least fifty times in the training corpus.
%% XXX: magic number. Do we need to argue about fifty? What if we went down to
%% forty? Lower than that and this looks pretty ridiculous.

\begin{figure*}
  \begin{tiny}
  \begin{centering}
  \begin{tabular}{|r|c|c|}
    \hline
    rank & word type & count \\
    \hline
1 & say & 6992 \\
2 & yahweh & 6833 \\
3 & shall & 4802 \\
4 & god & 4340 \\
5 & come & 3820 \\
6 & son & 3490 \\
7 & go & 3198 \\
8 & king & 2875 \\
9 & one & 2700 \\
10 & israel & 2581 \\
11 & make & 2520 \\
12 & day & 2373 \\
13 & man & 2332 \\
14 & people & 2149 \\
15 & house & 2130 \\
16 & give & 2115 \\
17 & child & 1981 \\
18 & take & 1938 \\
19 & father & 1913 \\
20 & hand & 1874 \\
21 & land & 1818 \\
22 & men & 1615 \\
23 & also & 1556 \\
24 & let & 1508 \\
25 & thing & 1479 \\
26 & bring & 1467 \\
27 & lord & 1434 \\
28 & us & 1407 \\
29 & know & 1401 \\
30 & may & 1351 \\
31 & behold & 1342 \\
32 & therefore & 1310 \\
33 & city & 1288 \\
34 & speak & 1271 \\
35 & word & 1261 \\
36 & like & 1179 \\
37 & even & 1177 \\
38 & servant & 1129 \\
39 & name & 1117 \\
40 & see & 1106 \\
    \multicolumn{3}{|c|}{...} \\ 
1029 & preserve & 50 \\
1030 & calf & 50 \\
1031 & prey & 50 \\
1032 & blue & 50 \\
1033 & ephod & 50 \\
1034 & repent & 50 \\
1035 & hearing & 50 \\
1036 & basket & 50 \\
1037 & circumcise & 50 \\
1038 & veil & 50 \\
1039 & weak & 50 \\
1040 & scripture & 50 \\
1041 & milk & 50 \\
1042 & ought & 50 \\
1043 & wheat & 50 \\
1044 & justify & 50 \\
1045 & engrave & 50 \\
    \hline
  \end{tabular}
  \quad
  \begin{tabular}{|r|c|c|}
    \hline
    rank & word type & count \\
    \hline
1 & ser & 8994 \\
2 & haber & 8812 \\
3 & jehová & 6839 \\
4 & decir & 6324 \\
5 & hijo & 5136 \\
6 & dios & 4399 \\
7 & hacer & 4092 \\
8 & tierra & 2944 \\
9 & rey & 2807 \\
10 & hombre & 2554 \\
11 & israel & 2549 \\
12 & tener & 2411 \\
13 & día & 2263 \\
14 & dar & 2197 \\
15 & casa & 2145 \\
16 & entonces & 2138 \\
17 & ir/ser & 2110 \\
18 & pues & 2038 \\
19 & pueblo & 2007 \\
20 & si & 1804 \\
21 & vosotros & 1709 \\
22 & así & 1692 \\
23 & mano & 1606 \\
24 & padre & 1587 \\
25 & señor & 1478 \\
26 & delante & 1360 \\
27 & ver & 1326 \\
28 & ciudad & 1317 \\
29 & poner & 1316 \\
30 & palabra & 1246 \\
31 & venir & 1244 \\
32 & tomar & 1150 \\
33 & david & 1045 \\
34 & cosa & 1044 \\
35 & responder & 1024 \\
36 & mujer & 1010 \\
37 & volver & 1009 \\
38 & poder & 1005 \\
39 & ir & 985 \\
40 & grande & 965 \\
    \multicolumn{3}{|c|}{...} \\ 
1030 & parábola & 51 \\
1031 & tropezar & 51 \\
1032 & plaza & 51 \\
1033 & mancha & 51 \\
1034 & necesitar & 51 \\
1035 & cabrío & 51 \\
1036 & espalda & 51 \\
1037 & rescatar & 50 \\
1038 & adversario & 50 \\
1039 & preso & 50 \\
1040 & recompensa & 50 \\
1041 & dirigir & 50 \\
1042 & dedo & 50 \\
1043 & banquete & 50 \\
1044 & circuncidar & 50 \\
1045 & engaño & 50 \\
1046 & simeón & 50 \\
    \hline

  \end{tabular}
  \end{centering}
  \end{tiny}
  \caption{Some of the most common (lemmatized, non-stopword) word types in our
  English and Spanish Bibles}
  \label{fig:mostcommon-en-es}
\end{figure*}

\begin{figure*}
  \begin{centering}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    rank & word type (es) & translations (en) & translations (gn) & translations (qu) \\
    \hline
    1    & dios   &  god, the management, the lord & guarani & quechua \\
    2    & pelear &  hit, smack, smite             & guarani & quechua \\
    \multicolumn{5}{|c|}{...} \\ 
    500 &  & & & \\
    \hline
  \end{tabular}
  \end{centering}
  \caption{Common Spanish word types with their associated translations.}
  \label{fig:mostcommon-es-translations}
\end{figure*}

Using larger bitexts of course allows us to construct training and test sets
for a broader vocabulary, however even when working with the Bible, we have
at least fifty uses of over a thousand different lemmas, for English and
Spanish, as we see in Figure \ref{fig:mostcommon-en-es}.

Our text from Europarl is in a significantly different domain, as we can see by
plotting the most common words from that bitext.

\section{Baseline System}
%% XXX

At its heart, the Chipa software takes in a bitext corpus and the associated
alignments and then, on demand, trains classifiers for words types from the
source language in order to determine their translations.

The software holds all of the available bitext in memory, retrieving the
relevant training sentences as necessary.
If a source word has been seen with multiple different translations, then a
classifier will be trained for it. If it has been seen aligned to only one
target-language type, then this is simply noted, and if the source word is not
present in the training data, then that word is marked out-of-vocabulary.

%% XXX: it would be cool to have an example alignment here, maybe a word
%% aligned to a multi-word expression

Classifiers are trained with the scikit-learn machine learning toolkit
\cite{scikit-learn} for Python and its associated NLTK interface, though in
earlier versions, we used megam \cite{daume04cg-bfgs}, also through NLTK.

Since source-language tokens may be NULL-aligned (i.e., not all words in the
source text will have corresponding words in the target text), both in the
training data and in translations, we allow users to request classifiers that
consider NULL as a valid label for classification, or not, as appropriate for
the application. We report classification accuracies for both settings.

Memory permitting, these classifiers and annotations are kept cached for later
usage. Chipa can also be run as a server, providing an interface whereby client
programs can request CL-WSD decisions over RPC.

\begin{figure*}
  \begin{centering}
  \begin{tabular}{|r|c|}
    \hline
    name      & description  \\
    \hline
    what      & what is this \\
    no really & please tell me \\
    \hline
  \end{tabular}
  \end{centering}
  \caption{Features for the baseline Chipa system}
  \label{fig:baselinefeatures}
\end{figure*}

\section{Classification Results for the Baseline System}
%% XXX

\begin{itemize}
\item en-es
\item es-en
\item es-gn
\item es-qu
\end{itemize}

Words in Spanish for which we gain the most by training a classifier --
measured as the difference in accuracy between

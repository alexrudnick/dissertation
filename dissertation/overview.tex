\chapter{Overview}
In this dissertation, I will investigate techniques for machine translation for
a language pair with modest resources, particularly one for which we have
relatively little training data available for the target language.  I propose
that cross-lingual word sense disambiguation (CL-WSD) is a feasible and
practical means for lexical selection in this setting, and will demonstrate
this in experiments on translating from Spanish to Guarani, the co-official
languages of Paraguay.

I will develop new CL-WSD approaches, including the use of unsupervised
clustering on source-language text, multilingual corpora for the source
language, and sequence-labeling techniques. I will also integrate the CL-WSD
software into several different machine translation systems to show its
practical applicability.

%% XXX: why the black box here?
Lexical ambiguity presents a daunting challenge for rule-based machine
translation (RBMT) systems;
many words have multiple possible translations in the target language.
Moreover, several translations of a given word may all be syntactically valid
in context, but have significantly different meanings. Even when choosing among
near-synonyms, we would like to respect the selectional preferences of the
target language so as to produce natural-sounding output text.  The human
authors of the rules in an RBMT system may not have enumerated the exact
contexts or classes of contexts in which each target-language form should be
used -- this is a difficult and time-consuming task!

%% XXX
We really need to talk more about hybridity in here and what's rule-based and
what's statistical and how this is part of the ongoing battles getting fought
out in MT research.



To appreciate the word-sense disambiguation problem embedded in machine
translation, consider for a moment the different senses of ``have" in
English. In \emph{have a sandwich}, \emph{have a bath}, \emph{have an
argument}, and even \emph{have a good argument}, the meaning of the verb ``to
have" is quite different. It would be surprising if our target language,
especially if it is not closely related, used a light verb that could appear in
all of these contexts.

A concrete example for different lexicalization patterns in Spanish and Quechua
are the transitive motion verbs: The Spanish lemmas contain information about
the path of the movement, e.g. {\em traer} - 'bring (here)' vs. {\em llevar} -
'take (there)'. Quechua roots, on the other hand, use a suffix ({\em -mu}) to
express direction, but instead lexicalize information about the manner of
movement and the object that is being moved. Consider the following examples:

\begin{itemize}
\renewcommand{\labelitemii}{$\bullet$}
 \small
 \item[] general motion verbs:
 \begin{itemize}
 \item {\em pusa-(mu-)}: `take/bring a person'
 \item {\em apa-(mu-)-}: `take/bring an animal or an inanimated object'
 \end{itemize}
 \item[] motion verbs with manner:
 \begin{itemize}
 \item {\em marq'a-(mu-)}: `take/bring smth. in one's arms'
 \item {\em q'ipi-(mu-)}:  `take/bring smth. on one's back or in a bundle'
 \item {\em millqa-(mu-)}: `take/bring smth. in one's skirts'
 \item {\em hapt'a-(mu-)}: `take/bring smth. in one's fists'
 \item {\em lluk'i-(mu-)}: `take/bring smth. below their arms'
 \item {\em rikra-(mu-)}:  `take/bring smth. on one's shoulders'
 \item {\em rampa-(mu-)}:  `take/bring a person holding their hand'
 \end{itemize}
\end{itemize}

The correct translation of Spanish {\em traer} or {\em llevar} into Quechua
thus depends on the context. Furthermore, different languages simply make
different distinctions about
the world. The Spanish \emph{hermano} 'brother', \emph{hijo} 'son' and
\emph{hija} 'daughter'
all translate to different
Quechua terms based on the person related to the referent; a daughter relative to
her father is \emph{ususi}, but when described relative to her mother,
\emph{warmi wawa} \cite{academiamayor}.

Chipa, then, must learn to make these distinctions automatically, learning from
examples in available word-aligned bitext corpora. Given such a corpus, we can
discover the different possible translations for each source-language word, and
with supervised learning, how to discriminate between them.  Since instances of
a source-language word may be NULL-aligned, both in the training data and in
actual translations, we allow users to request classifiers that consider NULL
as a valid label for classification, or not, as appropriate for the
application.

The software holds all of the available bitext in a database, retrieving the
relevant training sentences and learning classifiers on demand.
If a source word has been seen with multiple different translations, then a
classifier will be trained for it. If it has been seen aligned to only one
target-language type, then this is simply noted, and if the source word is not
present in the training data, then that word is marked out-of-vocabulary.
Memory permitting, these classifiers and annotations are kept cached for later
usage. Chipa can be run as a server, providing an interface whereby client
programs can request CL-WSD decisions over RPC.



%%TODO(alexr): get lots of good examples
%% XXX: need several good examples here

%% ... we should actually discover these from data! Let's align europarl and find
%% them!

%% es->en examples...

%% example of completely different meanings

%% example of selectional preferences, but similar meaning

%% then es->gn examples

Writing lexical selection rules by hand, while possible in principle, is
tedious and error-prone.
Bilingual informants, if available, may not be able to enumerate the contexts
in which they would choose one alternative over another. Thus we would like to
learn from corpora when possible.
We do not have very large sentence-aligned bitext corpora for most language
pairs, however, and will have to make do with the Bible, which is an acceptable
starting point.\footnote{The
Bible has been translated into a substantial fraction of the world's languages
and contains more than thirty thousand verses (roughly, sentences) in
a variety of text genres. For an overview of using the Bible for multilingual
NLP applications, please see Resnik \emph{et al.}
\cite{DBLP:journals/lre/ResnikOD99} and Mayer and Cysouw
\cite{MAYER14.220.L14-1215}.} As statistical machine translation systems
improve dramatically as larger bitext corpora become available, ... , some
techniques . However, that project is not the focus of this dissertation work.

However, for most language pairs, suitably large
sentence-aligned bitext corpora are not available,
so creating and deploying a
translation system based on machine learning techniques will require collecting
a larger corpus.

The major contributions of this work will be new approaches for CL-WSD,
integrating CL-WSD into MT systems, and building MT systems that target
morphologically rich languages with few language resources.  Additionally, on a
practical level we will develop a suite of reusable open-source software
including a hybrid MT system for Spanish-Guarani,

All of the software used in this dissertation has been developed in public
repositories, and is freely available online at
\url{http://github.com/hltdi} and \url{http://github.com/alexrudnick}.

\section{Thesis statement}
Cross-lingual word sense disambiguation is a feasible and practical
means for lexical selection in a hybrid machine translation system for a
language pair with relatively modest resources.

\section{Questions to address}
\begin{enumerate}
\item Can we use monolingual resources from the source language to improve
accuracy on the cross-lingual word sense disambiguation task?
\item Can we use multilingual resources, such as bitext corpora pairing the
source language with languages other than the current target language?
\item Will sequence labeling techniques, in which we jointly model labeling
source-language words with target-language labels, improve results for the
CL-WSD task?
\item Which kinds of machine translation systems can benefit from CL-WSD?
\end{enumerate}

\section{Dissertation Structure}
In the following chapters, we will discuss the relevant background information
and investigate these questions, as follows.

%% XXX: I vs we -- what to do here?
\begin{itemize}
\item In Chapter \ref{chap:background} we will discuss the previous work on
CL-WSD and its applications, as well as some of the approaches that have been
taken for hybrid rule-based/statistical machine translation systems. We will
also discuss Spanish and Guarani, the languages of Paraguay, and some of the
historical and social context for this translation pair.
\item In Chapter \ref{chap:evaluation}, we will discuss the data sets used in
this work and our evaluation metrics for measuring success in this project.
\item In Chapter \ref{chap:monolingual}, we will explore some approaches for
using the available resources for Spanish, including 
\item In Chapter \ref{chap:multilingual}, we will expand our approaches to
include the multilingual resources for Spanish, CL-WSD classifiers for
languages other than Guarani.
\item In Chapter \ref{chap:sequence} ... addressing Question 3.
\item In Chapter \ref{chap:combinations}, we will 
\item In Chapter \ref{chap:integration}, we will discuss integrating our CL-WSD
software into a small sampling of running machine translation systems,
addressing Question 4 and supporting our thesis statement in practice.
\item Finally, in Chapter \ref{chap:conclusions}, we will conclude and describe
possible work for the future.
\end{itemize}

%% XXX: maybe this goes in the preface?
\section{Works That Led To This Dissertation}
There's usually a section about this sort of thing, right?
But maybe it goes in the preface.
Francis has a section like this; Andy does not.

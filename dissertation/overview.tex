\chapter{Overview}

\begin{centering}
\emph{A thing with just one meaning has scarcely any meaning at
all.} \\
-- Marvin Minsky, \emph{Society of Mind} \cite{minsky1988society} \\
\end{centering}
\bigskip

In this dissertation, I investigate techniques for machine translation for
a language pair with modest resources, particularly one for which we have
little training data available for the target language. I propose
that cross-lingual word sense disambiguation (CL-WSD) is a feasible and
practical means for lexical selection in this setting. I demonstrate
this with translation experiments covering several language pairs focusing on
Spanish-Guarani -- the co-official languages of Paraguay -- and including
Quechua and English.

I describe new CL-WSD approaches, including the use of unsupervised
clustering on source-language text and multilingual corpora for the source
language as a source of classification features and sequence-labeling
techniques. I also evaluate their effectiveness in practice. All of these
techniques have been implemented in a new CL-WSD software system called Chipa
\footnote{Chipa the software is named for chipa the snack food, popular in many
  parts of South America. It is a cheesy bread made from cassava flour, often
  served in a bagel-like shape in Paraguay.  Also \emph{chipa} means 'rivet,
  bolt, screw' in Quechua; something for holding things together.  The software
is available at \\ \url{http://github.com/alexrudnick/chipa} under the GPL.}
We also integrate Chipa with several machine translation systems of different
architectures to show its practical applicability.

The research presented here is motivated in large part to allow primarily
rule-based machine translation (RBMT) systems to make use of any available
bitext training data, so that they can improve their translations without their
human authors having to write new rules.  Despite the dominance and enormous
success of statistical machine translation (SMT) approaches over the past
twenty years, they typically require large bitext corpora to be useful. RBMT
approaches remain attractive for many language pairs due to a simple dearth of
training data, and there is an active RBMT community working on developing
machine translation systems for the world's under-resourced and
under-represented languages.

My hope for this work is that it will provide techniques and tools useful for
anyone working on machine translation into these languages, whatever the
architecture of their MT software, allowing them to make use of bitext as it
becomes available for their language pair.

%% While there have been attempts to make the SMT learning possible from simply
%% comparable corpora, rather than (XXX: cite Annie?) ...

Lexical ambiguity presents a daunting challenge for rule-based machine
translation (RBMT) systems;
many words will have multiple possible translations in the target language.
Moreover, several translations of a given word may all be syntactically valid
in context, but have significantly different meanings. Even when choosing among
near-synonyms, we would like to respect the selectional preferences of the
target language so as to produce natural-sounding output text. The human
authors of the rules in an RBMT system may not have enumerated the exact
contexts or classes of contexts in which each target-language form should be
used -- this is a difficult and time-consuming task, and there will be many
thousands of words that might need to be translated in any given source
language.

To appreciate the word-sense disambiguation problem embedded in machine
translation, consider for a moment the different senses of ``have" in
English. In \emph{have a sandwich}, \emph{have a bath}, \emph{have an
argument}, and even \emph{have a good argument}, the meaning of the verb ``to
have" is quite different. It would be surprising for our target language,
especially if it is not closely related, to use a single light verb in
all of these distinct contexts. Even in a language as closely related as
Spanish, we see these English expressions rendered with some different
verbs: \emph{tomar un bocadillo} but \emph{tener un argumento}, for example.

Another concrete example, due to Annette Rios \cite{rudnick:saltmil2014},
surfaces when translating from Spanish to Quechua. 
Here we see different lexicalization patterns in Spanish and Quechua
for transitive motion verbs. The Spanish lemmas contain information about the
path of the movement, e.g. {\em traer} - 'bring (here)' vs. {\em llevar} -
'take (there)'. Quechua roots, on the other hand, use a suffix ({\em -mu}) to
express direction, but instead lexicalize information about the manner of
movement and the object that is being moved.  Consider the following examples:

\begin{itemize}
\renewcommand{\labelitemii}{$\bullet$}
 \item[] general motion verbs:
 \begin{itemize}
 \item {\em pusa-(mu-)}: `take/bring a person'
 \item {\em apa-(mu-)-}: `take/bring an animal or an inanimated object'
 \end{itemize}
 \item[] motion verbs with manner:
 \begin{itemize}
 \item {\em marq'a-(mu-)}: `take/bring something in one's arms'
 \item {\em q'ipi-(mu-)}:  `take/bring something on one's back or in a bundle'
 \item {\em millqa-(mu-)}: `take/bring something in one's skirts'
 \item {\em hapt'a-(mu-)}: `take/bring something in one's fists'
 \item {\em lluk'i-(mu-)}: `take/bring something below their arms'
 \item {\em rikra-(mu-)}:  `take/bring something on one's shoulders'
 \item {\em rampa-(mu-)}:  `take/bring a person holding their hand'
 \end{itemize}
\end{itemize}
%% XXX: why so much space underneath this?

The correct translation of Spanish {\em traer} or {\em llevar} into Quechua
thus depends on the context. Furthermore, different languages simply make
different distinctions about
the world. The Spanish \emph{hermano} 'brother', \emph{hijo} 'son' and
\emph{hija} 'daughter' all translate to different Quechua terms based on the
person related to the referent; a daughter relative to her father is
\emph{ususi}, but when described relative to her mother, \emph{warmi wawa}
\cite{academiamayor}.

Chipa, then, must somehow make these distinctions automatically if it is to
help translation systems generate valid Quechua.
Rather than having a human write down the rules for when to select different
translations, we would like to learn from examples in the available bitext
corpora.
Given such a sufficient corpus, we can discover the different possible
translations for each source-language word, and with supervised learning, how
to discriminate between them.


%%TODO(alexr): get lots of good examples
%% XXX: need several good examples here

%% ... we should actually discover these from data! Let's align europarl and find
%% them!

%% es->en examples...

%% example of completely different meanings

%% example of selectional preferences, but similar meaning

%% then es->gn examples

Writing lexical selection rules by hand, while possible in principle, is
tedious and error-prone.
Bilingual informants, if available, may not be able to enumerate the contexts
in which they would choose one alternative over another. Thus we would like to
learn from corpora when possible.
We do not have very large sentence-aligned bitext corpora for most language
pairs, however, and will have to make do with the Bible (though other bitext is
absolutely welcome and valuable!), which is an acceptable
starting point containing many commonly used words.\footnote{The
Bible has been translated into a substantial fraction of the world's languages
and contains more than thirty thousand verses (roughly, sentences) in
a variety of text genres. For an overview of using the Bible for multilingual
NLP applications, please see Resnik \emph{et al.}
\cite{DBLP:journals/lre/ResnikOD99} and Mayer and Cysouw
\cite{MAYER14.220.L14-1215}.}

%%However, for most language pairs, suitably large sentence-aligned bitext
%%corpora are not available, so creating and deploying a translation system based
%%on machine learning techniques will require collecting a larger corpus.
%%However, that project is not the focus of this dissertation work.

The major contributions of this work will be new approaches for CL-WSD,
integrating CL-WSD into MT systems ...
Additionally, on a practical level we will develop a suite of reusable
open-source software including a hybrid MT system for Spanish-Guarani,

All of the software used in this dissertation has been developed in public
repositories, and is freely available online at
\url{http://github.com/hltdi} and \url{http://github.com/alexrudnick}.

\section{Thesis statement}
Cross-lingual word sense disambiguation is a feasible and practical
means for lexical selection in a hybrid machine translation system for a
language pair with relatively modest resources.

\section{Questions to address}
\begin{enumerate}
\item Can we use monolingual resources from the source language to improve
accuracy on the cross-lingual word sense disambiguation task?
\item Can we use multilingual resources, such as bitext corpora pairing the
source language with languages other than the current target language?
\item Will sequence labeling techniques, in which we jointly model labeling
source-language words with target-language labels, improve results for the
CL-WSD task?
\item Which kinds of machine translation systems can benefit from CL-WSD?
\end{enumerate}

\section{Dissertation Structure}
In the following chapters, I will discuss the relevant background information
and investigate these questions, as follows.

\begin{itemize}
\item Chapter \ref{chap:background} gives some background on word sense
disambiguation, its history, and its applications in machine translation, as
well as on Spanish and Guarani, the languages of Paraguay, and some of the
social context for this translation pair.
\item Chapter \ref{chap:relatedwork} reviews some recent work on CL-WSD and its
applications.
\item Chapter \ref{chap:evaluation} presents the data sets used in this work,
our evaluation metrics for this project, and the basic architecture of the
Chipa system.
\item Chapter \ref{chap:monolingual} explores some approaches for learning from
the available resources for Spanish to give us better representations for
translating out of it.
\item Chapter \ref{chap:multilingual} expands our approaches to include the
multilingual resources for Spanish, integrating CL-WSD classifiers for
languages other than Guarani.
\item Chapter \ref{chap:sequence} discusses approaches for jointly assigning
target-language labels to an entire sentence at once with sequence models.
\item Chapter \ref{chap:combinations} shows ways to combine the previous
approaches ... %% XXX do we need this chapter?
\item Chapter \ref{chap:integration} presents practical applications of Chipa;
here we will see how to integrate CL-WSD software into different running
machine translation systems.
\item Finally, in Chapter \ref{chap:conclusions}, we will conclude and describe
possible work for the future.
\end{itemize}

%% XXX: maybe this goes in the preface?
\section{Works That Led To This Dissertation}
There's usually a section about this sort of thing, right?
But maybe it goes in the preface.
Francis has a section like this; Andy does not.

\begin{itemize}
\item Rudnick 2011 \cite{rudnick:2011:RANLPStud}
\item Rudnick and Gasser 2013 \cite{rudnick-gasser:2013:HyTra}
\item Rudnick, Rios and Gasser 2014 \cite{rudnick:saltmil2014}
\item Rudnick, Liu and Gasser 2013 \cite{rudnick-liu-gasser:2013:SemEval-2013}
\item related: Rudnick, Skidmore, Samaniego and Gasser 2014 \cite{RUDNICK14.151}
\end{itemize}

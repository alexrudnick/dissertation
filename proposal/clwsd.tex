\section{Techniques for CL-WSD}

%% XXX: this sentence needs some help
We will develop and extend at least two broad approaches for CL-WSD: the use
of multilingual evidence where available and sequence labeling.

Both of these techniques have been prototyped and presented at workshops, but
they will be refined significantly and integrated into a more general tool for
use in practice with MT systems.

\subsection{Using multilingual evidence}
For many languages, we have multiple bitext corpora, where each corpus covers a
different language pair. There are, for example, many bitext corpora available
for English and other languages, or for Spanish and other languages.
We would like to be able to make use of evidence from all of these corpora when
translating into any particular target language, if possible.
Each corpus may contain useful examples of a given source-language word,
and senses of that word may be lexicalized in varying, non-overlapping ways in
the different target languages.
We would want a CL-WSD system to be able to pick up on the relationships
between senses of a given word -- two target languages may happen to surface
the same sense distinctions, perhaps due to being related languages, or simply
by coincidence. Alternatively, a combination of translations into several
languages may provide evidence for a certain lexical choice in the target
language.

XXX some good examples would be good XXX

\begin{figure}
  \includegraphics[width=12cm]{hutchins-leg-etc.png}
  \caption{Overlap of words related to ``leg"; relationships between English
  and French words. Translation is pretty complicated.}
  \label{fig:leg}
\end{figure}
(Figure 21.2 from Jurafsky and Martin first edition, apparently due to Hutchins
et al (XXX fix citation))

We could also imagine using a sense-annotated corpus to train a ``monolingual"
WSD system, and use this as a 
... basically ensemble methods for WSD.

This approach is informed by the work of Lefever and Hoste
\cite{lefever-hoste-decock:2011:ACL-HLT2011}, although their technique requires
an entire machine translation system to perform CL-WSD, which is somewhat
unwieldy when we want to use CL-WSD as a subcomponent of a machine translation
system.

Earlier this year, we developed a prototype version of this kind of CL-WSD
system \cite{rudnick-liu-gasser:2013:SemEval-2013} and produced some of the
best results in a SemEval shared task on CL-WSD \cite{task10}, in which we
translated polysemous nouns from English into other European languages. In our
SemEval paper, we presented three variations on CL-WSD systems: a
straightforward classification approach, a classifier stacking approach, and a
graphical model system based on loopy belief propagation on a Markov random
field.

The simplest system presented in that work, based on a maximum entropy
classifier, simply extracted features from a window around the English noun to
be translated and used these to make a prediction. The same system could also
return a probability distribution over target-language words or phrases. This
simple system was used as a subcomponent in the two more sophisticated systems.


The classifier stacking approach ...
for each available bitext corpus, for each source-language word that we want to
model, we train a classifier to predict translations from the source language
into the other language of the bitext.
Then we can include the output of that classifier as a feature when training
our classifier for the desired target language.


In the graphical model approach, we treat the problem of 


\subsection{Lexical selection as a sequence labeling problem}

We have also developed a prototype
CL-WSD in both English-Spanish and Spanish-Guarani translation tasks at the
HyTra workshop in August \cite{rudnick-gasser:2013:HyTra-2013}.

We will also investigate the use of sequence-labeling models for
lexical selection.  The intuition behind the sequence-labeling approach is that
machine translation implies an ``all-words" WSD task, in that we need to choose
a translation for every word or phrase in the source sentence, and that the
sequence of translations chosen should make sense when taken together.

One promising formalism for this line of work is the Maximum
Entropy Markov Model (MEMM), which can be combined in a straightforward way
with the simpler Hidden Markov Model (HMM). This combination allows for
efficient inference and the ability to trade more computational resources for
richer modeling. More sophisticated sequence models, such as Conditional Random
Fields, may be useful in this task as well.


\subsection{Applying similar techniques for morphology prediction}

\section{Techniques for CL-WSD}

%% XXX: this sentence needs some help
We will develop and extend at least two broad approaches for CL-WSD: the use
of multilingual evidence where available and sequence labeling.

Both of these techniques have been prototyped and presented at workshops, but
they will be refined significantly.

\subsection{Using multilingual evidence}
One of the techniques for CL-WSD that we will investigate is the use of
multiple bitext corpora, so that the software can make use of information from
available bitext from several different language pairs.
This approach is informed by the work of Lefever and Hoste
\cite{lefever-hoste-decock:2011:ACL-HLT2011}, although their technique requires
an entire machine translation system to perform CL-WSD, which is somewhat
unwieldy when we want to use CL-WSD as a subcomponent of a machine translation
system.

Earlier this year, we developed a prototype version of this kind of CL-WSD
system \cite{rudnick-liu-gasser:2013:SemEval-2013} and produced some of the
best results in a SemEval shared task on CL-WSD \cite{task10}, in which we
translated polysemous nouns from English into other European languages. In our
SemEval paper, we presented three variations on CL-WSD systems: a
straightforward classification approach, a classifier stacking approach, and a
graphical model system based on loopy belief propagation on a Markov random
field.

The simplest system presented in that work, based on a maximum entropy
classifier, simply extracted features from a window around the English noun to
be translated and used these to make a prediction. The same system could also
return a probability distribution over target-language words or phrases. This
simple system was used as a subcomponent in the two more sophisticated systems.


The classifier stacking approach ...


In the graphical model approach, we treat the problem of 


\subsection{Lexical selection as a sequence labeling problem}

We have also developed a prototype
CL-WSD in both English-Spanish and Spanish-Guarani translation tasks at the
HyTra workshop in August \cite{rudnick-gasser:2013:HyTra-2013}.

We will also investigate the use of sequence-labeling models for
lexical selection.  The intuition behind the sequence-labeling approach is that
machine translation implies an ``all-words" WSD task, in that we need to choose
a translation for every word or phrase in the source sentence, and that the
sequence of translations chosen should make sense when taken together.

One promising formalism for this line of work is the Maximum
Entropy Markov Model (MEMM), which can be combined in a straightforward way
with the simpler Hidden Markov Model (HMM). This combination allows for
efficient inference and the ability to trade more computational resources for
richer modeling. More sophisticated sequence models, such as Conditional Random
Fields, may be useful in this task as well.


\subsection{Applying similar techniques for morphology prediction}

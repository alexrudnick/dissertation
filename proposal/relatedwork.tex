\section{Related Work}
\label{sec:relatedwork}

\subsection{Cross-lingual Word Sense Disambiguation}

\subsubsection{CL-WSD in SMT}
While most SMT systems do not make use of an explicit WSD module, recently
there has been work on adding WSD classifiers in to statistical MT systems.
Particularly, Carpuat and Wu have shown how to use CL-WSD, or more broadly,
cross-lingual phrase sense disambiguation, to improve modern phrase-based SMT
systems
\cite{carpuatpsd,carpuat-wu:2007:EMNLP-CoNLL2007,carpuat2008evaluation}. In
Carpuat's work, classifiers are used to label multi-word expressions (phrases,
in the phrase-based SMT sense) with target language phrases. She demonstrates
how this is more appropriate in an an SMT setting than simply labeling
individual words with WordNet synsets, as had previously been attempted, and
showed significant improvements on a Chinese-English translation task.

There has also been work on using discriminative MaxEnt models to adapt
the ``forward" translation model of an SMT system, using richer
source-language context features \cite{vzabokrtsky-popel-marevcek:2010:WMT}.
Somewhat interestingly, while it has the same effect, this work does not
describe itself in terms of word sense disambiguation.

\subsubsection{CL-WSD \emph{in vitro}}
ParaSense, the CL-WSD system developed by Els Lefever
\cite{lefever-hoste-decock:2011:ACL-HLT2011}, takes into account evidence from
several different parallel corpora.
For predicting the translation of a source word into
any particular target language, ParaSense creates
bag-of-words features from the translations of the input sentence into every
other language that it knows about. As of this writing, ParaSense handles
translation from English into French, Spanish, Italian, Dutch and German.
Given corpora that are parallel over many languages, such as Europarl, this is
straightforward at
training time. However, at testing time it requires a complete MT system for
each of the four other languages, which seems computationally prohibitive. In
the past, ParaSense has simply called out to the Google Translate API to
generate the bag-of-words features required for test sentences. This seems
unwieldy, and thus in our work, we learn from several parallel corpora but
require neither a locally running MT system nor access to an online translation
API.

To our knowledge, there has not been other work on framing CL-WSD for all words
in an input sentence as a sequence labeling problem. However, in monolingual
WSD, Molina \textit{et al.} \shortcite{DBLP:conf/iberamia/MolinaPS02} have made
use of HMMs for WSD. 

%%\subsection{Translation into Morphologically Rich Languages}
%%Chris Dyer's recent paper at EMNLP
%%\cite{chahuneau:2013:emnlp}
%%Talk about prediction for morphology.
%%\cite{toutanova-suzuki-ruopp:2008:ACLMain}
%%Also factored models...
%%\cite{yeniterzi-oflazer:2010:ACL}

\subsection{Hybrid Machine Translation for Under-Resourced Languages}
%% text from the HyTra paper
However, there has been work recently on using WSD techniques for translation
into lower-resourced languages, such as the English-Slovene language pair, as
in \cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012}. 

The Apertium team has a particular practical interest in improving lexical
selection in RBMT; they recently have been developing
a new system, described in \cite{tyers-fst}, that learns finite-state
transducers for lexical selection from the available parallel corpora. It is
intended to be both very fast, for use in practical translation systems, and
to produce lexical selection rules that are understandable and modifiable by
humans.

\subsection{Creating Corpora Through Crowdsourcing}
There has been some work on creating multilingual corpora through
crowdsourcing, in several variations. Some projects have explicitly had the
goal of creating sentence-aligned bitexts useful for training MT systems, while
others have aimed to create useful resources for humans. Also the means of
crowdsourcing has varied; some contributors have been paid, and others are
volunteers.

\subsubsection{Crowdsourcing Bitext for MT}
Vamshi Ambati's recent work \cite{ambati_naacl,ambati_act} focuses on using
crowdsourcing and active learning to produce a corpus for English-Spanish SMT.
His system has a representation of which words and phrases it should learn
about, finding n-grams that are common in a development set but have little
support in the training set, and then automatically creates Mechanical Turk
tasks to elicit translations containing translations of these n-grams. This
work makes use of the relatively large population of Internet users (and thus
MTurk users) familiar with both English and Spanish, and succeeded in building
a large bitext corpus quickly and cheaply.

The Joshua team at Johns Hopkins has also successfully used Mechanical Turk to
crowdsource the creation of corpora for SMT
\cite{post-callisonburch-osborne:2012:WMT}. In this work, the team built
large parallel corpora for many languages of the Indian
subcontinent\footnote{\url{http://joshua-decoder.org/indian-parallel-corpora/}}
and developed a simple approach for discovering the high-quality contributions,
in which MTurk users vote on which translations they consider the best.

%% XXX
There's also MonoTrans
\url{http://www.cs.umd.edu/hcil/monotrans/}
"... an iterative protocol in which monolingual human participants work
together to improve imperfect machine translations." 

%% Tradubi
Tradubi project
\url{http://wiki.apertium.org/wiki/Tradubi}

(project seems to have died, though)

%% XXX
then there's Amara and obviously DuoLingo...

\subsubsection{Crowdsourcing Resources for People}

There are also a number of community-driven projects for building translations
of documents with human readers as the intended audience. These projects are
all run by volunteers, 

Contrastingly, the Tatoeba project is a repository of 
%% Tatoeba
Tatoeba -- repository of a bunch of example sentences translated into a variety
of languages.
\url{http://tatoeba.org/}


%% Traduwiki
Traduwiki
\url{http://traduwiki.org}

This looks awesome. If we could just bring up an instance of this, that would
be close to the right thing.

%% OPUS
OPUS, "the open parallel corpus".
\url{http://opus.lingfil.uu.se/}

\subsection{Online Language Tools for Guarani}

While there are not currently many online language tools for the Guarani, there
are a few ...

Notably, there is 
iGuarani
\url{http://iguarani.com/}

Wolf Lustig's dictionary
\footnote{\url{http://www.uni-mainz.de/cgi-bin/guarani2/dictionary.pl}}, which
has versions in Spanish, German, and English.


\footnote{\url{http://cafehistoria.ning.com/profiles/blogs/la-lengua-guarani-o-avanee-en}}

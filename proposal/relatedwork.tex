\section{Related Work}
\label{sec:relatedwork}

\subsection{Cross-lingual Word Sense Disambiguation}

While most SMT systems do not make use of an explicit WSD module, recently
Carpuat and Wu have shown how to use word-sense disambiguation techniques to
improve modern phrase-based SMT systems, in a normally 

\cite{carpuatpsd,carpuat-wu:2007:EMNLP-CoNLL2007,carpuat2008evaluation}


ParaSense, the system of Lefever
and Hoste, takes into account evidence from all of the available parallel
corpora. Let $S$ be the set of five target languages and $t$ be the particular
target language of interest at the moment; ParaSense creates bag-of-words
features from the translations of the target sentence into the languages $S -
\lbrace{t \rbrace}$.
Given corpora that are parallel over many languages, this is straightforward at
training time. However, at testing time it requires a complete MT system for
each of the four other languages, which is computationally prohibitive. Thus in
our work, we learn from several parallel corpora but require neither a locally
running MT system nor access to an online translation API.

Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to make use of
multiple bitext corpora for the CL-WSD task.

To our knowledge, there has not been other work on framing all-words WSD as a
sequence labeling problem. However, in monolingual WSD, Molina \textit{et al.}
\shortcite{DBLP:conf/iberamia/MolinaPS02}
have made use of HMMs for WSD. 


\subsection{Translation into Morphologically Rich Languages}



Chris Dyer's recent paper at EMNLP
\cite{chahuneau:2013:emnlp}

Talk about prediction for morphology.
\cite{toutanova-suzuki-ruopp:2008:ACLMain}

Also factored models...
\cite{yeniterzi-oflazer:2010:ACL}

\subsection{Hybrid Machine Translation for Under-Resourced Languages}
HMT for low-resource languages...

%% text from the HyTra paper
However, there has been work recently on using WSD techniques for translation
into lower-resourced languages, such as the English-Slovene language pair, as
in \cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012}. 

The Apertium team has a particular practical interest in improving lexical
selection in RBMT; they recently have been developing
a new system, described in \cite{tyers-fst}, that learns finite-state
transducers for lexical selection from the available parallel corpora. It is
intended to be both very fast, for use in practical translation systems, and
to produce lexical selection rules that are understandable and modifiable by
humans.

Put in some stuff about SAMT and Stat-XFER.

Who's doing transfer rules that are hand-written?


\subsection{Creating Corpora Through Crowdsourcing}
There has been some work on creating multilingual corpora through
crowdsourcing, in several variations. Some projects have explicitly had the
goal of creating sentence-aligned bitexts useful for training MT systems, while
others have aimed to create useful resources for humans. Also the means of
crowdsourcing has varied; some contributors have been paid, and others are
volunteers.

Vamshi Ambati's recent work \cite{ambati_naacl,ambati_act} focuses on using
crowdsourcing and active learning to produce a corpus for English-Spanish SMT.
His system has a representation of which words and phrases it should learn
about, finding n-grams that are common in a development set but have little
support in the training set, and then automatically creates Mechanical Turk
tasks to elicit translations containing translations of these n-grams. This
work makes use of the relatively large population of Internet users (and thus
MTurk users) familiar with both English and Spanish, and succeeded in building
a large bitext corpus quickly and cheaply.

The Joshua team at Johns Hopkins has also successfully used Mechanical Turk to
crowdsource the creation of corpora for SMT
\cite{post-callisonburch-osborne:2012:WMT}. In this work, the team built
large parallel corpora for many languages of the Indian
subcontinent\footnote{\url{http://joshua-decoder.org/indian-parallel-corpora/}}
and developed a simple approach for discovering the high-quality contributions,
in which MTurk users vote on which translations they consider the best.

There's also MonoTrans
\url{http://www.cs.umd.edu/hcil/monotrans/}
"... an iterative protocol in which monolingual human participants work
together to improve imperfect machine translations." 

Contrastingly, the Tatoeba project is a repository of 
%% Tatoeba
Tatoeba -- repository of a bunch of example sentences translated into a variety
of languages.
\url{http://tatoeba.org/}

%% Tradubi
Tradubi project
\url{http://wiki.apertium.org/wiki/Tradubi}

(project seems to have died, though)

%% Traduwiki
Traduwiki
\url{http://traduwiki.org}

This looks awesome. If we could just bring up an instance of this, that would
be close to the right thing.

%% OPUS
OPUS, "the open parallel corpus".
\url{http://opus.lingfil.uu.se/}

\subsection{Online Language Tools for Guarani}

While there are not currently many online language tools for the Guarani, there
are a few ...

Notably, there is 
iGuarani
\url{http://iguarani.com/}

Wolf Lustig's dictionary
\footnote{\url{http://www.uni-mainz.de/cgi-bin/guarani2/dictionary.pl}}, which
has versions in Spanish, German, and English.


\footnote{\url{http://cafehistoria.ning.com/profiles/blogs/la-lengua-guarani-o-avanee-en}}

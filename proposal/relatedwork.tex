\section{Related Work}

\subsection{Cross-lingual word sense disambiguation}

Framing the resolution of lexical ambiguities in machine translation as an
explicit classification task has a long history; practical work in integrating
WSD with statistical machine translation dates back to early SMT work at IBM
\cite{Brown91word-sensedisambiguation}, but the problem itself was described in
Warren Weaver's prescient 1949 memorandum \cite{weavermemo}, which outlines the
modern conception of word sense disambiguation.

More recently, Carpuat and Wu have shown how to use word-sense disambiguation
techniques to improve modern phrase-based SMT systems \cite{carpuatpsd}, even
though most SMT systems do not use an explicit WSD module \cite{wsdchap3}, as
the language model and phrase tables of these systems mitigate lexical
ambiguities somewhat.

Treating lexical selection as a word-sense disambiguation problem in which the
sense inventory for each source-language word is its set of possible
translations is often called cross-lingual WSD (CL-WSD). This framing has
received enough attention to warrant shared tasks at recent SemEval workshops;
the most recent running of the task is described in \cite{task10}.

\subsection{lexical selection}


\cite{carpuat2008evaluation}

\cite{carpuat-wu:2007:EMNLP-CoNLL2007}

\begin{itemize}
  \item Lefever and Hoste
  \item Carpuat and Wu
  \item Francis Tyers \cite{tyers-fst}
  \item Quechua verb disambiguation?
\end{itemize}


The Apertium team has a particular practical interest in improving lexical
selection in RBMT; they recently have been developing
a new system, described in \cite{tyers-fst}, that learns finite-state
transducers for lexical selection from the available parallel corpora. It is
intended to be both very fast, for use in practical translation systems, and
to produce lexical selection rules that are understandable and modifiable by
humans.

Outside of the CL-WSD setting, there has been work on framing all-words WSD as
a sequence labeling problem. Particularly, Molina \textit{et al.}
\shortcite{DBLP:conf/iberamia/MolinaPS02} have made use of HMMs for all-words
WSD in a monolingual setting.

%% /text from the HyTra paper


%% text from the semeval paper
In the cross-language word-sense disambiguation (CL-WSD) task, given an
instance of an ambiguous word used in a context, we want to predict the
appropriate translation into some target language. This setting for WSD has an
immediate application in machine translation, since many words have multiple
possible translations. Framing the resolution of lexical ambiguities as an
explicit classification task has a long history, and was considered in early
SMT work at IBM \cite{Brown91word-sensedisambiguation}. More recently, Carpuat
and Wu have shown how to use CL-WSD techniques to improve modern phrase-based
SMT systems \cite{carpuatpsd}, even though the language model and phrase-tables
of these systems mitigate the problem of lexical ambiguities somewhat.

In the SemEval-2013 CL-WSD shared task \cite{task10}, entrants are asked to
build a system that can provide translations for twenty ambiguous English
nouns, given appropriate contexts -- here the particular usage of the ambiguous
noun is called the \emph{target} word. The five target languages of the shared
task are Spanish, Dutch, German, Italian and French. In the evaluation, for
each of the twenty ambiguous nouns, systems are to provide translations for
the target word in each of fifty sentences or short passages. The translations
of each English word may be single words or short phrases in the target
language, but in either case, they should be lemmatized.

Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to make use of
multiple bitext corpora for the CL-WSD task. ParaSense, the system of Lefever
and Hoste, takes into account evidence from all of the available parallel
corpora. Let $S$ be the set of five target languages and $t$ be the particular
target language of interest at the moment; ParaSense creates bag-of-words
features from the translations of the target sentence into the languages $S -
\lbrace{t \rbrace}$. Given corpora that are parallel over many languages, this
is straightforward at training time. However, at testing time it requires a
complete MT system for each of the four other languages, which is
computationally prohibitive. Thus in our work, we learn from several parallel
corpora but require neither a locally running MT system nor access to an online
translation API.

At the time of the evaluation, our simplest system had the top results in the
shared task for the out-of-five evaluation for three languages (Spanish,
German, and Italian).  However, after the evaluation deadline, we fixed a
simple bug in our MRF code, and the MRF system then achieved even better
results for the \emph{oof} evaluation. For the \emph{best} evaluation, our two
more sophisticated systems posted better results than the L1 version. All of
our systems beat the ``most-frequent sense" baseline in every case.

%%/text from the semeval paper


\subsection{translation into morphologically rich languages}

how do people do this usually?


\subsection{Hybrid machine translation}
HMT for low-resource languages...

%% text from the HyTra paper
To our knowledge, there has not been work specifically on sequence labeling
applied to lexical selection for RBMT systems. However, 
there has been work recently on using WSD techniques for translation into
lower-resourced languages, such as the English-Slovene language pair, as in 
\cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012}. 


Put in some stuff about SAMT.

Who's doing transfer rules that are hand-written?

\subsection{Creating corpora through crowdsourcing}

\begin{itemize}
  \item Vamshi Ambati for the crowdsourcing (who else)?
\end{itemize}


\cite{ambati_naacl}

\cite{ambati_act}


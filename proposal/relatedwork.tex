\section{Related Work}

\subsection{Cross-lingual Word Sense Disambiguation}

While most SMT systems do not make use of an explicit WSD module, recently
Carpuat and Wu have shown how to use word-sense disambiguation techniques to
improve modern phrase-based SMT systems, in a normally 

\cite{carpuatpsd,carpuat-wu:2007:EMNLP-CoNLL2007,carpuat2008evaluation}


ParaSense, the system of Lefever
and Hoste, takes into account evidence from all of the available parallel
corpora. Let $S$ be the set of five target languages and $t$ be the particular
target language of interest at the moment; ParaSense creates bag-of-words
features from the translations of the target sentence into the languages $S -
\lbrace{t \rbrace}$.
Given corpora that are parallel over many languages, this is straightforward at
training time. However, at testing time it requires a complete MT system for
each of the four other languages, which is computationally prohibitive. Thus in
our work, we learn from several parallel corpora but require neither a locally
running MT system nor access to an online translation API.

Following the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, we wanted to make use of
multiple bitext corpora for the CL-WSD task.

To our knowledge, there has not been other work on framing all-words WSD as a
sequence labeling problem. However, in monolingual WSD, Molina \textit{et al.}
\shortcite{DBLP:conf/iberamia/MolinaPS02}
have made use of HMMs for WSD. 


\subsection{Translation into Morphologically Rich Languages}

Chris Dyer's recent paper at EMNLP
\cite{chahuneau:2013:emnlp}

Talk about prediction for morphology.
\cite{toutanova-suzuki-ruopp:2008:ACLMain}

Also factored models...
\cite{yeniterzi-oflazer:2010:ACL}

\subsection{Hybrid Machine Translation for Under-Resourced Languages}
HMT for low-resource languages...

%% text from the HyTra paper
However, there has been work recently on using WSD techniques for translation
into lower-resourced languages, such as the English-Slovene language pair, as
in \cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012}. 

The Apertium team has a particular practical interest in improving lexical
selection in RBMT; they recently have been developing
a new system, described in \cite{tyers-fst}, that learns finite-state
transducers for lexical selection from the available parallel corpora. It is
intended to be both very fast, for use in practical translation systems, and
to produce lexical selection rules that are understandable and modifiable by
humans.

Put in some stuff about SAMT and Stat-XFER.

Who's doing transfer rules that are hand-written?




\subsection{Creating corpora through crowdsourcing}

There has been some work on creating corpora for statistical machine
translation through crowdsourcing.

Vamshi Ambati's dissertation work focuses on using crowdsourcing and active
learning to produce a corpus for English-Spanish SMT. His system has a notion
of which words and phrases it would like to learn about -- n-grams that are
common in a development set but have little support in the training set -- and
then automatically creates Mechanical Turk tasks to elicit translations
containing translations of these n-grams.

This work makes use of the relatively large population of Internet users (and
thus MTurk users) familiar with both English and Spanish.

\cite{ambati_naacl}

\cite{ambati_act}

Large parallel corpora have also been created by that Johns Hopkins group
with the Indian languages
\cite{post-callisonburch-osborne:2012:WMT}

\subsection{Translation with volunteers}

%% Tatoeba
Tatoeba -- repository of a bunch of example sentences translated into a variety
of languages.
\url{http://tatoeba.org/}

There's also MonoTrans
\url{http://www.cs.umd.edu/hcil/monotrans/}

"... an iterative protocol in which monolingual human participants work
together to improve imperfect machine translations." 

%% Tradubi
Tradubi project
\url{http://wiki.apertium.org/wiki/Tradubi}

(project seems to have died, though)

%% Traduwiki
Traduwiki
\url{http://traduwiki.org}

This looks awesome. If we could just bring up an instance of this, that would
be close to the right thing.

%% OPUS
OPUS, "the open parallel corpus".
\url{http://opus.lingfil.uu.se/}

\subsection{Online Language Tools for Guarani}

While there are not currently many online language tools for the Guarani, there
are a few ...

Notably, there is 
iGuarani
\url{http://iguarani.com/}

Wolf Lustig's dictionary...

